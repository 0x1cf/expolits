From: Henry <zhangzhi2022@gmail.com>
Date: Sun Aug 1 15:11:14 2010 
Subject: [PATCH] kvm patch

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 76f5483..48252c9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -390,8 +390,8 @@ struct kvm_arch {
 	 */
 	struct list_head active_mmu_pages;
 	struct list_head assigned_dev_head;
-	struct iommu_domain *iommu_domain;
-	int iommu_flags;
+	struct list_head iommu_domain;
+	int iommu_flags;		/* ored flags */
 	struct kvm_pic *vpic;
 	struct kvm_ioapic *vioapic;
 	struct kvm_pit *vpit;
diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index f0b6e5d..7753e97 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -11,6 +11,8 @@
 #include <asm/page.h>
 #include <asm/types.h>
 
+#define OTHER_CL_MONITOR
+
 /*
  * low level task data that entry.S needs immediate access to
  * - this struct should fit entirely inside of one cache line
@@ -41,8 +43,22 @@ struct thread_info {
 	__u8			supervisor_stack[0];
 #endif
 	int			uaccess_err;
+#ifdef OTHER_CL_MONITOR
+	__u32 			*monitored_ptr;
+	__u32			*resched_ptr;
+	__u32			monitored_val;
+#endif
 };
 
+#ifdef OTHER_CL_MONITOR
+#define CL_MONITOR_INIT 			\
+			.monitored_ptr = NULL, 	\
+			.resched_ptr = NULL,	
+#else
+#define CL_MONITOR_INIT
+#endif
+
+
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.task		= &tsk,			\
@@ -54,6 +70,7 @@ struct thread_info {
 	.restart_block = {			\
 		.fn = do_no_restart_syscall,	\
 	},					\
+	CL_MONITOR_INIT				\
 }
 
 #define init_thread_info	(init_thread_union.thread_info)
@@ -253,6 +270,35 @@ static inline void set_restore_sigmask(void)
 	ti->status |= TS_RESTORE_SIGMASK;
 	set_bit(TIF_SIGPENDING, (unsigned long *)&ti->flags);
 }
+
+static inline void set_cl_monitored_ptr(__u32* ptr)
+{
+	struct thread_info *ti = current_thread_info();
+	ti->monitored_ptr = ptr;
+}
+
+static inline void set_cl_monitored_val(__u32 val)
+{
+	struct thread_info *ti = current_thread_info();
+	ti->monitored_val = val;
+}
+
+static inline void set_cl_resched_ptr(__u32* ptr)
+{
+	struct thread_info *ti = current_thread_info();
+	ti->resched_ptr = ptr;
+}
+
+static inline void wake_cl_monitoring(struct thread_info *ti)
+{
+#if 0
+	BUG_ON(!irqs_disabled()); /* Remove later - just for asserting no interrupts here */
+#endif
+	if (ti->resched_ptr) {
+		(*(ti->resched_ptr))++;
+	}
+}
+
 #endif	/* !__ASSEMBLY__ */
 
 #ifndef __ASSEMBLY__
diff --git a/arch/x86/kernel/amd_iommu.c b/arch/x86/kernel/amd_iommu.c
index 0d20286..0b45c79 100644
--- a/arch/x86/kernel/amd_iommu.c
+++ b/arch/x86/kernel/amd_iommu.c
@@ -2546,7 +2546,8 @@ static int amd_iommu_unmap(struct iommu_domain *dom, unsigned long iova,
 }
 
 static phys_addr_t amd_iommu_iova_to_phys(struct iommu_domain *dom,
-					  unsigned long iova)
+					  unsigned long iova,
+					  int* mapped)
 {
 	struct protection_domain *domain = dom->priv;
 	unsigned long offset_mask;
@@ -2575,6 +2576,25 @@ static int amd_iommu_domain_has_cap(struct iommu_domain *domain,
 	return 0;
 }
 
+static
+int amd_iommu_get_iommu(struct iommu_domain* domain,
+                                struct pci_dev *dev)
+{
+       return -1; /* not implemented */
+}
+
+static
+int amd_iommu_vdomain_init(struct iommu_domain* domain)
+{
+	return -1;
+}
+
+static
+int amd_iommu_vdomain_destroy(struct iommu_domain* domain)
+{
+	return -1;
+}
+
 static struct iommu_ops amd_iommu_ops = {
 	.domain_init = amd_iommu_domain_init,
 	.domain_destroy = amd_iommu_domain_destroy,
@@ -2584,6 +2604,9 @@ static struct iommu_ops amd_iommu_ops = {
 	.unmap = amd_iommu_unmap,
 	.iova_to_phys = amd_iommu_iova_to_phys,
 	.domain_has_cap = amd_iommu_domain_has_cap,
+	.get_iommu = amd_iommu_get_iommu,
+	.vdomain_init  = amd_iommu_vdomain_init,
+	.vdomain_destroy = amd_iommu_vdomain_destroy,
 };
 
 /*****************************************************************************
diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c
index 5db5b7d..1726596 100644
--- a/arch/x86/kernel/cpu/perf_event.c
+++ b/arch/x86/kernel/cpu/perf_event.c
@@ -533,14 +533,14 @@ static int __hw_perf_event_init(struct perf_event *event)
 {
 	int err;
 
-	if (!x86_pmu_initialized())
+	if (!x86_pmu_initialized()) 
 		return -ENODEV;
 
 	err = 0;
 	if (!atomic_inc_not_zero(&active_events)) {
 		mutex_lock(&pmc_reserve_mutex);
 		if (atomic_read(&active_events) == 0) {
-			if (!reserve_pmc_hardware())
+			if (!reserve_pmc_hardware()) 
 				err = -EBUSY;
 			else {
 				err = reserve_ds_buffers();
@@ -847,7 +847,7 @@ void hw_perf_enable(void)
 
 			if (!match_prev_assignment(hwc, cpuc, i))
 				x86_assign_hw_event(event, cpuc, i);
-			else if (i < n_running)
+			else if (i < n_running) 
 				continue;
 
 			x86_pmu_start(event);
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index e7e3521..85b1628 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -26,6 +26,8 @@ unsigned long idle_halt;
 EXPORT_SYMBOL(idle_halt);
 unsigned long idle_nomwait;
 EXPORT_SYMBOL(idle_nomwait);
+void (*pm_cl_monitoring)(u64* monitored, u64 prev_val, u32 cstate);
+EXPORT_SYMBOL(pm_cl_monitoring);
 
 struct kmem_cache *task_xstate_cachep;
 
@@ -334,7 +336,10 @@ EXPORT_SYMBOL(boot_option_idle_override);
  */
 void (*pm_idle)(void);
 EXPORT_SYMBOL(pm_idle);
-
+/*
+extern void (*pm_cl_monitoring)(u64* monitored, u64 prev_val);
+EXPORT_SYMBOL(pm_cl_monitoring);
+*/
 #ifdef CONFIG_X86_32
 /*
  * This halt magic was a workaround for ancient floppy DMA
@@ -441,35 +446,68 @@ EXPORT_SYMBOL_GPL(cpu_idle_wait);
  */
 void mwait_idle_with_hints(unsigned long ax, unsigned long cx)
 {
+	struct thread_info *ti = current_thread_info();
+	void* monitored_ptr = ti->monitored_ptr ? ti->monitored_ptr :
+		 ((void *)&current_thread_info()->flags);
 	trace_power_start(POWER_CSTATE, (ax>>4)+1);
-	if (!need_resched()) {
+
+	if (!need_resched() && (ti->monitored_ptr == NULL || 
+		*ti->monitored_ptr == ti->monitored_val)) {
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
-			clflush((void *)&current_thread_info()->flags);
+			clflush((void*)monitored_ptr);
 
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		__monitor((void *)monitored_ptr, 0, 0);
 		smp_mb();
-		if (!need_resched())
+		if (!need_resched() && (ti->monitored_ptr == NULL || 
+			*ti->monitored_ptr == ti->monitored_val)) {
 			__mwait(ax, cx);
+		}
 	}
 }
 
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
 {
-	if (!need_resched()) {
+	struct thread_info *ti = current_thread_info();
+	void* monitored_ptr = ti->monitored_ptr ? ti->monitored_ptr :
+		 ((void *)&current_thread_info()->flags);
+	if (!need_resched() && (ti->monitored_ptr == NULL ||
+			*ti->monitored_ptr == ti->monitored_val)) {
 		trace_power_start(POWER_CSTATE, 1);
 		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
-			clflush((void *)&current_thread_info()->flags);
+			clflush((void *)monitored_ptr);
 
-		__monitor((void *)&current_thread_info()->flags, 0, 0);
+		__monitor((void *)monitored_ptr, 0, 0);
 		smp_mb();
-		if (!need_resched())
+		if (!need_resched() && (ti->monitored_ptr == NULL || 
+			*ti->monitored_ptr == ti->monitored_val)) {
 			__sti_mwait(0, 0);
+		} else {
+			local_irq_enable();
+		}
+	} else
+		local_irq_enable();
+}
+
+/* Default MONITOR/MWAIT with no hints, used for default C1 state */
+static void mwait_idle_cl_monitoring(u64* monitored, u64 prev_val, u32 cstate)
+{
+	if (!need_resched() && prev_val == *monitored) {
+		trace_power_start(POWER_CSTATE, cstate);
+		if (cpu_has(&current_cpu_data, X86_FEATURE_CLFLUSH_MONITOR))
+			clflush(monitored);
+
+		__monitor(monitored, 0, 0);
+/*		smp_mb();*/
+		if (!need_resched() && *monitored == prev_val)
+			__sti_mwait((cstate-1)<<4, 0);
 		else
 			local_irq_enable();
 	} else
 		local_irq_enable();
 }
+EXPORT_SYMBOL(mwait_idle_cl_monitoring);
+
 
 /*
  * On SMP it's slightly faster (but much more power-consuming!)
@@ -638,6 +676,7 @@ void __cpuinit select_idle_routine(const struct cpuinfo_x86 *c)
 		 */
 		printk(KERN_INFO "using mwait in idle threads.\n");
 		pm_idle = mwait_idle;
+		pm_cl_monitoring = mwait_idle_cl_monitoring;
 	} else if (check_c1e_idle(c)) {
 		printk(KERN_INFO "using C1E aware idle routine\n");
 		pm_idle = c1e_idle;
diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index 31a7035..e4a0035 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -8,7 +8,7 @@ CFLAGS_vmx.o := -I.
 kvm-y			+= $(addprefix ../../../virt/kvm/, kvm_main.o ioapic.o \
 				coalesced_mmio.o irq_comm.o eventfd.o \
 				assigned-dev.o)
-kvm-$(CONFIG_IOMMU_API)	+= $(addprefix ../../../virt/kvm/, iommu.o)
+kvm-$(CONFIG_IOMMU_API) += $(addprefix ../../../virt/kvm/, iommu.o viommu.o)
 
 kvm-y			+= x86.o mmu.o emulate.o i8259.o irq.o lapic.o \
 			   i8254.o timer.o
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index ee03679..239fae6 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -26,6 +26,7 @@
 #include <linux/sched.h>
 #include <linux/moduleparam.h>
 #include <linux/ftrace_event.h>
+#include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/tboot.h>
 #include "kvm_cache_regs.h"
@@ -4097,7 +4098,7 @@ static u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 	 */
 	if (is_mmio)
 		ret = MTRR_TYPE_UNCACHABLE << VMX_EPT_MT_EPTE_SHIFT;
-	else if (vcpu->kvm->arch.iommu_domain &&
+	else if (!list_empty(&vcpu->kvm->arch.iommu_domain) &&	
 		!(vcpu->kvm->arch.iommu_flags & KVM_IOMMU_CACHE_COHERENCY))
 		ret = kvm_get_guest_memory_type(vcpu, gfn) <<
 		      VMX_EPT_MT_EPTE_SHIFT;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 7fa89c3..a1d3e97 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -37,6 +37,7 @@
 #include <linux/iommu.h>
 #include <linux/intel-iommu.h>
 #include <linux/cpufreq.h>
+#include <linux/viommu.h>
 #include <linux/user-return-notifier.h>
 #include <linux/srcu.h>
 #include <linux/slab.h>
@@ -53,6 +54,8 @@
 #include <asm/mtrr.h>
 #include <asm/mce.h>
 
+#include <linux/viommu.h>
+
 #define MAX_IO_MSRS 256
 #define CR0_RESERVED_BITS						\
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -4707,6 +4710,8 @@ static int __vcpu_run(struct kvm_vcpu *vcpu)
 
 	srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
 
+ 	viommu_mark_vmexit(kvm);
+
 	vapic_exit(vcpu);
 
 	return r;
@@ -4755,6 +4760,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	r = __vcpu_run(vcpu);
 
 out:
+
 	post_kvm_run_save(vcpu);
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
@@ -5372,6 +5378,8 @@ struct  kvm *kvm_arch_create_vm(void)
 
 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
 	INIT_LIST_HEAD(&kvm->arch.assigned_dev_head);
+	INIT_LIST_HEAD(&kvm->arch.iommu_domain);
+	kvm->arch.iommu_flags = 0;
 
 	/* Reserve bit 0 of irq_sources_bitmap for userspace irq source */
 	set_bit(KVM_USERSPACE_IRQ_SOURCE_ID, &kvm->arch.irq_sources_bitmap);
@@ -5417,6 +5425,7 @@ void kvm_arch_sync_events(struct kvm *kvm)
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvm_iommu_unmap_guest(kvm);
+	kvm_viommu_exit(kvm);
 	kvm_free_pit(kvm);
 	kfree(kvm->arch.vpic);
 	kfree(kvm->arch.vioapic);
diff --git a/drivers/acpi/processor_idle.c b/drivers/acpi/processor_idle.c
index e9a8026..69b05e0 100644
--- a/drivers/acpi/processor_idle.c
+++ b/drivers/acpi/processor_idle.c
@@ -141,7 +141,8 @@ static void acpi_safe_halt(void)
 	 */
 	smp_mb();
 	if (!need_resched()) {
-		safe_halt();
+		if (!current_thread_info()->monitored_ptr)
+			safe_halt();
 		local_irq_disable();
 	}
 	current_thread_info()->status |= TS_POLLING;
diff --git a/drivers/base/iommu.c b/drivers/base/iommu.c
index 6e6b6a1..e336a0a 100644
--- a/drivers/base/iommu.c
+++ b/drivers/base/iommu.c
@@ -39,7 +39,7 @@ bool iommu_found(void)
 }
 EXPORT_SYMBOL_GPL(iommu_found);
 
-struct iommu_domain *iommu_domain_alloc(void)
+struct iommu_domain *iommu_domain_alloc(struct pci_dev* dev)
 {
 	struct iommu_domain *domain;
 	int ret;
@@ -48,10 +48,13 @@ struct iommu_domain *iommu_domain_alloc(void)
 	if (!domain)
 		return NULL;
 
-	ret = iommu_ops->domain_init(domain);
+	ret = iommu_ops->get_iommu(domain, dev);
 	if (ret)
 		goto out_free;
 
+	/* Clearing the flags and link */
+	domain->iommu_flags = 0;
+	INIT_LIST_HEAD(&domain->link);
 	return domain;
 
 out_free:
@@ -81,9 +84,9 @@ void iommu_detach_device(struct iommu_domain *domain, struct device *dev)
 EXPORT_SYMBOL_GPL(iommu_detach_device);
 
 phys_addr_t iommu_iova_to_phys(struct iommu_domain *domain,
-			       unsigned long iova)
+			       unsigned long iova, int *mapped)
 {
-	return iommu_ops->iova_to_phys(domain, iova);
+	return iommu_ops->iova_to_phys(domain, iova, mapped);
 }
 EXPORT_SYMBOL_GPL(iommu_iova_to_phys);
 
@@ -94,6 +97,25 @@ int iommu_domain_has_cap(struct iommu_domain *domain,
 }
 EXPORT_SYMBOL_GPL(iommu_domain_has_cap);
 
+
+int iommu_get_iommu_domain_for_dev(struct iommu_domain* domain, struct pci_dev* dev)
+{
+       return iommu_ops->get_iommu(domain, dev);
+}
+EXPORT_SYMBOL_GPL(iommu_get_iommu_domain_for_dev);
+
+int iommu_vdomain_init(struct iommu_domain* domain)
+{
+	return iommu_ops->vdomain_init(domain);
+}
+EXPORT_SYMBOL_GPL(iommu_vdomain_init);
+
+int iommu_vdomain_destroy(struct iommu_domain* domain)
+{
+	return iommu_ops->vdomain_destroy(domain);
+}
+EXPORT_SYMBOL_GPL(iommu_vdomain_destroy);
+
 int iommu_map(struct iommu_domain *domain, unsigned long iova,
 	      phys_addr_t paddr, int gfp_order, int prot)
 {
diff --git a/drivers/net/benet/Kconfig b/drivers/net/benet/Kconfig
index 1a41a49..fdb6e81 100644
--- a/drivers/net/benet/Kconfig
+++ b/drivers/net/benet/Kconfig
@@ -1,6 +1,6 @@
 config BE2NET
-	tristate "ServerEngines' 10Gbps NIC - BladeEngine"
+	tristate "ServerEngines' 10Gbps NIC - BladeEngine 2"
 	depends on PCI && INET
 	help
 	This driver implements the NIC functionality for ServerEngines'
-	10Gbps network adapter - BladeEngine.
+	10Gbps network adapter - BladeEngine 2.
diff --git a/drivers/net/benet/be.h b/drivers/net/benet/be.h
index b46be49..5bc7459 100644
--- a/drivers/net/benet/be.h
+++ b/drivers/net/benet/be.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -29,30 +29,31 @@
 #include <linux/workqueue.h>
 #include <linux/interrupt.h>
 #include <linux/firmware.h>
-#include <linux/slab.h>
 
 #include "be_hw.h"
 
-#define DRV_VER			"2.102.147u"
+#define DRV_VER			"2.101.346u"
 #define DRV_NAME		"be2net"
 #define BE_NAME			"ServerEngines BladeEngine2 10Gbps NIC"
 #define BE3_NAME		"ServerEngines BladeEngine3 10Gbps NIC"
 #define OC_NAME			"Emulex OneConnect 10Gbps NIC"
 #define OC_NAME1		"Emulex OneConnect 10Gbps NIC (be3)"
-#define DRV_DESC		"ServerEngines BladeEngine 10Gbps NIC Driver"
+#define DRV_DESC		BE_NAME "Driver"
 
 #define BE_VENDOR_ID 		0x19a2
 #define BE_DEVICE_ID1		0x211
 #define BE_DEVICE_ID2		0x221
 #define OC_DEVICE_ID1		0x700
-#define OC_DEVICE_ID2		0x710
+#define OC_DEVICE_ID2		0x701
+#define OC_DEVICE_ID3		0x710
 
 static inline char *nic_name(struct pci_dev *pdev)
 {
 	switch (pdev->device) {
 	case OC_DEVICE_ID1:
-		return OC_NAME;
 	case OC_DEVICE_ID2:
+		return OC_NAME;
+	case OC_DEVICE_ID3:
 		return OC_NAME1;
 	case BE_DEVICE_ID2:
 		return BE3_NAME;
@@ -84,8 +85,6 @@ static inline char *nic_name(struct pci_dev *pdev)
 
 #define FW_VER_LEN		32
 
-#define BE_MAX_VF		32
-
 struct be_dma_mem {
 	void *va;
 	dma_addr_t dma;
@@ -154,7 +153,6 @@ struct be_eq_obj {
 struct be_mcc_obj {
 	struct be_queue_info q;
 	struct be_queue_info cq;
-	bool rearm_cq;
 };
 
 struct be_drvr_stats {
@@ -167,7 +165,6 @@ struct be_drvr_stats {
 	ulong be_tx_jiffies;
 	u64 be_tx_bytes;
 	u64 be_tx_bytes_prev;
-	u64 be_tx_pkts;
 	u32 be_tx_rate;
 
 	u32 cache_barrier[16];
@@ -179,7 +176,6 @@ struct be_drvr_stats {
 	ulong be_rx_jiffies;
 	u64 be_rx_bytes;
 	u64 be_rx_bytes_prev;
-	u64 be_rx_pkts;
 	u32 be_rx_rate;
 	/* number of non ether type II frames dropped where
 	 * frame len > length field of Mac Hdr */
@@ -209,7 +205,7 @@ struct be_tx_obj {
 /* Struct to remember the pages posted for rx frags */
 struct be_rx_page_info {
 	struct page *page;
-	DEFINE_DMA_UNMAP_ADDR(bus);
+	dma_addr_t bus;
 	u16 page_offset;
 	bool last_page_user;
 };
@@ -256,8 +252,7 @@ struct be_adapter {
 	bool rx_post_starved;	/* Zero rx frags have been posted to BE */
 
 	struct vlan_group *vlan_grp;
-	u16 vlans_added;
-	u16 max_vlans;	/* Number of vlans supported */
+	u16 num_vlans;
 	u8 vlan_tag[VLAN_GROUP_ARRAY_LEN];
 	struct be_dma_mem mc_cmd_mem;
 
@@ -271,7 +266,6 @@ struct be_adapter {
 	u32 if_handle;		/* Used to configure filtering */
 	u32 pmac_id;		/* MAC addr handle used by BE card */
 
-	bool eeh_err;
 	bool link_up;
 	u32 port_num;
 	bool promiscuous;
@@ -283,17 +277,8 @@ struct be_adapter {
 	u8 port_type;
 	u8 transceiver;
 	u8 generation;		/* BladeEngine ASIC generation */
-	u32 flash_status;
-	struct completion flash_compl;
-
-	bool sriov_enabled;
-	u32 vf_if_handle[BE_MAX_VF];
-	u32 vf_pmac_id[BE_MAX_VF];
-	u8 base_eq_id;
 };
 
-#define be_physfn(adapter) (!adapter->pdev->is_virtfn)
-
 /* BladeEngine Generation numbers */
 #define BE_GEN2 2
 #define BE_GEN3 3
@@ -302,6 +287,11 @@ extern const struct ethtool_ops be_ethtool_ops;
 
 #define drvr_stats(adapter)		(&adapter->stats.drvr_stats)
 
+static inline unsigned int be_pci_func(struct be_adapter *adapter)
+{
+	return PCI_FUNC(adapter->pdev->devfn);
+}
+
 #define BE_SET_NETDEV_OPS(netdev, ops)	(netdev->netdev_ops = ops)
 
 #define PAGE_SHIFT_4K		12
diff --git a/drivers/net/benet/be_cmds.c b/drivers/net/benet/be_cmds.c
index b9ad799..dd4d4ea 100644
--- a/drivers/net/benet/be_cmds.c
+++ b/drivers/net/benet/be_cmds.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -59,13 +59,6 @@ static int be_mcc_compl_process(struct be_adapter *adapter,
 
 	compl_status = (compl->status >> CQE_STATUS_COMPL_SHIFT) &
 				CQE_STATUS_COMPL_MASK;
-
-	if ((compl->tag0 == OPCODE_COMMON_WRITE_FLASHROM) &&
-		(compl->tag1 == CMD_SUBSYSTEM_COMMON)) {
-		adapter->flash_status = compl_status;
-		complete(&adapter->flash_compl);
-	}
-
 	if (compl_status == MCC_STATUS_SUCCESS) {
 		if (compl->tag0 == OPCODE_ETH_GET_STATISTICS) {
 			struct be_cmd_resp_get_stats *resp =
@@ -111,26 +104,10 @@ static struct be_mcc_compl *be_mcc_compl_get(struct be_adapter *adapter)
 	return NULL;
 }
 
-void be_async_mcc_enable(struct be_adapter *adapter)
-{
-	spin_lock_bh(&adapter->mcc_cq_lock);
-
-	be_cq_notify(adapter, adapter->mcc_obj.cq.id, true, 0);
-	adapter->mcc_obj.rearm_cq = true;
-
-	spin_unlock_bh(&adapter->mcc_cq_lock);
-}
-
-void be_async_mcc_disable(struct be_adapter *adapter)
-{
-	adapter->mcc_obj.rearm_cq = false;
-}
-
-int be_process_mcc(struct be_adapter *adapter, int *status)
+int be_process_mcc(struct be_adapter *adapter)
 {
 	struct be_mcc_compl *compl;
-	int num = 0;
-	struct be_mcc_obj *mcc_obj = &adapter->mcc_obj;
+	int num = 0, status = 0;
 
 	spin_lock_bh(&adapter->mcc_cq_lock);
 	while ((compl = be_mcc_compl_get(adapter))) {
@@ -142,31 +119,31 @@ int be_process_mcc(struct be_adapter *adapter, int *status)
 			be_async_link_state_process(adapter,
 				(struct be_async_event_link_state *) compl);
 		} else if (compl->flags & CQE_FLAGS_COMPLETED_MASK) {
-				*status = be_mcc_compl_process(adapter, compl);
-				atomic_dec(&mcc_obj->q.used);
+				status = be_mcc_compl_process(adapter, compl);
+				atomic_dec(&adapter->mcc_obj.q.used);
 		}
 		be_mcc_compl_use(compl);
 		num++;
 	}
 
+	if (num)
+		be_cq_notify(adapter, adapter->mcc_obj.cq.id, true, num);
+
 	spin_unlock_bh(&adapter->mcc_cq_lock);
-	return num;
+	return status;
 }
 
 /* Wait till no more pending mcc requests are present */
 static int be_mcc_wait_compl(struct be_adapter *adapter)
 {
 #define mcc_timeout		120000 /* 12s timeout */
-	int i, num, status = 0;
-	struct be_mcc_obj *mcc_obj = &adapter->mcc_obj;
-
+	int i, status;
 	for (i = 0; i < mcc_timeout; i++) {
-		num = be_process_mcc(adapter, &status);
-		if (num)
-			be_cq_notify(adapter, mcc_obj->cq.id,
-				mcc_obj->rearm_cq, num);
+		status = be_process_mcc(adapter);
+		if (status)
+			return status;
 
-		if (atomic_read(&mcc_obj->q.used) == 0)
+		if (atomic_read(&adapter->mcc_obj.q.used) == 0)
 			break;
 		udelay(100);
 	}
@@ -174,7 +151,7 @@ static int be_mcc_wait_compl(struct be_adapter *adapter)
 		dev_err(&adapter->pdev->dev, "mccq poll timed out\n");
 		return -1;
 	}
-	return status;
+	return 0;
 }
 
 /* Notify MCC requests and wait for completion */
@@ -190,14 +167,7 @@ static int be_mbox_db_ready_wait(struct be_adapter *adapter, void __iomem *db)
 	u32 ready;
 
 	do {
-		ready = ioread32(db);
-		if (ready == 0xffffffff) {
-			dev_err(&adapter->pdev->dev,
-				"pci slot disconnected\n");
-			return -1;
-		}
-
-		ready &= MPU_MAILBOX_DB_RDY_MASK;
+		ready = ioread32(db) & MPU_MAILBOX_DB_RDY_MASK;
 		if (ready)
 			break;
 
@@ -228,11 +198,6 @@ static int be_mbox_notify_wait(struct be_adapter *adapter)
 	struct be_mcc_mailbox *mbox = mbox_mem->va;
 	struct be_mcc_compl *compl = &mbox->compl;
 
-	/* wait for ready to be set */
-	status = be_mbox_db_ready_wait(adapter, db);
-	if (status != 0)
-		return status;
-
 	val |= MPU_MAILBOX_DB_HI_MASK;
 	/* at bits 2 - 31 place mbox dma addr msb bits 34 - 63 */
 	val |= (upper_32_bits(mbox_mem->dma) >> 2) << 2;
@@ -294,7 +259,7 @@ int be_cmd_POST(struct be_adapter *adapter)
 		} else {
 			return 0;
 		}
-	} while (timeout < 40);
+	} while (timeout < 20);
 
 	dev_err(&adapter->pdev->dev, "POST timeout; stage=0x%x\n", stage);
 	return -1;
@@ -432,9 +397,6 @@ int be_cmd_fw_clean(struct be_adapter *adapter)
 	u8 *wrb;
 	int status;
 
-	if (adapter->eeh_err)
-		return -EIO;
-
 	spin_lock(&adapter->mbox_lock);
 
 	wrb = (u8 *)wrb_from_mbox(adapter);
@@ -472,6 +434,8 @@ int be_cmd_eq_create(struct be_adapter *adapter,
 
 	req->num_pages =  cpu_to_le16(PAGES_4K_SPANNED(q_mem->va, q_mem->size));
 
+	AMAP_SET_BITS(struct amap_eq_context, func, req->context,
+			be_pci_func(adapter));
 	AMAP_SET_BITS(struct amap_eq_context, valid, req->context, 1);
 	/* 4byte eqe*/
 	AMAP_SET_BITS(struct amap_eq_context, size, req->context, 0);
@@ -634,6 +598,7 @@ int be_cmd_cq_create(struct be_adapter *adapter,
 	AMAP_SET_BITS(struct amap_cq_context, eventable, ctxt, 1);
 	AMAP_SET_BITS(struct amap_cq_context, eqid, ctxt, eq->id);
 	AMAP_SET_BITS(struct amap_cq_context, armed, ctxt, 1);
+	AMAP_SET_BITS(struct amap_cq_context, func, ctxt, be_pci_func(adapter));
 	be_dws_cpu_to_le(ctxt, sizeof(req->context));
 
 	be_cmd_page_addrs_prepare(req->pages, ARRAY_SIZE(req->pages), q_mem);
@@ -680,8 +645,9 @@ int be_cmd_mccq_create(struct be_adapter *adapter,
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_COMMON,
 			OPCODE_COMMON_MCC_CREATE, sizeof(*req));
 
-	req->num_pages = cpu_to_le16(PAGES_4K_SPANNED(q_mem->va, q_mem->size));
+	req->num_pages = PAGES_4K_SPANNED(q_mem->va, q_mem->size);
 
+	AMAP_SET_BITS(struct amap_mcc_context, fid, ctxt, be_pci_func(adapter));
 	AMAP_SET_BITS(struct amap_mcc_context, valid, ctxt, 1);
 	AMAP_SET_BITS(struct amap_mcc_context, ring_size, ctxt,
 		be_encoded_q_len(mccq->len));
@@ -730,6 +696,8 @@ int be_cmd_txq_create(struct be_adapter *adapter,
 
 	AMAP_SET_BITS(struct amap_tx_context, tx_ring_size, ctxt,
 		be_encoded_q_len(txq->len));
+	AMAP_SET_BITS(struct amap_tx_context, pci_func_id, ctxt,
+			be_pci_func(adapter));
 	AMAP_SET_BITS(struct amap_tx_context, ctx_valid, ctxt, 1);
 	AMAP_SET_BITS(struct amap_tx_context, cq_id_send, ctxt, cq->id);
 
@@ -801,9 +769,6 @@ int be_cmd_q_destroy(struct be_adapter *adapter, struct be_queue_info *q,
 	u8 subsys = 0, opcode = 0;
 	int status;
 
-	if (adapter->eeh_err)
-		return -EIO;
-
 	spin_lock(&adapter->mbox_lock);
 
 	wrb = wrb_from_mbox(adapter);
@@ -850,8 +815,7 @@ int be_cmd_q_destroy(struct be_adapter *adapter, struct be_queue_info *q,
  * Uses mbox
  */
 int be_cmd_if_create(struct be_adapter *adapter, u32 cap_flags, u32 en_flags,
-		u8 *mac, bool pmac_invalid, u32 *if_handle, u32 *pmac_id,
-		u32 domain)
+		u8 *mac, bool pmac_invalid, u32 *if_handle, u32 *pmac_id)
 {
 	struct be_mcc_wrb *wrb;
 	struct be_cmd_req_if_create *req;
@@ -868,7 +832,6 @@ int be_cmd_if_create(struct be_adapter *adapter, u32 cap_flags, u32 en_flags,
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_COMMON,
 		OPCODE_COMMON_NTWK_INTERFACE_CREATE, sizeof(*req));
 
-	req->hdr.domain = domain;
 	req->capability_flags = cpu_to_le32(cap_flags);
 	req->enable_flags = cpu_to_le32(en_flags);
 	req->pmac_invalid = pmac_invalid;
@@ -894,9 +857,6 @@ int be_cmd_if_destroy(struct be_adapter *adapter, u32 interface_id)
 	struct be_cmd_req_if_destroy *req;
 	int status;
 
-	if (adapter->eeh_err)
-		return -EIO;
-
 	spin_lock(&adapter->mbox_lock);
 
 	wrb = wrb_from_mbox(adapter);
@@ -1120,10 +1080,6 @@ int be_cmd_promiscuous_config(struct be_adapter *adapter, u8 port_num, bool en)
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_ETH,
 		OPCODE_ETH_PROMISCUOUS, sizeof(*req));
 
-	/* In FW versions X.102.149/X.101.487 and later,
-	 * the port setting associated only with the
-	 * issuing pci function will take effect
-	 */
 	if (port_num)
 		req->port1_promiscuous = en;
 	else
@@ -1169,12 +1125,11 @@ int be_cmd_multicast_set(struct be_adapter *adapter, u32 if_id,
 
 	req->interface_id = if_id;
 	if (netdev) {
-		int i;
+		int i = 0;
 		struct netdev_hw_addr *ha;
 
 		req->num_mac = cpu_to_le16(netdev_mc_count(netdev));
 
-		i = 0;
 		netdev_for_each_mc_addr(ha, netdev)
 			memcpy(req->mac[i].byte, ha->addr, ETH_ALEN);
 	} else {
@@ -1419,24 +1374,22 @@ int be_cmd_write_flashrom(struct be_adapter *adapter, struct be_dma_mem *cmd,
 			u32 flash_type, u32 flash_opcode, u32 buf_size)
 {
 	struct be_mcc_wrb *wrb;
-	struct be_cmd_write_flashrom *req;
+	struct be_cmd_write_flashrom *req = cmd->va;
 	struct be_sge *sge;
 	int status;
 
 	spin_lock_bh(&adapter->mcc_lock);
-	adapter->flash_status = 0;
 
 	wrb = wrb_from_mccq(adapter);
 	if (!wrb) {
 		status = -EBUSY;
-		goto err_unlock;
+		goto err;
 	}
 	req = cmd->va;
 	sge = nonembedded_sgl(wrb);
 
 	be_wrb_hdr_prepare(wrb, cmd->size, false, 1,
 			OPCODE_COMMON_WRITE_FLASHROM);
-	wrb->tag1 = CMD_SUBSYSTEM_COMMON;
 
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_COMMON,
 		OPCODE_COMMON_WRITE_FLASHROM, cmd->size);
@@ -1448,24 +1401,14 @@ int be_cmd_write_flashrom(struct be_adapter *adapter, struct be_dma_mem *cmd,
 	req->params.op_code = cpu_to_le32(flash_opcode);
 	req->params.data_buf_size = cpu_to_le32(buf_size);
 
-	be_mcc_notify(adapter);
-	spin_unlock_bh(&adapter->mcc_lock);
-
-	if (!wait_for_completion_timeout(&adapter->flash_compl,
-			msecs_to_jiffies(12000)))
-		status = -1;
-	else
-		status = adapter->flash_status;
-
-	return status;
+	status = be_mcc_notify_wait(adapter);
 
-err_unlock:
+err:
 	spin_unlock_bh(&adapter->mcc_lock);
 	return status;
 }
 
-int be_cmd_get_flash_crc(struct be_adapter *adapter, u8 *flashed_crc,
-			 int offset)
+int be_cmd_get_flash_crc(struct be_adapter *adapter, u8 *flashed_crc)
 {
 	struct be_mcc_wrb *wrb;
 	struct be_cmd_write_flashrom *req;
@@ -1486,10 +1429,10 @@ int be_cmd_get_flash_crc(struct be_adapter *adapter, u8 *flashed_crc,
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_COMMON,
 		OPCODE_COMMON_READ_FLASHROM, sizeof(*req)+4);
 
-	req->params.op_type = cpu_to_le32(IMG_TYPE_REDBOOT);
+	req->params.op_type = cpu_to_le32(FLASHROM_TYPE_REDBOOT);
 	req->params.op_code = cpu_to_le32(FLASHROM_OPER_REPORT);
-	req->params.offset = cpu_to_le32(offset);
-	req->params.data_buf_size = cpu_to_le32(0x4);
+	req->params.offset = 0x3FFFC;
+	req->params.data_buf_size = 0x4;
 
 	status = be_mcc_notify_wait(adapter);
 	if (!status)
@@ -1500,7 +1443,7 @@ err:
 	return status;
 }
 
-int be_cmd_enable_magic_wol(struct be_adapter *adapter, u8 *mac,
+extern int be_cmd_enable_magic_wol(struct be_adapter *adapter, u8 *mac,
 				struct be_dma_mem *nonemb_cmd)
 {
 	struct be_mcc_wrb *wrb;
@@ -1593,7 +1536,7 @@ int be_cmd_loopback_test(struct be_adapter *adapter, u32 port_num,
 
 	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_LOWLEVEL,
 			OPCODE_LOWLEVEL_LOOPBACK_TEST, sizeof(*req));
-	req->hdr.timeout = cpu_to_le32(4);
+	req->hdr.timeout = 4;
 
 	req->pattern = cpu_to_le64(pattern);
 	req->src_port = cpu_to_le32(port_num);
@@ -1664,33 +1607,3 @@ err:
 	spin_unlock_bh(&adapter->mcc_lock);
 	return status;
 }
-
-int be_cmd_get_seeprom_data(struct be_adapter *adapter,
-				struct be_dma_mem *nonemb_cmd)
-{
-	struct be_mcc_wrb *wrb;
-	struct be_cmd_req_seeprom_read *req;
-	struct be_sge *sge;
-	int status;
-
-	spin_lock_bh(&adapter->mcc_lock);
-
-	wrb = wrb_from_mccq(adapter);
-	req = nonemb_cmd->va;
-	sge = nonembedded_sgl(wrb);
-
-	be_wrb_hdr_prepare(wrb, sizeof(*req), false, 1,
-			OPCODE_COMMON_SEEPROM_READ);
-
-	be_cmd_hdr_prepare(&req->hdr, CMD_SUBSYSTEM_COMMON,
-			OPCODE_COMMON_SEEPROM_READ, sizeof(*req));
-
-	sge->pa_hi = cpu_to_le32(upper_32_bits(nonemb_cmd->dma));
-	sge->pa_lo = cpu_to_le32(nonemb_cmd->dma & 0xFFFFFFFF);
-	sge->len = cpu_to_le32(nonemb_cmd->size);
-
-	status = be_mcc_notify_wait(adapter);
-
-	spin_unlock_bh(&adapter->mcc_lock);
-	return status;
-}
diff --git a/drivers/net/benet/be_cmds.h b/drivers/net/benet/be_cmds.h
index 763dc19..8e2d89c 100644
--- a/drivers/net/benet/be_cmds.h
+++ b/drivers/net/benet/be_cmds.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -124,7 +124,6 @@ struct be_mcc_mailbox {
 #define OPCODE_COMMON_CQ_CREATE				12
 #define OPCODE_COMMON_EQ_CREATE				13
 #define OPCODE_COMMON_MCC_CREATE        		21
-#define OPCODE_COMMON_SEEPROM_READ			30
 #define OPCODE_COMMON_NTWK_RX_FILTER    		34
 #define OPCODE_COMMON_GET_FW_VERSION			35
 #define OPCODE_COMMON_SET_FLOW_CONTROL			36
@@ -856,19 +855,6 @@ struct be_cmd_resp_ddrdma_test {
 	u8  rcv_buff[4096];
 };
 
-/*********************** SEEPROM Read ***********************/
-
-#define BE_READ_SEEPROM_LEN 1024
-struct be_cmd_req_seeprom_read {
-	struct be_cmd_req_hdr hdr;
-	u8 rsvd0[BE_READ_SEEPROM_LEN];
-};
-
-struct be_cmd_resp_seeprom_read {
-	struct be_cmd_req_hdr hdr;
-	u8 seeprom_data[BE_READ_SEEPROM_LEN];
-};
-
 extern int be_pci_fnum_get(struct be_adapter *adapter);
 extern int be_cmd_POST(struct be_adapter *adapter);
 extern int be_cmd_mac_addr_query(struct be_adapter *adapter, u8 *mac_addr,
@@ -878,7 +864,7 @@ extern int be_cmd_pmac_add(struct be_adapter *adapter, u8 *mac_addr,
 extern int be_cmd_pmac_del(struct be_adapter *adapter, u32 if_id, u32 pmac_id);
 extern int be_cmd_if_create(struct be_adapter *adapter, u32 cap_flags,
 			u32 en_flags, u8 *mac, bool pmac_invalid,
-			u32 *if_handle, u32 *pmac_id, u32 domain);
+			u32 *if_handle, u32 *pmac_id);
 extern int be_cmd_if_destroy(struct be_adapter *adapter, u32 if_handle);
 extern int be_cmd_eq_create(struct be_adapter *adapter,
 			struct be_queue_info *eq, int eq_delay);
@@ -920,7 +906,7 @@ extern int be_cmd_get_flow_control(struct be_adapter *adapter,
 extern int be_cmd_query_fw_cfg(struct be_adapter *adapter,
 			u32 *port_num, u32 *cap);
 extern int be_cmd_reset_function(struct be_adapter *adapter);
-extern int be_process_mcc(struct be_adapter *adapter, int *status);
+extern int be_process_mcc(struct be_adapter *adapter);
 extern int be_cmd_set_beacon_state(struct be_adapter *adapter,
 			u8 port_num, u8 beacon, u8 status, u8 state);
 extern int be_cmd_get_beacon_state(struct be_adapter *adapter,
@@ -930,21 +916,15 @@ extern int be_cmd_read_port_type(struct be_adapter *adapter, u32 port,
 extern int be_cmd_write_flashrom(struct be_adapter *adapter,
 			struct be_dma_mem *cmd, u32 flash_oper,
 			u32 flash_opcode, u32 buf_size);
-int be_cmd_get_flash_crc(struct be_adapter *adapter, u8 *flashed_crc,
-				int offset);
+extern int be_cmd_get_flash_crc(struct be_adapter *adapter, u8 *flashed_crc);
 extern int be_cmd_enable_magic_wol(struct be_adapter *adapter, u8 *mac,
 				struct be_dma_mem *nonemb_cmd);
 extern int be_cmd_fw_init(struct be_adapter *adapter);
 extern int be_cmd_fw_clean(struct be_adapter *adapter);
-extern void be_async_mcc_enable(struct be_adapter *adapter);
-extern void be_async_mcc_disable(struct be_adapter *adapter);
 extern int be_cmd_loopback_test(struct be_adapter *adapter, u32 port_num,
 				u32 loopback_type, u32 pkt_size,
 				u32 num_pkts, u64 pattern);
 extern int be_cmd_ddr_dma_test(struct be_adapter *adapter, u64 pattern,
 			u32 byte_cnt, struct be_dma_mem *cmd);
-extern int be_cmd_get_seeprom_data(struct be_adapter *adapter,
-				struct be_dma_mem *nonemb_cmd);
 extern int be_cmd_set_loopback(struct be_adapter *adapter, u8 port_num,
 				u8 loopback_type, u8 enable);
-
diff --git a/drivers/net/benet/be_ethtool.c b/drivers/net/benet/be_ethtool.c
index 200e985..5d001c4 100644
--- a/drivers/net/benet/be_ethtool.c
+++ b/drivers/net/benet/be_ethtool.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -112,7 +112,6 @@ static const char et_self_tests[][ETH_GSTRING_LEN] = {
 	"PHY Loopback test",
 	"External Loopback test",
 	"DDR DMA test"
-	"Link test"
 };
 
 #define ETHTOOL_TESTS_NUM ARRAY_SIZE(et_self_tests)
@@ -276,6 +275,8 @@ be_get_ethtool_stats(struct net_device *netdev,
 		data[i] = (et_stats[i].size == sizeof(u64)) ?
 				*(u64 *)p: *(u32 *)p;
 	}
+
+	return;
 }
 
 static void
@@ -464,6 +465,7 @@ be_get_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)
 	else
 		wol->wolopts = 0;
 	memset(&wol->sopass, 0, sizeof(wol->sopass));
+	return;
 }
 
 static int
@@ -487,13 +489,13 @@ be_test_ddr_dma(struct be_adapter *adapter)
 {
 	int ret, i;
 	struct be_dma_mem ddrdma_cmd;
-	u64 pattern[2] = {0x5a5a5a5a5a5a5a5aULL, 0xa5a5a5a5a5a5a5a5ULL};
+	u64 pattern[2] = {0x5a5a5a5a5a5a5a5a, 0xa5a5a5a5a5a5a5a5};
 
 	ddrdma_cmd.size = sizeof(struct be_cmd_req_ddrdma_test);
 	ddrdma_cmd.va = pci_alloc_consistent(adapter->pdev, ddrdma_cmd.size,
 					&ddrdma_cmd.dma);
 	if (!ddrdma_cmd.va) {
-		dev_err(&adapter->pdev->dev, "Memory allocation failure\n");
+		dev_err(&adapter->pdev->dev, "Memory allocation failure \n");
 		return -ENOMEM;
 	}
 
@@ -527,9 +529,6 @@ static void
 be_self_test(struct net_device *netdev, struct ethtool_test *test, u64 *data)
 {
 	struct be_adapter *adapter = netdev_priv(netdev);
-	bool link_up;
-	u8 mac_speed = 0;
-	u16 qos_link_speed = 0;
 
 	memset(data, 0, sizeof(u64) * ETHTOOL_TESTS_NUM);
 
@@ -546,20 +545,12 @@ be_self_test(struct net_device *netdev, struct ethtool_test *test, u64 *data)
 						&data[2]) != 0) {
 			test->flags |= ETH_TEST_FL_FAILED;
 		}
-	}
 
-	if (be_test_ddr_dma(adapter) != 0) {
-		data[3] = 1;
-		test->flags |= ETH_TEST_FL_FAILED;
+		data[3] = be_test_ddr_dma(adapter);
+		if (data[3] != 0)
+			test->flags |= ETH_TEST_FL_FAILED;
 	}
 
-	if (be_cmd_link_status_query(adapter, &link_up, &mac_speed,
-				&qos_link_speed) != 0) {
-		test->flags |= ETH_TEST_FL_FAILED;
-		data[4] = -1;
-	} else if (mac_speed) {
-		data[4] = 1;
-	}
 }
 
 static int
@@ -576,57 +567,12 @@ be_do_flash(struct net_device *netdev, struct ethtool_flash *efl)
 	return be_load_fw(adapter, file_name);
 }
 
-static int
-be_get_eeprom_len(struct net_device *netdev)
-{
-	return BE_READ_SEEPROM_LEN;
-}
-
-static int
-be_read_eeprom(struct net_device *netdev, struct ethtool_eeprom *eeprom,
-			uint8_t *data)
-{
-	struct be_adapter *adapter = netdev_priv(netdev);
-	struct be_dma_mem eeprom_cmd;
-	struct be_cmd_resp_seeprom_read *resp;
-	int status;
-
-	if (!eeprom->len)
-		return -EINVAL;
-
-	eeprom->magic = BE_VENDOR_ID | (adapter->pdev->device<<16);
-
-	memset(&eeprom_cmd, 0, sizeof(struct be_dma_mem));
-	eeprom_cmd.size = sizeof(struct be_cmd_req_seeprom_read);
-	eeprom_cmd.va = pci_alloc_consistent(adapter->pdev, eeprom_cmd.size,
-				&eeprom_cmd.dma);
-
-	if (!eeprom_cmd.va) {
-		dev_err(&adapter->pdev->dev,
-			"Memory allocation failure. Could not read eeprom\n");
-		return -ENOMEM;
-	}
-
-	status = be_cmd_get_seeprom_data(adapter, &eeprom_cmd);
-
-	if (!status) {
-		resp = (struct be_cmd_resp_seeprom_read *) eeprom_cmd.va;
-		memcpy(data, resp->seeprom_data + eeprom->offset, eeprom->len);
-	}
-	pci_free_consistent(adapter->pdev, eeprom_cmd.size, eeprom_cmd.va,
-			eeprom_cmd.dma);
-
-	return status;
-}
-
 const struct ethtool_ops be_ethtool_ops = {
 	.get_settings = be_get_settings,
 	.get_drvinfo = be_get_drvinfo,
 	.get_wol = be_get_wol,
 	.set_wol = be_set_wol,
 	.get_link = ethtool_op_get_link,
-	.get_eeprom_len = be_get_eeprom_len,
-	.get_eeprom = be_read_eeprom,
 	.get_coalesce = be_get_coalesce,
 	.set_coalesce = be_set_coalesce,
 	.get_ringparam = be_get_ringparam,
diff --git a/drivers/net/benet/be_hw.h b/drivers/net/benet/be_hw.h
index 063026d..e2b3bef 100644
--- a/drivers/net/benet/be_hw.h
+++ b/drivers/net/benet/be_hw.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -99,67 +99,6 @@
 /* Number of entries posted */
 #define DB_MCCQ_NUM_POSTED_SHIFT	(16)	/* bits 16 - 29 */
 
-/********** SRIOV VF PCICFG OFFSET ********/
-#define SRIOV_VF_PCICFG_OFFSET		(4096)
-
-/* Flashrom related descriptors */
-#define IMAGE_TYPE_FIRMWARE		160
-#define IMAGE_TYPE_BOOTCODE		224
-#define IMAGE_TYPE_OPTIONROM		32
-
-#define NUM_FLASHDIR_ENTRIES		32
-
-#define IMG_TYPE_ISCSI_ACTIVE		0
-#define IMG_TYPE_REDBOOT		1
-#define IMG_TYPE_BIOS			2
-#define IMG_TYPE_PXE_BIOS		3
-#define IMG_TYPE_FCOE_BIOS		8
-#define IMG_TYPE_ISCSI_BACKUP		9
-#define IMG_TYPE_FCOE_FW_ACTIVE		10
-#define IMG_TYPE_FCOE_FW_BACKUP 	11
-#define IMG_TYPE_NCSI_FW		13
-
-#define FLASHROM_OPER_FLASH		1
-#define FLASHROM_OPER_SAVE		2
-#define FLASHROM_OPER_REPORT		4
-
-#define FLASH_IMAGE_MAX_SIZE_g2            (1310720) /* Max firmware image sz */
-#define FLASH_BIOS_IMAGE_MAX_SIZE_g2       (262144)  /* Max OPTION ROM img sz */
-#define FLASH_REDBOOT_IMAGE_MAX_SIZE_g2	  (262144)  /* Max Redboot image sz */
-#define FLASH_IMAGE_MAX_SIZE_g3            (2097152) /* Max fw image size */
-#define FLASH_BIOS_IMAGE_MAX_SIZE_g3       (524288)  /* Max OPTION ROM img sz */
-#define FLASH_REDBOOT_IMAGE_MAX_SIZE_g3	  (1048576)  /* Max Redboot image sz */
-#define FLASH_NCSI_IMAGE_MAX_SIZE_g3       (262144)  /* Max NSCI image sz */
-
-#define FLASH_NCSI_MAGIC		(0x16032009)
-#define FLASH_NCSI_DISABLED		(0)
-#define FLASH_NCSI_ENABLED		(1)
-
-#define FLASH_NCSI_BITFILE_HDR_OFFSET	(0x600000)
-
-/* Offsets for components on Flash. */
-#define FLASH_iSCSI_PRIMARY_IMAGE_START_g2 (1048576)
-#define FLASH_iSCSI_BACKUP_IMAGE_START_g2  (2359296)
-#define FLASH_FCoE_PRIMARY_IMAGE_START_g2  (3670016)
-#define FLASH_FCoE_BACKUP_IMAGE_START_g2   (4980736)
-#define FLASH_iSCSI_BIOS_START_g2          (7340032)
-#define FLASH_PXE_BIOS_START_g2            (7864320)
-#define FLASH_FCoE_BIOS_START_g2           (524288)
-#define FLASH_REDBOOT_START_g2		  (0)
-
-#define FLASH_NCSI_START_g3		   (15990784)
-#define FLASH_iSCSI_PRIMARY_IMAGE_START_g3 (2097152)
-#define FLASH_iSCSI_BACKUP_IMAGE_START_g3  (4194304)
-#define FLASH_FCoE_PRIMARY_IMAGE_START_g3  (6291456)
-#define FLASH_FCoE_BACKUP_IMAGE_START_g3   (8388608)
-#define FLASH_iSCSI_BIOS_START_g3          (12582912)
-#define FLASH_PXE_BIOS_START_g3            (13107200)
-#define FLASH_FCoE_BIOS_START_g3           (13631488)
-#define FLASH_REDBOOT_START_g3             (262144)
-
-
-
-
 /*
  * BE descriptors: host memory data structures whose formats
  * are hardwired in BE silicon.
@@ -168,7 +107,6 @@
 #define EQ_ENTRY_VALID_MASK 		0x1	/* bit 0 */
 #define EQ_ENTRY_RES_ID_MASK 		0xFFFF	/* bits 16 - 31 */
 #define EQ_ENTRY_RES_ID_SHIFT 		16
-
 struct be_eq_entry {
 	u32 evt;
 };
@@ -283,6 +221,41 @@ struct be_eth_rx_compl {
 	u32 dw[4];
 };
 
+/* Flashrom related descriptors */
+#define IMAGE_TYPE_FIRMWARE		160
+#define IMAGE_TYPE_BOOTCODE		224
+#define IMAGE_TYPE_OPTIONROM		32
+
+#define NUM_FLASHDIR_ENTRIES		32
+
+#define FLASHROM_TYPE_ISCSI_ACTIVE	0
+#define FLASHROM_TYPE_REDBOOT		1
+#define FLASHROM_TYPE_BIOS		2
+#define FLASHROM_TYPE_PXE_BIOS		3
+#define FLASHROM_TYPE_FCOE_BIOS		8
+#define FLASHROM_TYPE_ISCSI_BACKUP	9
+#define FLASHROM_TYPE_FCOE_FW_ACTIVE	10
+#define FLASHROM_TYPE_FCOE_FW_BACKUP 	11
+
+#define FLASHROM_OPER_FLASH		1
+#define FLASHROM_OPER_SAVE		2
+#define FLASHROM_OPER_REPORT		4
+
+#define FLASH_IMAGE_MAX_SIZE            (1310720) /* Max firmware image size */
+#define FLASH_BIOS_IMAGE_MAX_SIZE       (262144)  /* Max OPTION ROM image sz */
+#define FLASH_REDBOOT_IMAGE_MAX_SIZE    (262144)  /* Max redboot image sz */
+
+/* Offsets for components on Flash. */
+#define FLASH_iSCSI_PRIMARY_IMAGE_START (1048576)
+#define FLASH_iSCSI_BACKUP_IMAGE_START  (2359296)
+#define FLASH_FCoE_PRIMARY_IMAGE_START  (3670016)
+#define FLASH_FCoE_BACKUP_IMAGE_START   (4980736)
+#define FLASH_iSCSI_BIOS_START          (7340032)
+#define FLASH_PXE_BIOS_START            (7864320)
+#define FLASH_FCoE_BIOS_START           (524288)
+#define FLASH_REDBOOT_START		(32768)
+#define FLASH_REDBOOT_ISM_START		(0)
+
 struct controller_id {
 	u32 vendor;
 	u32 device;
@@ -290,20 +263,7 @@ struct controller_id {
 	u32 subdevice;
 };
 
-struct flash_comp {
-	unsigned long offset;
-	int optype;
-	int size;
-};
-
-struct image_hdr {
-	u32 imageid;
-	u32 imageoffset;
-	u32 imagelength;
-	u32 image_checksum;
-	u8 image_version[32];
-};
-struct flash_file_hdr_g2 {
+struct flash_file_hdr {
 	u8 sign[32];
 	u32 cksum;
 	u32 antidote;
@@ -315,17 +275,6 @@ struct flash_file_hdr_g2 {
 	u8 build[24];
 };
 
-struct flash_file_hdr_g3 {
-	u8 sign[52];
-	u8 ufi_version[4];
-	u32 file_len;
-	u32 cksum;
-	u32 antidote;
-	u32 num_imgs;
-	u8 build[24];
-	u8 rsvd[32];
-};
-
 struct flash_section_hdr {
 	u32 format_rev;
 	u32 cksum;
diff --git a/drivers/net/benet/be_main.c b/drivers/net/benet/be_main.c
index 54b1427..f700e60 100644
--- a/drivers/net/benet/be_main.c
+++ b/drivers/net/benet/be_main.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2005 - 2010 ServerEngines
+ * Copyright (C) 2005 - 2009 ServerEngines
  * All rights reserved.
  *
  * This program is free software; you can redistribute it and/or
@@ -18,6 +18,8 @@
 #include "be.h"
 #include "be_cmds.h"
 #include <asm/div64.h>
+#include <linux/kthread.h>
+#include <linux/netpoll.h>
 
 MODULE_VERSION(DRV_VER);
 MODULE_DEVICE_TABLE(pci, be_dev_ids);
@@ -26,21 +28,225 @@ MODULE_AUTHOR("ServerEngines Corporation");
 MODULE_LICENSE("GPL");
 
 static unsigned int rx_frag_size = 2048;
-static unsigned int num_vfs;
 module_param(rx_frag_size, uint, S_IRUGO);
-module_param(num_vfs, uint, S_IRUGO);
 MODULE_PARM_DESC(rx_frag_size, "Size of a fragment that holds rcvd data.");
-MODULE_PARM_DESC(num_vfs, "Number of PCI VFs to initialize");
+
+static unsigned int disable_msix=1;
+module_param(disable_msix,uint, 1);
+MODULE_PARM_DESC(disable_msix, "disable msi in device: 0 (default)  or 1");
 
 static DEFINE_PCI_DEVICE_TABLE(be_dev_ids) = {
 	{ PCI_DEVICE(BE_VENDOR_ID, BE_DEVICE_ID1) },
 	{ PCI_DEVICE(BE_VENDOR_ID, BE_DEVICE_ID2) },
 	{ PCI_DEVICE(BE_VENDOR_ID, OC_DEVICE_ID1) },
 	{ PCI_DEVICE(BE_VENDOR_ID, OC_DEVICE_ID2) },
+	{ PCI_DEVICE(BE_VENDOR_ID, OC_DEVICE_ID3) },
 	{ 0 }
 };
 MODULE_DEVICE_TABLE(pci, be_dev_ids);
 
+/* polling stuff */
+struct task_struct *poll_task = NULL;
+int polling_mode_rx_enabled = 0;
+int polling_mode_tx_enabled = 0;
+int polling_thread_rx_active = 0;
+int polling_thread_tx_active = 0;
+struct be_adapter *polling_adapter = NULL;
+
+static int polling_thread(void* arg);
+
+/* polling /proc stuff */
+
+#include <linux/proc_fs.h> /* for exit proc fs */
+static u64 inactive_cycles;
+static u64 active_cycles;
+static u64 d1;
+static u64 d2;
+static u64 d3;
+static u64 d4;
+static u64 d5;
+
+#define POLL_PROC_DIR "be2net"
+static struct proc_dir_entry *be2net_dir, *stat_file;
+static int proc_read_poll(char *buffer,
+              char **buffer_location,
+              off_t offset, int buffer_length, int *eof, void *data)
+{
+        int ret;
+	char temp_buf[1024];
+
+        printk(KERN_INFO "procfile_read (/proc/poll) called\n");
+
+        if (offset > 0) {
+                /* we have finished to read, return 0 */
+                ret = 0;
+        } else {
+		ret = sprintf(temp_buf, "1.active: %llx\n2.inactive: %llx\nd1: %llx\nd2: %llx\nd3: %llx\nd4: %llx\nd5: %llx\n", 
+			active_cycles, inactive_cycles, 
+			d1, d2, d3, d4, d5
+			);
+                memcpy(buffer, temp_buf, ret);
+	}
+
+	return ret;
+}
+
+
+#define POLL_PROC_INPUT_LEN 2000
+static int proc_write_poll(struct file *file,
+			     const char *buffer,
+			     unsigned long count,
+			     void *data)
+{
+	int len;
+	char str[POLL_PROC_INPUT_LEN];
+
+	if (count > POLL_PROC_INPUT_LEN)
+		len = POLL_PROC_INPUT_LEN;
+	else
+		len = count;
+
+	if (copy_from_user(str, buffer, len))
+		return -EFAULT;
+
+	str[len] = '\0';
+
+	switch (str[0]) {
+	case '?':
+		printk("v - view status\n" );
+		printk("1 - enable polling\n" );
+		printk("0 - disable polling\n" );
+		printk("s - start/stop polling thread\n" );
+		printk("r - enable/disable rx polling\n" );
+		printk("t - enable/disable tx polling \n" );
+		break;
+	case 'v':
+		printk("polling rx %s\n", polling_mode_rx_enabled?"enabled":"disabled" );
+		printk("polling tx %s\n", polling_mode_tx_enabled?"enabled":"disabled" );
+		printk("polling thread is %s\n", (poll_task == NULL)?"stopped":"running" );
+
+		break;
+
+	case 's':
+		if (poll_task == NULL) {			
+			poll_task = kthread_run(polling_thread, NULL, "%s", "be2net-polling");
+		} else {
+			printk("polling thread is running - stoping the thread\n");
+			kthread_stop(poll_task);
+			poll_task = NULL;
+		}
+
+		break;
+		
+	case 'r':
+		if (polling_mode_rx_enabled)
+			polling_mode_rx_enabled = 0;
+		else
+			polling_mode_rx_enabled = 1;
+		
+		printk("polling rx %s\n", polling_mode_rx_enabled?"enabled":"disabled" );
+		break;
+		
+	case 't':
+		if (polling_mode_tx_enabled)
+			polling_mode_tx_enabled = 0;
+		else
+			polling_mode_tx_enabled = 1;
+		
+		printk("polling tx %s\n", polling_mode_tx_enabled?"enabled":"disabled" );
+		break;
+
+	case '1':
+		if (polling_mode_tx_enabled || polling_mode_rx_enabled || poll_task != NULL) {		
+			printk("polling already set\n");
+			break;
+		}
+
+		poll_task = kthread_run(polling_thread, NULL, "%s", "be2net-polling");
+
+		msleep(1000);
+		
+		polling_mode_rx_enabled = 1;
+
+		msleep(1000);
+
+		polling_mode_tx_enabled = 1;
+		
+		printk("polling enabled\n");
+		
+		break;
+
+	case '0':
+		if (!polling_mode_tx_enabled || !polling_mode_rx_enabled || poll_task == NULL) {		
+			printk("polling already not set\n");
+			break;
+		}
+		
+		polling_mode_tx_enabled = 0;
+
+		msleep(1000);
+
+		polling_mode_rx_enabled = 0;
+
+		msleep(1000);
+
+		kthread_stop(poll_task);
+		poll_task = NULL;
+
+		printk("polling disabled\n");
+		
+		break;
+	default:
+		printk("unknown option %c\n", str[0]);
+
+	}
+		return len;
+}
+
+static int init_poll_procfs(void)
+{
+	int rv = 0;
+
+	/* create directory */
+	be2net_dir = proc_mkdir(POLL_PROC_DIR, NULL);
+	if (be2net_dir == NULL) {
+		rv = -ENOMEM;
+		goto out;
+	}
+
+	stat_file = create_proc_entry("poll", 0644, be2net_dir);
+	if (stat_file == NULL) {
+		rv = -ENOMEM;
+		goto no_poll;
+	}
+
+	stat_file->data = NULL;
+	stat_file->read_proc = proc_read_poll;
+	stat_file->write_proc = proc_write_poll;
+
+	/* everything OK */
+	printk(KERN_INFO "%s initialized\n",
+	       POLL_PROC_DIR);
+	return 0;
+
+	remove_proc_entry("poll", be2net_dir);
+no_poll:
+	remove_proc_entry(POLL_PROC_DIR, NULL);
+out:
+	return rv;
+}
+
+static void cleanup_procfs(void)
+{
+	remove_proc_entry("poll", be2net_dir);
+	remove_proc_entry(POLL_PROC_DIR, NULL);
+}
+
+
+/* end of proc stuff */
+
+
+
 static void be_queue_free(struct be_adapter *adapter, struct be_queue_info *q)
 {
 	struct be_dma_mem *mem = &q->dma_mem;
@@ -71,9 +277,6 @@ static void be_intr_set(struct be_adapter *adapter, bool enable)
 	u32 reg = ioread32(addr);
 	u32 enabled = reg & MEMBAR_CTRL_INT_CTRL_HOSTINTR_MASK;
 
-	if (adapter->eeh_err)
-		return;
-
 	if (!enabled && enable)
 		reg |= MEMBAR_CTRL_INT_CTRL_HOSTINTR_MASK;
 	else if (enabled && !enable)
@@ -105,10 +308,6 @@ static void be_eq_notify(struct be_adapter *adapter, u16 qid,
 {
 	u32 val = 0;
 	val |= qid & DB_EQ_RING_ID_MASK;
-
-	if (adapter->eeh_err)
-		return;
-
 	if (arm)
 		val |= 1 << DB_EQ_REARM_SHIFT;
 	if (clear_int)
@@ -122,10 +321,6 @@ void be_cq_notify(struct be_adapter *adapter, u16 qid, bool arm, u16 num_popped)
 {
 	u32 val = 0;
 	val |= qid & DB_CQ_RING_ID_MASK;
-
-	if (adapter->eeh_err)
-		return;
-
 	if (arm)
 		val |= 1 << DB_CQ_REARM_SHIFT;
 	val |= num_popped << DB_CQ_NUM_POPPED_SHIFT;
@@ -141,19 +336,12 @@ static int be_mac_addr_set(struct net_device *netdev, void *p)
 	if (!is_valid_ether_addr(addr->sa_data))
 		return -EADDRNOTAVAIL;
 
-	/* MAC addr configuration will be done in hardware for VFs
-	 * by their corresponding PFs. Just copy to netdev addr here
-	 */
-	if (!be_physfn(adapter))
-		goto netdev_addr;
-
 	status = be_cmd_pmac_del(adapter, adapter->if_handle, adapter->pmac_id);
 	if (status)
 		return status;
 
 	status = be_cmd_pmac_add(adapter, (u8 *)addr->sa_data,
 			adapter->if_handle, &adapter->pmac_id);
-netdev_addr:
 	if (!status)
 		memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
 
@@ -169,10 +357,13 @@ void netdev_stats_update(struct be_adapter *adapter)
 	struct net_device_stats *dev_stats = &adapter->netdev->stats;
 	struct be_erx_stats *erx_stats = &hw_stats->erx;
 
-	dev_stats->rx_packets = drvr_stats(adapter)->be_rx_pkts;
-	dev_stats->tx_packets = drvr_stats(adapter)->be_tx_pkts;
-	dev_stats->rx_bytes = drvr_stats(adapter)->be_rx_bytes;
-	dev_stats->tx_bytes = drvr_stats(adapter)->be_tx_bytes;
+	dev_stats->rx_packets = port_stats->rx_total_frames;
+	dev_stats->tx_packets = port_stats->tx_unicastframes +
+		port_stats->tx_multicastframes + port_stats->tx_broadcastframes;
+	dev_stats->rx_bytes = (u64) port_stats->rx_bytes_msd << 32 |
+				(u64) port_stats->rx_bytes_lsd;
+	dev_stats->tx_bytes = (u64) port_stats->tx_bytes_msd << 32 |
+				(u64) port_stats->tx_bytes_lsd;
 
 	/* bad pkts received */
 	dev_stats->rx_errors = port_stats->rx_crc_errors +
@@ -329,13 +520,12 @@ static void be_tx_rate_update(struct be_adapter *adapter)
 }
 
 static void be_tx_stats_update(struct be_adapter *adapter,
-			u32 wrb_cnt, u32 copied, u32 gso_segs, bool stopped)
+			u32 wrb_cnt, u32 copied, bool stopped)
 {
 	struct be_drvr_stats *stats = drvr_stats(adapter);
 	stats->be_tx_reqs++;
 	stats->be_tx_wrbs += wrb_cnt;
 	stats->be_tx_bytes += copied;
-	stats->be_tx_pkts += (gso_segs ? gso_segs : 1);
 	if (stopped)
 		stats->be_tx_stops++;
 }
@@ -396,48 +586,26 @@ static void wrb_fill_hdr(struct be_eth_hdr_wrb *hdr, struct sk_buff *skb,
 	AMAP_SET_BITS(struct amap_eth_hdr_wrb, len, hdr, len);
 }
 
-static void unmap_tx_frag(struct pci_dev *pdev, struct be_eth_wrb *wrb,
-		bool unmap_single)
-{
-	dma_addr_t dma;
-
-	be_dws_le_to_cpu(wrb, sizeof(*wrb));
-
-	dma = (u64)wrb->frag_pa_hi << 32 | (u64)wrb->frag_pa_lo;
-	if (wrb->frag_len) {
-		if (unmap_single)
-			pci_unmap_single(pdev, dma, wrb->frag_len,
-				PCI_DMA_TODEVICE);
-		else
-			pci_unmap_page(pdev, dma, wrb->frag_len,
-				PCI_DMA_TODEVICE);
-	}
-}
 
 static int make_tx_wrbs(struct be_adapter *adapter,
 		struct sk_buff *skb, u32 wrb_cnt, bool dummy_wrb)
 {
-	dma_addr_t busaddr;
-	int i, copied = 0;
+	u64 busaddr;
+	u32 i, copied = 0;
 	struct pci_dev *pdev = adapter->pdev;
 	struct sk_buff *first_skb = skb;
 	struct be_queue_info *txq = &adapter->tx_obj.q;
 	struct be_eth_wrb *wrb;
 	struct be_eth_hdr_wrb *hdr;
-	bool map_single = false;
-	u16 map_head;
 
 	hdr = queue_head_node(txq);
+	atomic_add(wrb_cnt, &txq->used);
 	queue_head_inc(txq);
-	map_head = txq->head;
 
 	if (skb->len > skb->data_len) {
-		int len = skb_headlen(skb);
+		int len = skb->len - skb->data_len;
 		busaddr = pci_map_single(pdev, skb->data, len,
 					 PCI_DMA_TODEVICE);
-		if (pci_dma_mapping_error(pdev, busaddr))
-			goto dma_err;
-		map_single = true;
 		wrb = queue_head_node(txq);
 		wrb_fill(wrb, busaddr, len);
 		be_dws_cpu_to_le(wrb, sizeof(*wrb));
@@ -451,8 +619,6 @@ static int make_tx_wrbs(struct be_adapter *adapter,
 		busaddr = pci_map_page(pdev, frag->page,
 				       frag->page_offset,
 				       frag->size, PCI_DMA_TODEVICE);
-		if (pci_dma_mapping_error(pdev, busaddr))
-			goto dma_err;
 		wrb = queue_head_node(txq);
 		wrb_fill(wrb, busaddr, frag->size);
 		be_dws_cpu_to_le(wrb, sizeof(*wrb));
@@ -472,16 +638,6 @@ static int make_tx_wrbs(struct be_adapter *adapter,
 	be_dws_cpu_to_le(hdr, sizeof(*hdr));
 
 	return copied;
-dma_err:
-	txq->head = map_head;
-	while (copied) {
-		wrb = queue_head_node(txq);
-		unmap_tx_frag(pdev, wrb, map_single);
-		map_single = false;
-		copied -= wrb->frag_len;
-		queue_head_inc(txq);
-	}
-	return 0;
 }
 
 static netdev_tx_t be_xmit(struct sk_buff *skb,
@@ -506,7 +662,6 @@ static netdev_tx_t be_xmit(struct sk_buff *skb,
 		 * *BEFORE* ringing the tx doorbell, so that we serialze the
 		 * tx compls of the current transmit which'll wake up the queue
 		 */
-		atomic_add(wrb_cnt, &txq->used);
 		if ((BE_MAX_TX_FRAG_COUNT + atomic_read(&txq->used)) >=
 								txq->len) {
 			netif_stop_queue(netdev);
@@ -515,8 +670,7 @@ static netdev_tx_t be_xmit(struct sk_buff *skb,
 
 		be_txq_notify(adapter, txq->id, wrb_cnt);
 
-		be_tx_stats_update(adapter, wrb_cnt, copied,
-				skb_shinfo(skb)->gso_segs, stopped);
+		be_tx_stats_update(adapter, wrb_cnt, copied, stopped);
 	} else {
 		txq->head = start;
 		dev_kfree_skb_any(skb);
@@ -528,12 +682,10 @@ static int be_change_mtu(struct net_device *netdev, int new_mtu)
 {
 	struct be_adapter *adapter = netdev_priv(netdev);
 	if (new_mtu < BE_MIN_MTU ||
-			new_mtu > (BE_MAX_JUMBO_FRAME_SIZE -
-					(ETH_HLEN + ETH_FCS_LEN))) {
+			new_mtu > BE_MAX_JUMBO_FRAME_SIZE) {
 		dev_info(&adapter->pdev->dev,
 			"MTU must be between %d and %d bytes\n",
-			BE_MIN_MTU,
-			(BE_MAX_JUMBO_FRAME_SIZE - (ETH_HLEN + ETH_FCS_LEN)));
+			BE_MIN_MTU, BE_MAX_JUMBO_FRAME_SIZE);
 		return -EINVAL;
 	}
 	dev_info(&adapter->pdev->dev, "MTU changed from %d to %d bytes\n",
@@ -543,16 +695,17 @@ static int be_change_mtu(struct net_device *netdev, int new_mtu)
 }
 
 /*
- * A max of 64 (BE_NUM_VLANS_SUPPORTED) vlans can be configured in BE.
- * If the user configures more, place BE in vlan promiscuous mode.
+ * if there are BE_NUM_VLANS_SUPPORTED or lesser number of VLANS configured,
+ * program them in BE.  If more than BE_NUM_VLANS_SUPPORTED are configured,
+ * set the BE in promiscuous VLAN mode.
  */
 static int be_vid_config(struct be_adapter *adapter)
 {
 	u16 vtag[BE_NUM_VLANS_SUPPORTED];
 	u16 ntags = 0, i;
-	int status = 0;
+	int status;
 
-	if (adapter->vlans_added <= adapter->max_vlans)  {
+	if (adapter->num_vlans <= BE_NUM_VLANS_SUPPORTED)  {
 		/* Construct VLAN Table to give to HW */
 		for (i = 0; i < VLAN_GROUP_ARRAY_LEN; i++) {
 			if (adapter->vlan_tag[i]) {
@@ -586,27 +739,21 @@ static void be_vlan_add_vid(struct net_device *netdev, u16 vid)
 {
 	struct be_adapter *adapter = netdev_priv(netdev);
 
-	if (!be_physfn(adapter))
-		return;
-
+	adapter->num_vlans++;
 	adapter->vlan_tag[vid] = 1;
-	adapter->vlans_added++;
-	if (adapter->vlans_added <= (adapter->max_vlans + 1))
-		be_vid_config(adapter);
+
+	be_vid_config(adapter);
 }
 
 static void be_vlan_rem_vid(struct net_device *netdev, u16 vid)
 {
 	struct be_adapter *adapter = netdev_priv(netdev);
 
-	if (!be_physfn(adapter))
-		return;
-
+	adapter->num_vlans--;
 	adapter->vlan_tag[vid] = 0;
+
 	vlan_group_set_device(adapter->vlan_grp, vid, NULL);
-	adapter->vlans_added--;
-	if (adapter->vlans_added <= adapter->max_vlans)
-		be_vid_config(adapter);
+	be_vid_config(adapter);
 }
 
 static void be_set_multicast_list(struct net_device *netdev)
@@ -626,41 +773,19 @@ static void be_set_multicast_list(struct net_device *netdev)
 	}
 
 	/* Enable multicast promisc if num configured exceeds what we support */
-	if (netdev->flags & IFF_ALLMULTI ||
-	    netdev_mc_count(netdev) > BE_MAX_MC) {
-		be_cmd_multicast_set(adapter, adapter->if_handle, NULL,
+	if (netdev->flags & IFF_ALLMULTI || netdev_mc_count(netdev) > BE_MAX_MC) {
+		be_cmd_multicast_set(adapter, adapter->if_handle, NULL, 
 				&adapter->mc_cmd_mem);
 		goto done;
 	}
 
 	be_cmd_multicast_set(adapter, adapter->if_handle, netdev,
 		&adapter->mc_cmd_mem);
+
 done:
 	return;
 }
 
-static int be_set_vf_mac(struct net_device *netdev, int vf, u8 *mac)
-{
-	struct be_adapter *adapter = netdev_priv(netdev);
-	int status;
-
-	if (!adapter->sriov_enabled)
-		return -EPERM;
-
-	if (!is_valid_ether_addr(mac) || (vf >= num_vfs))
-		return -EINVAL;
-
-	status = be_cmd_pmac_del(adapter, adapter->vf_if_handle[vf],
-				adapter->vf_pmac_id[vf]);
-
-	status = be_cmd_pmac_add(adapter, mac, adapter->vf_if_handle[vf],
-				&adapter->vf_pmac_id[vf]);
-	if (!status)
-		dev_err(&adapter->pdev->dev, "MAC %pM set on VF %d Failed\n",
-				mac, vf);
-	return status;
-}
-
 static void be_rx_rate_update(struct be_adapter *adapter)
 {
 	struct be_drvr_stats *stats = drvr_stats(adapter);
@@ -691,7 +816,6 @@ static void be_rx_stats_update(struct be_adapter *adapter,
 	stats->be_rx_compl++;
 	stats->be_rx_frags += numfrags;
 	stats->be_rx_bytes += pktsize;
-	stats->be_rx_pkts++;
 }
 
 static inline bool do_pkt_csum(struct be_eth_rx_compl *rxcp, bool cso)
@@ -719,11 +843,9 @@ get_rx_page_info(struct be_adapter *adapter, u16 frag_idx)
 	rx_page_info = &adapter->rx_obj.page_info_tbl[frag_idx];
 	BUG_ON(!rx_page_info->page);
 
-	if (rx_page_info->last_page_user) {
-		pci_unmap_page(adapter->pdev, dma_unmap_addr(rx_page_info, bus),
+	if (rx_page_info->last_page_user)
+		pci_unmap_page(adapter->pdev, pci_unmap_addr(rx_page_info, bus),
 			adapter->big_page_size, PCI_DMA_FROMDEVICE);
-		rx_page_info->last_page_user = false;
-	}
 
 	atomic_dec(&rxq->used);
 	return rx_page_info;
@@ -753,17 +875,17 @@ static void be_rx_compl_discard(struct be_adapter *adapter,
  * indicated by rxcp.
  */
 static void skb_fill_rx_data(struct be_adapter *adapter,
-			struct sk_buff *skb, struct be_eth_rx_compl *rxcp,
-			u16 num_rcvd)
+			struct sk_buff *skb, struct be_eth_rx_compl *rxcp)
 {
 	struct be_queue_info *rxq = &adapter->rx_obj.q;
 	struct be_rx_page_info *page_info;
-	u16 rxq_idx, i, j;
+	u16 rxq_idx, i, num_rcvd, j;
 	u32 pktsize, hdr_len, curr_frag_len, size;
 	u8 *start;
 
 	rxq_idx = AMAP_GET_BITS(struct amap_eth_rx_compl, fragndx, rxcp);
 	pktsize = AMAP_GET_BITS(struct amap_eth_rx_compl, pktsize, rxcp);
+	num_rcvd = AMAP_GET_BITS(struct amap_eth_rx_compl, numfrags, rxcp);
 
 	page_info = get_rx_page_info(adapter, rxq_idx);
 
@@ -791,7 +913,7 @@ static void skb_fill_rx_data(struct be_adapter *adapter,
 		skb->data_len = curr_frag_len - hdr_len;
 		skb->tail += hdr_len;
 	}
-	page_info->page = NULL;
+	memset(page_info, 0, sizeof(*page_info));
 
 	if (pktsize <= rx_frag_size) {
 		BUG_ON(num_rcvd != 1);
@@ -824,12 +946,13 @@ static void skb_fill_rx_data(struct be_adapter *adapter,
 		skb->len += curr_frag_len;
 		skb->data_len += curr_frag_len;
 
-		page_info->page = NULL;
+		memset(page_info, 0, sizeof(*page_info));
 	}
 	BUG_ON(j > MAX_SKB_FRAGS);
 
 done:
 	be_rx_stats_update(adapter, pktsize, num_rcvd);
+	return;
 }
 
 /* Process the RX completion indicated by rxcp when GRO is disabled */
@@ -838,23 +961,25 @@ static void be_rx_compl_process(struct be_adapter *adapter,
 {
 	struct sk_buff *skb;
 	u32 vlanf, vid;
-	u16 num_rcvd;
 	u8 vtm;
 
-	num_rcvd = AMAP_GET_BITS(struct amap_eth_rx_compl, numfrags, rxcp);
-	/* Is it a flush compl that has no data */
-	if (unlikely(num_rcvd == 0))
-		return;
+	vlanf = AMAP_GET_BITS(struct amap_eth_rx_compl, vtp, rxcp);
+	vtm = AMAP_GET_BITS(struct amap_eth_rx_compl, vtm, rxcp);
+
+	/* vlanf could be wrongly set in some cards.
+	 * ignore if vtm is not set */
+	if ((adapter->cap & 0x400) && !vtm)
+		vlanf = 0;
 
 	skb = netdev_alloc_skb_ip_align(adapter->netdev, BE_HDR_LEN);
-	if (unlikely(!skb)) {
+	if (!skb) {
 		if (net_ratelimit())
 			dev_warn(&adapter->pdev->dev, "skb alloc failed\n");
 		be_rx_compl_discard(adapter, rxcp);
 		return;
 	}
 
-	skb_fill_rx_data(adapter, skb, rxcp, num_rcvd);
+	skb_fill_rx_data(adapter, skb, rxcp);
 
 	if (do_pkt_csum(rxcp, adapter->rx_csum))
 		skb->ip_summed = CHECKSUM_NONE;
@@ -863,26 +988,21 @@ static void be_rx_compl_process(struct be_adapter *adapter,
 
 	skb->truesize = skb->len + sizeof(struct sk_buff);
 	skb->protocol = eth_type_trans(skb, adapter->netdev);
+	skb->dev = adapter->netdev;
 
-	vlanf = AMAP_GET_BITS(struct amap_eth_rx_compl, vtp, rxcp);
-	vtm = AMAP_GET_BITS(struct amap_eth_rx_compl, vtm, rxcp);
-
-	/* vlanf could be wrongly set in some cards.
-	 * ignore if vtm is not set */
-	if ((adapter->cap & 0x400) && !vtm)
-		vlanf = 0;
-
-	if (unlikely(vlanf)) {
-		if (!adapter->vlan_grp || adapter->vlans_added == 0) {
+	if (vlanf) {
+		if (!adapter->vlan_grp || adapter->num_vlans == 0) {
 			kfree_skb(skb);
 			return;
 		}
 		vid = AMAP_GET_BITS(struct amap_eth_rx_compl, vlan_tag, rxcp);
-		vid = swab16(vid);
+		vid = be16_to_cpu(vid);
 		vlan_hwaccel_receive_skb(skb, adapter->vlan_grp, vid);
 	} else {
 		netif_receive_skb(skb);
 	}
+
+	return;
 }
 
 /* Process the RX completion indicated by rxcp when GRO is enabled */
@@ -898,10 +1018,6 @@ static void be_rx_compl_process_gro(struct be_adapter *adapter,
 	u8 vtm;
 
 	num_rcvd = AMAP_GET_BITS(struct amap_eth_rx_compl, numfrags, rxcp);
-	/* Is it a flush compl that has no data */
-	if (unlikely(num_rcvd == 0))
-		return;
-
 	pkt_size = AMAP_GET_BITS(struct amap_eth_rx_compl, pktsize, rxcp);
 	vlanf = AMAP_GET_BITS(struct amap_eth_rx_compl, vtp, rxcp);
 	rxq_idx = AMAP_GET_BITS(struct amap_eth_rx_compl, fragndx, rxcp);
@@ -953,15 +1069,16 @@ static void be_rx_compl_process_gro(struct be_adapter *adapter,
 		napi_gro_frags(&eq_obj->napi);
 	} else {
 		vid = AMAP_GET_BITS(struct amap_eth_rx_compl, vlan_tag, rxcp);
-		vid = swab16(vid);
+		vid = be16_to_cpu(vid);
 
-		if (!adapter->vlan_grp || adapter->vlans_added == 0)
+		if (!adapter->vlan_grp || adapter->num_vlans == 0)
 			return;
 
 		vlan_gro_frags(&eq_obj->napi, adapter->vlan_grp, vid);
 	}
 
 	be_rx_stats_update(adapter, pkt_size, num_rcvd);
+	return;
 }
 
 static struct be_eth_rx_compl *be_rx_compl_get(struct be_adapter *adapter)
@@ -1027,7 +1144,7 @@ static void be_post_rx_frags(struct be_adapter *adapter)
 		}
 		page_offset = page_info->page_offset;
 		page_info->page = pagep;
-		dma_unmap_addr_set(page_info, bus, page_dmaaddr);
+		pci_unmap_addr_set(page_info, bus, page_dmaaddr);
 		frag_dmaaddr = page_dmaaddr + page_info->page_offset;
 
 		rxd = queue_head_node(rxq);
@@ -1055,6 +1172,8 @@ static void be_post_rx_frags(struct be_adapter *adapter)
 		/* Let be_worker replenish when memory is available */
 		adapter->rx_post_starved = true;
 	}
+
+	return;
 }
 
 static struct be_eth_tx_compl *be_tx_compl_get(struct be_queue_info *tx_cq)
@@ -1078,26 +1197,35 @@ static void be_tx_compl_process(struct be_adapter *adapter, u16 last_index)
 	struct be_eth_wrb *wrb;
 	struct sk_buff **sent_skbs = adapter->tx_obj.sent_skb_list;
 	struct sk_buff *sent_skb;
-	u16 cur_index, num_wrbs = 1; /* account for hdr wrb */
-	bool unmap_skb_hdr = true;
+	u64 busaddr;
+	u16 cur_index, num_wrbs = 0;
 
-	sent_skb = sent_skbs[txq->tail];
+	cur_index = txq->tail;
+	sent_skb = sent_skbs[cur_index];
 	BUG_ON(!sent_skb);
-	sent_skbs[txq->tail] = NULL;
-
-	/* skip header wrb */
+	sent_skbs[cur_index] = NULL;
+	wrb = queue_tail_node(txq);
+	be_dws_le_to_cpu(wrb, sizeof(*wrb));
+	busaddr = ((u64)wrb->frag_pa_hi << 32) | (u64)wrb->frag_pa_lo;
+	if (busaddr != 0) {
+		pci_unmap_single(adapter->pdev, busaddr,
+				 wrb->frag_len, PCI_DMA_TODEVICE);
+	}
+	num_wrbs++;
 	queue_tail_inc(txq);
 
-	do {
+	while (cur_index != last_index) {
 		cur_index = txq->tail;
 		wrb = queue_tail_node(txq);
-		unmap_tx_frag(adapter->pdev, wrb, (unmap_skb_hdr &&
-					skb_headlen(sent_skb)));
-		unmap_skb_hdr = false;
-
+		be_dws_le_to_cpu(wrb, sizeof(*wrb));
+		busaddr = ((u64)wrb->frag_pa_hi << 32) | (u64)wrb->frag_pa_lo;
+		if (busaddr != 0) {
+			pci_unmap_page(adapter->pdev, busaddr,
+				       wrb->frag_len, PCI_DMA_TODEVICE);
+		}
 		num_wrbs++;
 		queue_tail_inc(txq);
-	} while (cur_index != last_index);
+	}
 
 	atomic_sub(num_wrbs, &txq->used);
 
@@ -1166,7 +1294,10 @@ static void be_rx_q_clean(struct be_adapter *adapter)
 	while ((rxcp = be_rx_compl_get(adapter)) != NULL) {
 		be_rx_compl_discard(adapter, rxcp);
 		be_rx_compl_reset(rxcp);
-		be_cq_notify(adapter, rx_cq->id, true, 1);
+		if (!polling_mode_rx_enabled)
+			be_cq_notify(adapter, rx_cq->id, true, 1);
+		else
+			be_cq_notify(adapter, rx_cq->id, false, 1);
 	}
 
 	/* Then free posted rx buffer that were not used */
@@ -1185,9 +1316,6 @@ static void be_tx_compl_clean(struct be_adapter *adapter)
 	struct be_queue_info *txq = &adapter->tx_obj.q;
 	struct be_eth_tx_compl *txcp;
 	u16 end_idx, cmpl = 0, timeo = 0;
-	struct sk_buff **sent_skbs = adapter->tx_obj.sent_skb_list;
-	struct sk_buff *sent_skb;
-	bool dummy_wrb;
 
 	/* Wait for a max of 200ms for all the tx-completions to arrive. */
 	do {
@@ -1211,15 +1339,6 @@ static void be_tx_compl_clean(struct be_adapter *adapter)
 	if (atomic_read(&txq->used))
 		dev_err(&adapter->pdev->dev, "%d pending tx-completions\n",
 			atomic_read(&txq->used));
-
-	/* free posted tx for which compls will never arrive */
-	while (atomic_read(&txq->used)) {
-		sent_skb = sent_skbs[txq->tail];
-		end_idx = txq->tail;
-		index_adv(&end_idx,
-			wrb_cnt_for_skb(sent_skb, &dummy_wrb) - 1, txq->len);
-		be_tx_compl_process(adapter, end_idx);
-	}
 }
 
 static void be_mcc_queues_destroy(struct be_adapter *adapter)
@@ -1312,8 +1431,6 @@ static int be_tx_queues_create(struct be_adapter *adapter)
 	/* Ask BE to create Tx Event queue */
 	if (be_cmd_eq_create(adapter, eq, adapter->tx_eq.cur_eqd))
 		goto tx_eq_free;
-	adapter->base_eq_id = adapter->tx_eq.q.id;
-
 	/* Alloc TX eth compl queue */
 	cq = &adapter->tx_obj.cq;
 	if (be_queue_alloc(adapter, cq, TX_CQ_LEN,
@@ -1354,11 +1471,6 @@ static void be_rx_queues_destroy(struct be_adapter *adapter)
 	q = &adapter->rx_obj.q;
 	if (q->created) {
 		be_cmd_q_destroy(adapter, q, QTYPE_RXQ);
-
-		/* After the rxq is invalidated, wait for a grace time
-		 * of 1ms for all dma to end and the flush compl to arrive
-		 */
-		mdelay(1);
 		be_rx_q_clean(adapter);
 	}
 	be_queue_free(adapter, q);
@@ -1441,7 +1553,7 @@ rx_eq_free:
 /* There are 8 evt ids per func. Retruns the evt id's bit number */
 static inline int be_evt_bit_get(struct be_adapter *adapter, u32 eq_id)
 {
-	return eq_id - adapter->base_eq_id;
+	return eq_id - 8 * be_pci_func(adapter);
 }
 
 static irqreturn_t be_intx(int irq, void *dev)
@@ -1490,11 +1602,9 @@ static inline bool do_gro(struct be_adapter *adapter,
 	return (tcp_frame && !err) ? true : false;
 }
 
-int be_poll_rx(struct napi_struct *napi, int budget)
+
+u32 _be_poll_rx(struct napi_struct *napi, struct be_adapter *adapter, int budget)
 {
-	struct be_eq_obj *rx_eq = container_of(napi, struct be_eq_obj, napi);
-	struct be_adapter *adapter =
-		container_of(rx_eq, struct be_adapter, rx_eq);
 	struct be_queue_info *rx_cq = &adapter->rx_obj.cq;
 	struct be_eth_rx_compl *rxcp;
 	u32 work_done;
@@ -1518,48 +1628,72 @@ int be_poll_rx(struct napi_struct *napi, int budget)
 		be_post_rx_frags(adapter);
 
 	/* All consumed */
-	if (work_done < budget) {
+	if (work_done < budget && !polling_thread_rx_active) {
 		napi_complete(napi);
-		be_cq_notify(adapter, rx_cq->id, true, work_done);
+		if (!polling_mode_rx_enabled) {
+			be_cq_notify(adapter, rx_cq->id, true, work_done);
+		} else {
+			be_cq_notify(adapter, rx_cq->id, false, work_done);
+			if (polling_mode_rx_enabled) {
+				if (polling_adapter == NULL)
+					polling_adapter = adapter;
+				else if (polling_adapter != adapter)
+					printk("???? got polling request for a different adapter...\n");
+				printk("activating thread for rx\n");
+				polling_thread_rx_active = 1;
+			}
+		}
+			
 	} else {
 		/* More to be consumed; continue with interrupts disabled */
 		be_cq_notify(adapter, rx_cq->id, false, work_done);
+/*		if (polling_thread_rx_active)
+			work_done = 1;*/
 	}
 	return work_done;
 }
 
-/* As TX and MCC share the same EQ check for both TX and MCC completions.
- * For TX/MCC we don't honour budget; consume everything
- */
-static int be_poll_tx_mcc(struct napi_struct *napi, int budget)
+int be_poll_rx(struct napi_struct *napi, int budget)
 {
-	struct be_eq_obj *tx_eq = container_of(napi, struct be_eq_obj, napi);
+	struct be_eq_obj *rx_eq = container_of(napi, struct be_eq_obj, napi);
 	struct be_adapter *adapter =
-		container_of(tx_eq, struct be_adapter, tx_eq);
+		container_of(rx_eq, struct be_adapter, rx_eq);
+	u32 work_done = 0;
+
+	if (polling_thread_rx_active) {
+		napi_complete(napi);
+		printk("rx napi called in polling mode\n");
+		return 0;
+	}
+
+	work_done = _be_poll_rx(napi, adapter, budget);
+	if (polling_thread_rx_active)
+		return 1;
+	return work_done;
+}
+
+
+
+u32 be_process_tx(struct be_adapter *adapter)
+{
 	struct be_queue_info *txq = &adapter->tx_obj.q;
 	struct be_queue_info *tx_cq = &adapter->tx_obj.cq;
 	struct be_eth_tx_compl *txcp;
-	int tx_compl = 0, mcc_compl, status = 0;
+	u32 num_cmpl = 0;
 	u16 end_idx;
 
 	while ((txcp = be_tx_compl_get(tx_cq))) {
 		end_idx = AMAP_GET_BITS(struct amap_eth_tx_compl,
-				wrb_index, txcp);
+					wrb_index, txcp);
 		be_tx_compl_process(adapter, end_idx);
-		tx_compl++;
-	}
-
-	mcc_compl = be_process_mcc(adapter, &status);
-
-	napi_complete(napi);
-
-	if (mcc_compl) {
-		struct be_mcc_obj *mcc_obj = &adapter->mcc_obj;
-		be_cq_notify(adapter, mcc_obj->cq.id, true, mcc_compl);
+		num_cmpl++;
 	}
 
-	if (tx_compl) {
-		be_cq_notify(adapter, adapter->tx_obj.cq.id, true, tx_compl);
+	if (num_cmpl) {
+		if (polling_mode_tx_enabled)
+			be_cq_notify(adapter, tx_cq->id, false, num_cmpl);
+		else
+			be_cq_notify(adapter, tx_cq->id, true, num_cmpl);
 
 		/* As Tx wrbs have been freed up, wake up netdev queue if
 		 * it was stopped due to lack of tx wrbs.
@@ -1570,12 +1704,117 @@ static int be_poll_tx_mcc(struct napi_struct *napi, int budget)
 		}
 
 		drvr_stats(adapter)->be_tx_events++;
-		drvr_stats(adapter)->be_tx_compl += tx_compl;
+		drvr_stats(adapter)->be_tx_compl += num_cmpl;
 	}
 
+	return num_cmpl;
+}
+
+/* As TX and MCC share the same EQ check for both TX and MCC completions.
+ * For TX/MCC we don't honour budget; consume everything
+ */
+static int _be_poll_tx_mcc(struct napi_struct *napi, struct be_adapter *adapter, int budget)
+{
+	napi_complete(napi);
+
+	be_process_tx(adapter);
+
+	be_process_mcc(adapter);
+
+	if (polling_mode_tx_enabled) {
+		polling_thread_tx_active = 1;
+
+		if (polling_adapter == NULL)
+			polling_adapter = adapter;
+		else if (polling_adapter != adapter)
+			printk("???? got polling request for a different adapter...\n");
+	}
+	
 	return 1;
 }
 
+static int be_poll_tx_mcc(struct napi_struct *napi, int budget)
+{
+	struct be_eq_obj *tx_eq = container_of(napi, struct be_eq_obj, napi);
+	struct be_adapter *adapter =
+		container_of(tx_eq, struct be_adapter, tx_eq);
+	
+	if (polling_thread_tx_active) {
+		napi_complete(napi);
+		// printk("tx napi called in polling mode\n");
+		return 0;
+	}
+
+	return _be_poll_tx_mcc(napi, adapter, budget);
+}
+
+static int polling_thread(void* arg)
+{
+	u32 work_done, temp;
+	printk("polling thread started\n");
+	cycles_t cur_cycles = get_cycles();
+	cycles_t prev_cycles;
+	struct softnet_data *sd = &__get_cpu_var(softnet_data);
+/*	struct napi_struct *n;
+	n = list_first_entry(&sd->poll_list, struct napi_struct, poll_list);
+	BUG_ON(!n);
+	void* have;
+	BUG_ON(!have);
+*/
+	while(1) {
+		work_done = 0;
+		if (polling_thread_rx_active) {
+			if (!polling_mode_rx_enabled) {
+				/* need to stop polling rx */
+				struct be_queue_info *rx_cq = &polling_adapter->rx_obj.cq;
+				be_cq_notify(polling_adapter, rx_cq->id, true, 0);
+				polling_thread_rx_active = 0;
+			} else {
+/*				have = netpoll_poll_lock(n);*/
+				temp = _be_poll_rx(NULL, polling_adapter, 10);
+/*				netpoll_poll_unlock(have);*/
+				work_done += temp;
+			}
+		}
+		
+		if (polling_thread_tx_active) {
+			if (!polling_mode_tx_enabled) {
+				/* need to stop polling tx */
+				struct be_queue_info *tx_cq = &polling_adapter->tx_obj.cq;
+				be_cq_notify(polling_adapter, tx_cq->id, true, 0);
+				polling_thread_tx_active = 0;
+			} else {
+				temp = be_process_tx(polling_adapter);
+				work_done += temp;
+				be_process_mcc(polling_adapter);
+			}
+		}
+
+		prev_cycles = cur_cycles;
+		cur_cycles = get_cycles();
+		if (!work_done) {
+			inactive_cycles += (cur_cycles - prev_cycles); 
+		} else {
+			active_cycles += (cur_cycles - prev_cycles);
+		}
+
+		if (need_resched()) {
+                        schedule();
+                        prev_cycles = cur_cycles;
+                        cur_cycles = get_cycles();
+                        active_cycles += (cur_cycles - prev_cycles);
+		}
+
+		if (kthread_should_stop())
+			break;
+
+	}
+
+	return 0;
+}
+
+
+
 static void be_worker(struct work_struct *work)
 {
 	struct be_adapter *adapter =
@@ -1616,27 +1855,7 @@ static void be_msix_enable(struct be_adapter *adapter)
 		BE_NUM_MSIX_VECTORS);
 	if (status == 0)
 		adapter->msix_enabled = true;
-}
-
-static void be_sriov_enable(struct be_adapter *adapter)
-{
-#ifdef CONFIG_PCI_IOV
-	int status;
-	if (be_physfn(adapter) && num_vfs) {
-		status = pci_enable_sriov(adapter->pdev, num_vfs);
-		adapter->sriov_enabled = status ? false : true;
-	}
-#endif
-}
-
-static void be_sriov_disable(struct be_adapter *adapter)
-{
-#ifdef CONFIG_PCI_IOV
-	if (adapter->sriov_enabled) {
-		pci_disable_sriov(adapter->pdev);
-		adapter->sriov_enabled = false;
-	}
-#endif
+	return;
 }
 
 static inline int be_msix_vec_get(struct be_adapter *adapter, u32 eq_id)
@@ -1696,9 +1915,6 @@ static int be_irq_register(struct be_adapter *adapter)
 		status = be_msix_register(adapter);
 		if (status == 0)
 			goto done;
-		/* INTx is not supported for VF */
-		if (!be_physfn(adapter))
-			return status;
 	}
 
 	/* INTx */
@@ -1733,6 +1949,7 @@ static void be_irq_unregister(struct be_adapter *adapter)
 	be_free_irq(adapter, &adapter->rx_eq);
 done:
 	adapter->isr_registered = false;
+	return;
 }
 
 static int be_open(struct net_device *netdev)
@@ -1762,28 +1979,24 @@ static int be_open(struct net_device *netdev)
 	/* Rx compl queue may be in unarmed state; rearm it */
 	be_cq_notify(adapter, adapter->rx_obj.cq.id, true, 0);
 
-	/* Now that interrupts are on we can process async mcc */
-	be_async_mcc_enable(adapter);
-
 	status = be_cmd_link_status_query(adapter, &link_up, &mac_speed,
 			&link_speed);
 	if (status)
 		goto ret_sts;
 	be_link_status_update(adapter, link_up);
 
-	if (be_physfn(adapter))
-		status = be_vid_config(adapter);
+	status = be_vid_config(adapter);
 	if (status)
 		goto ret_sts;
 
-	if (be_physfn(adapter)) {
-		status = be_cmd_set_flow_control(adapter,
-				adapter->tx_fc, adapter->rx_fc);
-		if (status)
-			goto ret_sts;
-	}
+	status = be_cmd_set_flow_control(adapter,
+					adapter->tx_fc, adapter->rx_fc);
+	if (status)
+		goto ret_sts;
 
 	schedule_delayed_work(&adapter->work, msecs_to_jiffies(100));
+
+		
 ret_sts:
 	return status;
 }
@@ -1807,7 +2020,7 @@ static int be_setup_wol(struct be_adapter *adapter, bool enable)
 			PCICFG_PM_CONTROL_OFFSET, PCICFG_PM_CONTROL_MASK);
 		if (status) {
 			dev_err(&adapter->pdev->dev,
-				"Could not enable Wake-on-lan\n");
+				"Could not enable Wake-on-lan \n");
 			pci_free_consistent(adapter->pdev, cmd.size, cmd.va,
 					cmd.dma);
 			return status;
@@ -1829,48 +2042,22 @@ static int be_setup_wol(struct be_adapter *adapter, bool enable)
 static int be_setup(struct be_adapter *adapter)
 {
 	struct net_device *netdev = adapter->netdev;
-	u32 cap_flags, en_flags, vf = 0;
+	u32 cap_flags, en_flags;
 	int status;
-	u8 mac[ETH_ALEN];
-
-	cap_flags = en_flags = BE_IF_FLAGS_UNTAGGED | BE_IF_FLAGS_BROADCAST;
 
-	if (be_physfn(adapter)) {
-		cap_flags |= BE_IF_FLAGS_MCAST_PROMISCUOUS |
-				BE_IF_FLAGS_PROMISCUOUS |
-				BE_IF_FLAGS_PASS_L3L4_ERRORS;
-		en_flags |= BE_IF_FLAGS_PASS_L3L4_ERRORS;
-	}
+	cap_flags = BE_IF_FLAGS_UNTAGGED | BE_IF_FLAGS_BROADCAST |
+			BE_IF_FLAGS_MCAST_PROMISCUOUS |
+			BE_IF_FLAGS_PROMISCUOUS |
+			BE_IF_FLAGS_PASS_L3L4_ERRORS;
+	en_flags = BE_IF_FLAGS_UNTAGGED | BE_IF_FLAGS_BROADCAST |
+			BE_IF_FLAGS_PASS_L3L4_ERRORS;
 
 	status = be_cmd_if_create(adapter, cap_flags, en_flags,
 			netdev->dev_addr, false/* pmac_invalid */,
-			&adapter->if_handle, &adapter->pmac_id, 0);
+			&adapter->if_handle, &adapter->pmac_id);
 	if (status != 0)
 		goto do_none;
 
-	if (be_physfn(adapter)) {
-		while (vf < num_vfs) {
-			cap_flags = en_flags = BE_IF_FLAGS_UNTAGGED
-					| BE_IF_FLAGS_BROADCAST;
-			status = be_cmd_if_create(adapter, cap_flags, en_flags,
-					mac, true, &adapter->vf_if_handle[vf],
-					NULL, vf+1);
-			if (status) {
-				dev_err(&adapter->pdev->dev,
-				"Interface Create failed for VF %d\n", vf);
-				goto if_destroy;
-			}
-			vf++;
-		}
-	} else if (!be_physfn(adapter)) {
-		status = be_cmd_mac_addr_query(adapter, mac,
-			MAC_ADDRESS_TYPE_NETWORK, false, adapter->if_handle);
-		if (!status) {
-			memcpy(adapter->netdev->dev_addr, mac, ETH_ALEN);
-			memcpy(adapter->netdev->perm_addr, mac, ETH_ALEN);
-		}
-	}
-
 	status = be_tx_queues_create(adapter);
 	if (status != 0)
 		goto if_destroy;
@@ -1892,9 +2079,6 @@ rx_qs_destroy:
 tx_qs_destroy:
 	be_tx_queues_destroy(adapter);
 if_destroy:
-	for (vf = 0; vf < num_vfs; vf++)
-		if (adapter->vf_if_handle[vf])
-			be_cmd_if_destroy(adapter, adapter->vf_if_handle[vf]);
 	be_cmd_if_destroy(adapter, adapter->if_handle);
 do_none:
 	return status;
@@ -1922,8 +2106,6 @@ static int be_close(struct net_device *netdev)
 
 	cancel_delayed_work_sync(&adapter->work);
 
-	be_async_mcc_disable(adapter);
-
 	netif_stop_queue(netdev);
 	netif_carrier_off(netdev);
 	adapter->link_up = false;
@@ -1947,7 +2129,7 @@ static int be_close(struct net_device *netdev)
 	 * all tx skbs are freed.
 	 */
 	be_tx_compl_clean(adapter);
-
+	
 	return 0;
 }
 
@@ -1956,19 +2138,15 @@ char flash_cookie[2][16] =	{"*** SE FLAS",
 				"H DIRECTORY *** "};
 
 static bool be_flash_redboot(struct be_adapter *adapter,
-			const u8 *p, u32 img_start, int image_size,
-			int hdr_size)
+			const u8 *p)
 {
 	u32 crc_offset;
 	u8 flashed_crc[4];
 	int status;
-
-	crc_offset = hdr_size + img_start + image_size - 4;
-
+	crc_offset = FLASH_REDBOOT_START + FLASH_REDBOOT_IMAGE_MAX_SIZE - 4
+			+ sizeof(struct flash_file_hdr) - 32*1024;
 	p += crc_offset;
-
-	status = be_cmd_get_flash_crc(adapter, flashed_crc,
-			(image_size - 4));
+	status = be_cmd_get_flash_crc(adapter, flashed_crc);
 	if (status) {
 		dev_err(&adapter->pdev->dev,
 		"could not get crc from flash, not flashing redboot\n");
@@ -1980,133 +2158,112 @@ static bool be_flash_redboot(struct be_adapter *adapter,
 		return false;
 	else
 		return true;
+
 }
 
-static int be_flash_data(struct be_adapter *adapter,
+static int be_flash_image(struct be_adapter *adapter,
 			const struct firmware *fw,
-			struct be_dma_mem *flash_cmd, int num_of_images)
-
+			struct be_dma_mem *flash_cmd, u32 flash_type)
 {
-	int status = 0, i, filehdr_size = 0;
-	u32 total_bytes = 0, flash_op;
+	int status;
+	u32 flash_op, image_offset = 0, total_bytes, image_size = 0;
 	int num_bytes;
 	const u8 *p = fw->data;
 	struct be_cmd_write_flashrom *req = flash_cmd->va;
-	struct flash_comp *pflashcomp;
-	int num_comp;
-
-	struct flash_comp gen3_flash_types[9] = {
-		{ FLASH_iSCSI_PRIMARY_IMAGE_START_g3, IMG_TYPE_ISCSI_ACTIVE,
-			FLASH_IMAGE_MAX_SIZE_g3},
-		{ FLASH_REDBOOT_START_g3, IMG_TYPE_REDBOOT,
-			FLASH_REDBOOT_IMAGE_MAX_SIZE_g3},
-		{ FLASH_iSCSI_BIOS_START_g3, IMG_TYPE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g3},
-		{ FLASH_PXE_BIOS_START_g3, IMG_TYPE_PXE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g3},
-		{ FLASH_FCoE_BIOS_START_g3, IMG_TYPE_FCOE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g3},
-		{ FLASH_iSCSI_BACKUP_IMAGE_START_g3, IMG_TYPE_ISCSI_BACKUP,
-			FLASH_IMAGE_MAX_SIZE_g3},
-		{ FLASH_FCoE_PRIMARY_IMAGE_START_g3, IMG_TYPE_FCOE_FW_ACTIVE,
-			FLASH_IMAGE_MAX_SIZE_g3},
-		{ FLASH_FCoE_BACKUP_IMAGE_START_g3, IMG_TYPE_FCOE_FW_BACKUP,
-			FLASH_IMAGE_MAX_SIZE_g3},
-		{ FLASH_NCSI_START_g3, IMG_TYPE_NCSI_FW,
-			FLASH_NCSI_IMAGE_MAX_SIZE_g3}
-	};
-	struct flash_comp gen2_flash_types[8] = {
-		{ FLASH_iSCSI_PRIMARY_IMAGE_START_g2, IMG_TYPE_ISCSI_ACTIVE,
-			FLASH_IMAGE_MAX_SIZE_g2},
-		{ FLASH_REDBOOT_START_g2, IMG_TYPE_REDBOOT,
-			FLASH_REDBOOT_IMAGE_MAX_SIZE_g2},
-		{ FLASH_iSCSI_BIOS_START_g2, IMG_TYPE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g2},
-		{ FLASH_PXE_BIOS_START_g2, IMG_TYPE_PXE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g2},
-		{ FLASH_FCoE_BIOS_START_g2, IMG_TYPE_FCOE_BIOS,
-			FLASH_BIOS_IMAGE_MAX_SIZE_g2},
-		{ FLASH_iSCSI_BACKUP_IMAGE_START_g2, IMG_TYPE_ISCSI_BACKUP,
-			FLASH_IMAGE_MAX_SIZE_g2},
-		{ FLASH_FCoE_PRIMARY_IMAGE_START_g2, IMG_TYPE_FCOE_FW_ACTIVE,
-			FLASH_IMAGE_MAX_SIZE_g2},
-		{ FLASH_FCoE_BACKUP_IMAGE_START_g2, IMG_TYPE_FCOE_FW_BACKUP,
-			 FLASH_IMAGE_MAX_SIZE_g2}
-	};
-
-	if (adapter->generation == BE_GEN3) {
-		pflashcomp = gen3_flash_types;
-		filehdr_size = sizeof(struct flash_file_hdr_g3);
-		num_comp = 9;
-	} else {
-		pflashcomp = gen2_flash_types;
-		filehdr_size = sizeof(struct flash_file_hdr_g2);
-		num_comp = 8;
-	}
-	for (i = 0; i < num_comp; i++) {
-		if ((pflashcomp[i].optype == IMG_TYPE_NCSI_FW) &&
-				memcmp(adapter->fw_ver, "3.102.148.0", 11) < 0)
-			continue;
-		if ((pflashcomp[i].optype == IMG_TYPE_REDBOOT) &&
-			(!be_flash_redboot(adapter, fw->data,
-			 pflashcomp[i].offset, pflashcomp[i].size,
-			 filehdr_size)))
-			continue;
-		p = fw->data;
-		p += filehdr_size + pflashcomp[i].offset
-			+ (num_of_images * sizeof(struct image_hdr));
-	if (p + pflashcomp[i].size > fw->data + fw->size)
+
+	switch (flash_type) {
+	case FLASHROM_TYPE_ISCSI_ACTIVE:
+		image_offset = FLASH_iSCSI_PRIMARY_IMAGE_START;
+		image_size = FLASH_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_ISCSI_BACKUP:
+		image_offset = FLASH_iSCSI_BACKUP_IMAGE_START;
+		image_size = FLASH_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_FCOE_FW_ACTIVE:
+		image_offset = FLASH_FCoE_PRIMARY_IMAGE_START;
+		image_size = FLASH_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_FCOE_FW_BACKUP:
+		image_offset = FLASH_FCoE_BACKUP_IMAGE_START;
+		image_size = FLASH_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_BIOS:
+		image_offset = FLASH_iSCSI_BIOS_START;
+		image_size = FLASH_BIOS_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_FCOE_BIOS:
+		image_offset = FLASH_FCoE_BIOS_START;
+		image_size = FLASH_BIOS_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_PXE_BIOS:
+		image_offset = FLASH_PXE_BIOS_START;
+		image_size = FLASH_BIOS_IMAGE_MAX_SIZE;
+		break;
+	case FLASHROM_TYPE_REDBOOT:
+		if (!be_flash_redboot(adapter, fw->data))
+			return 0;
+		image_offset = FLASH_REDBOOT_ISM_START;
+		image_size = FLASH_REDBOOT_IMAGE_MAX_SIZE;
+		break;
+	default:
+		return 0;
+	}
+
+	p += sizeof(struct flash_file_hdr) + image_offset;
+	if (p + image_size > fw->data + fw->size)
 		return -1;
-	total_bytes = pflashcomp[i].size;
-		while (total_bytes) {
-			if (total_bytes > 32*1024)
-				num_bytes = 32*1024;
-			else
-				num_bytes = total_bytes;
-			total_bytes -= num_bytes;
-
-			if (!total_bytes)
-				flash_op = FLASHROM_OPER_FLASH;
-			else
-				flash_op = FLASHROM_OPER_SAVE;
-			memcpy(req->params.data_buf, p, num_bytes);
-			p += num_bytes;
-			status = be_cmd_write_flashrom(adapter, flash_cmd,
-				pflashcomp[i].optype, flash_op, num_bytes);
-			if (status) {
-				dev_err(&adapter->pdev->dev,
-					"cmd to write to flash rom failed.\n");
-				return -1;
-			}
-			yield();
+
+	total_bytes = image_size;
+
+	while (total_bytes) {
+		if (total_bytes > 32*1024)
+			num_bytes = 32*1024;
+		else
+			num_bytes = total_bytes;
+		total_bytes -= num_bytes;
+
+		if (!total_bytes)
+			flash_op = FLASHROM_OPER_FLASH;
+		else
+			flash_op = FLASHROM_OPER_SAVE;
+		memcpy(req->params.data_buf, p, num_bytes);
+		p += num_bytes;
+		status = be_cmd_write_flashrom(adapter, flash_cmd,
+				flash_type, flash_op, num_bytes);
+		if (status) {
+			dev_err(&adapter->pdev->dev,
+			"cmd to write to flash rom failed. type/op %d/%d\n",
+			flash_type, flash_op);
+			return -1;
 		}
+		yield();
 	}
-	return 0;
-}
 
-static int get_ufigen_type(struct flash_file_hdr_g2 *fhdr)
-{
-	if (fhdr == NULL)
-		return 0;
-	if (fhdr->build[0] == '3')
-		return BE_GEN3;
-	else if (fhdr->build[0] == '2')
-		return BE_GEN2;
-	else
-		return 0;
+	return 0;
 }
 
 int be_load_fw(struct be_adapter *adapter, u8 *func)
 {
 	char fw_file[ETHTOOL_FLASH_MAX_FILENAME];
 	const struct firmware *fw;
-	struct flash_file_hdr_g2 *fhdr;
-	struct flash_file_hdr_g3 *fhdr3;
-	struct image_hdr *img_hdr_ptr = NULL;
+	struct flash_file_hdr *fhdr;
+	struct flash_section_info *fsec = NULL;
 	struct be_dma_mem flash_cmd;
-	int status, i = 0, num_imgs = 0;
+	int status;
 	const u8 *p;
+	bool entry_found = false;
+	int flash_type;
+	char fw_ver[FW_VER_LEN];
+	char fw_cfg;
 
+	status = be_cmd_get_fw_ver(adapter, fw_ver);
+	if (status)
+		return status;
+
+	fw_cfg = *(fw_ver + 2);
+	if (fw_cfg == '0')
+		fw_cfg = '1';
 	strcpy(fw_file, func);
 
 	status = request_firmware(&fw, fw_file, &adapter->pdev->dev);
@@ -2114,9 +2271,34 @@ int be_load_fw(struct be_adapter *adapter, u8 *func)
 		goto fw_exit;
 
 	p = fw->data;
-	fhdr = (struct flash_file_hdr_g2 *) p;
+	fhdr = (struct flash_file_hdr *) p;
+	if (memcmp(fhdr->sign, FW_FILE_HDR_SIGN, strlen(FW_FILE_HDR_SIGN))) {
+		dev_err(&adapter->pdev->dev,
+			"Firmware(%s) load error (signature did not match)\n",
+				fw_file);
+		status = -1;
+		goto fw_exit;
+	}
+
 	dev_info(&adapter->pdev->dev, "Flashing firmware file %s\n", fw_file);
 
+	p += sizeof(struct flash_file_hdr);
+	while (p < (fw->data + fw->size)) {
+		fsec = (struct flash_section_info *)p;
+		if (!memcmp(flash_cookie, fsec->cookie, sizeof(flash_cookie))) {
+			entry_found = true;
+			break;
+		}
+		p += 32;
+	}
+
+	if (!entry_found) {
+		status = -1;
+		dev_err(&adapter->pdev->dev,
+			"Flash cookie not found in firmware image\n");
+		goto fw_exit;
+	}
+
 	flash_cmd.size = sizeof(struct be_cmd_write_flashrom) + 32*1024;
 	flash_cmd.va = pci_alloc_consistent(adapter->pdev, flash_cmd.size,
 					&flash_cmd.dma);
@@ -2127,25 +2309,12 @@ int be_load_fw(struct be_adapter *adapter, u8 *func)
 		goto fw_exit;
 	}
 
-	if ((adapter->generation == BE_GEN3) &&
-			(get_ufigen_type(fhdr) == BE_GEN3)) {
-		fhdr3 = (struct flash_file_hdr_g3 *) fw->data;
-		num_imgs = le32_to_cpu(fhdr3->num_imgs);
-		for (i = 0; i < num_imgs; i++) {
-			img_hdr_ptr = (struct image_hdr *) (fw->data +
-					(sizeof(struct flash_file_hdr_g3) +
-					 i * sizeof(struct image_hdr)));
-			if (le32_to_cpu(img_hdr_ptr->imageid) == 1)
-				status = be_flash_data(adapter, fw, &flash_cmd,
-							num_imgs);
-		}
-	} else if ((adapter->generation == BE_GEN2) &&
-			(get_ufigen_type(fhdr) == BE_GEN2)) {
-		status = be_flash_data(adapter, fw, &flash_cmd, 0);
-	} else {
-		dev_err(&adapter->pdev->dev,
-			"UFI and Interface are not compatible for flashing\n");
-		status = -1;
+	for (flash_type = FLASHROM_TYPE_ISCSI_ACTIVE;
+		flash_type <= FLASHROM_TYPE_FCOE_FW_BACKUP; flash_type++) {
+		status = be_flash_image(adapter, fw, &flash_cmd,
+				flash_type);
+		if (status)
+			break;
 	}
 
 	pci_free_consistent(adapter->pdev, flash_cmd.size, flash_cmd.va,
@@ -2174,7 +2343,6 @@ static struct net_device_ops be_netdev_ops = {
 	.ndo_vlan_rx_register	= be_vlan_register,
 	.ndo_vlan_rx_add_vid	= be_vlan_add_vid,
 	.ndo_vlan_rx_kill_vid	= be_vlan_rem_vid,
-	.ndo_set_vf_mac		= be_set_vf_mac
 };
 
 static void be_netdev_init(struct net_device *netdev)
@@ -2216,48 +2384,37 @@ static void be_unmap_pci_bars(struct be_adapter *adapter)
 		iounmap(adapter->csr);
 	if (adapter->db)
 		iounmap(adapter->db);
-	if (adapter->pcicfg && be_physfn(adapter))
+	if (adapter->pcicfg)
 		iounmap(adapter->pcicfg);
 }
 
 static int be_map_pci_bars(struct be_adapter *adapter)
 {
 	u8 __iomem *addr;
-	int pcicfg_reg, db_reg;
+	int pcicfg_reg;
 
-	if (be_physfn(adapter)) {
-		addr = ioremap_nocache(pci_resource_start(adapter->pdev, 2),
-				pci_resource_len(adapter->pdev, 2));
-		if (addr == NULL)
-			return -ENOMEM;
-		adapter->csr = addr;
-	}
+	addr = ioremap_nocache(pci_resource_start(adapter->pdev, 2),
+			pci_resource_len(adapter->pdev, 2));
+	if (addr == NULL)
+		return -ENOMEM;
+	adapter->csr = addr;
 
-	if (adapter->generation == BE_GEN2) {
-		pcicfg_reg = 1;
-		db_reg = 4;
-	} else {
-		pcicfg_reg = 0;
-		if (be_physfn(adapter))
-			db_reg = 4;
-		else
-			db_reg = 0;
-	}
-	addr = ioremap_nocache(pci_resource_start(adapter->pdev, db_reg),
-				pci_resource_len(adapter->pdev, db_reg));
+	addr = ioremap_nocache(pci_resource_start(adapter->pdev, 4),
+			128 * 1024);
 	if (addr == NULL)
 		goto pci_map_err;
 	adapter->db = addr;
 
-	if (be_physfn(adapter)) {
-		addr = ioremap_nocache(
-				pci_resource_start(adapter->pdev, pcicfg_reg),
-				pci_resource_len(adapter->pdev, pcicfg_reg));
-		if (addr == NULL)
-			goto pci_map_err;
-		adapter->pcicfg = addr;
-	} else
-		adapter->pcicfg = adapter->db + SRIOV_VF_PCICFG_OFFSET;
+	if (adapter->generation == BE_GEN2)
+		pcicfg_reg = 1;
+	else
+		pcicfg_reg = 0;
+
+	addr = ioremap_nocache(pci_resource_start(adapter->pdev, pcicfg_reg),
+			pci_resource_len(adapter->pdev, pcicfg_reg));
+	if (addr == NULL)
+		goto pci_map_err;
+	adapter->pcicfg = addr;
 
 	return 0;
 pci_map_err:
@@ -2319,8 +2476,6 @@ static int be_ctrl_init(struct be_adapter *adapter)
 	spin_lock_init(&adapter->mcc_lock);
 	spin_lock_init(&adapter->mcc_cq_lock);
 
-	init_completion(&adapter->flash_compl);
-	pci_save_state(adapter->pdev);
 	return 0;
 
 free_mbox:
@@ -2372,8 +2527,6 @@ static void __devexit be_remove(struct pci_dev *pdev)
 
 	be_ctrl_cleanup(adapter);
 
-	be_sriov_disable(adapter);
-
 	be_msix_disable(adapter);
 
 	pci_set_drvdata(pdev, NULL);
@@ -2398,25 +2551,16 @@ static int be_get_config(struct be_adapter *adapter)
 		return status;
 
 	memset(mac, 0, ETH_ALEN);
-
-	if (be_physfn(adapter)) {
-		status = be_cmd_mac_addr_query(adapter, mac,
+	status = be_cmd_mac_addr_query(adapter, mac,
 			MAC_ADDRESS_TYPE_NETWORK, true /*permanent */, 0);
+	if (status)
+		return status;
 
-		if (status)
-			return status;
-
-		if (!is_valid_ether_addr(mac))
-			return -EADDRNOTAVAIL;
-
-		memcpy(adapter->netdev->dev_addr, mac, ETH_ALEN);
-		memcpy(adapter->netdev->perm_addr, mac, ETH_ALEN);
-	}
+	if (!is_valid_ether_addr(mac))
+		return -EADDRNOTAVAIL;
 
-	if (adapter->cap & 0x400)
-		adapter->max_vlans = BE_NUM_VLANS_SUPPORTED/4;
-	else
-		adapter->max_vlans = BE_NUM_VLANS_SUPPORTED;
+	memcpy(adapter->netdev->dev_addr, mac, ETH_ALEN);
+	memcpy(adapter->netdev->perm_addr, mac, ETH_ALEN);
 
 	return 0;
 }
@@ -2428,7 +2572,6 @@ static int __devinit be_probe(struct pci_dev *pdev,
 	struct be_adapter *adapter;
 	struct net_device *netdev;
 
-
 	status = pci_enable_device(pdev);
 	if (status)
 		goto do_none;
@@ -2464,7 +2607,8 @@ static int __devinit be_probe(struct pci_dev *pdev,
 	be_netdev_init(netdev);
 	SET_NETDEV_DEV(netdev, &pdev->dev);
 
-	be_msix_enable(adapter);
+	if (!disable_msix)
+		be_msix_enable(adapter);
 
 	status = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
 	if (!status) {
@@ -2477,29 +2621,23 @@ static int __devinit be_probe(struct pci_dev *pdev,
 		}
 	}
 
-	be_sriov_enable(adapter);
-
 	status = be_ctrl_init(adapter);
 	if (status)
 		goto free_netdev;
 
 	/* sync up with fw's ready state */
-	if (be_physfn(adapter)) {
-		status = be_cmd_POST(adapter);
-		if (status)
-			goto ctrl_clean;
-	}
+	status = be_cmd_POST(adapter);
+	if (status)
+		goto ctrl_clean;
 
 	/* tell fw we're ready to fire cmds */
 	status = be_cmd_fw_init(adapter);
 	if (status)
 		goto ctrl_clean;
 
-	if (be_physfn(adapter)) {
-		status = be_cmd_reset_function(adapter);
-		if (status)
-			goto ctrl_clean;
-	}
+	status = be_cmd_reset_function(adapter);
+	if (status)
+		goto ctrl_clean;
 
 	status = be_stats_init(adapter);
 	if (status)
@@ -2530,7 +2668,6 @@ ctrl_clean:
 	be_ctrl_cleanup(adapter);
 free_netdev:
 	be_msix_disable(adapter);
-	be_sriov_disable(adapter);
 	free_netdev(adapter->netdev);
 	pci_set_drvdata(pdev, NULL);
 rel_reg:
@@ -2598,120 +2735,13 @@ static int be_resume(struct pci_dev *pdev)
 	return 0;
 }
 
-/*
- * An FLR will stop BE from DMAing any data.
- */
-static void be_shutdown(struct pci_dev *pdev)
-{
-	struct be_adapter *adapter = pci_get_drvdata(pdev);
-	struct net_device *netdev =  adapter->netdev;
-
-	netif_device_detach(netdev);
-
-	be_cmd_reset_function(adapter);
-
-	if (adapter->wol)
-		be_setup_wol(adapter, true);
-
-	pci_disable_device(pdev);
-}
-
-static pci_ers_result_t be_eeh_err_detected(struct pci_dev *pdev,
-				pci_channel_state_t state)
-{
-	struct be_adapter *adapter = pci_get_drvdata(pdev);
-	struct net_device *netdev =  adapter->netdev;
-
-	dev_err(&adapter->pdev->dev, "EEH error detected\n");
-
-	adapter->eeh_err = true;
-
-	netif_device_detach(netdev);
-
-	if (netif_running(netdev)) {
-		rtnl_lock();
-		be_close(netdev);
-		rtnl_unlock();
-	}
-	be_clear(adapter);
-
-	if (state == pci_channel_io_perm_failure)
-		return PCI_ERS_RESULT_DISCONNECT;
-
-	pci_disable_device(pdev);
-
-	return PCI_ERS_RESULT_NEED_RESET;
-}
-
-static pci_ers_result_t be_eeh_reset(struct pci_dev *pdev)
-{
-	struct be_adapter *adapter = pci_get_drvdata(pdev);
-	int status;
-
-	dev_info(&adapter->pdev->dev, "EEH reset\n");
-	adapter->eeh_err = false;
-
-	status = pci_enable_device(pdev);
-	if (status)
-		return PCI_ERS_RESULT_DISCONNECT;
-
-	pci_set_master(pdev);
-	pci_set_power_state(pdev, 0);
-	pci_restore_state(pdev);
-
-	/* Check if card is ok and fw is ready */
-	status = be_cmd_POST(adapter);
-	if (status)
-		return PCI_ERS_RESULT_DISCONNECT;
-
-	return PCI_ERS_RESULT_RECOVERED;
-}
-
-static void be_eeh_resume(struct pci_dev *pdev)
-{
-	int status = 0;
-	struct be_adapter *adapter = pci_get_drvdata(pdev);
-	struct net_device *netdev =  adapter->netdev;
-
-	dev_info(&adapter->pdev->dev, "EEH resume\n");
-
-	pci_save_state(pdev);
-
-	/* tell fw we're ready to fire cmds */
-	status = be_cmd_fw_init(adapter);
-	if (status)
-		goto err;
-
-	status = be_setup(adapter);
-	if (status)
-		goto err;
-
-	if (netif_running(netdev)) {
-		status = be_open(netdev);
-		if (status)
-			goto err;
-	}
-	netif_device_attach(netdev);
-	return;
-err:
-	dev_err(&adapter->pdev->dev, "EEH resume failed\n");
-}
-
-static struct pci_error_handlers be_eeh_handlers = {
-	.error_detected = be_eeh_err_detected,
-	.slot_reset = be_eeh_reset,
-	.resume = be_eeh_resume,
-};
-
 static struct pci_driver be_driver = {
 	.name = DRV_NAME,
 	.id_table = be_dev_ids,
 	.probe = be_probe,
 	.remove = be_remove,
 	.suspend = be_suspend,
-	.resume = be_resume,
-	.shutdown = be_shutdown,
-	.err_handler = &be_eeh_handlers
+	.resume = be_resume
 };
 
 static int __init be_init_module(void)
@@ -2724,12 +2754,7 @@ static int __init be_init_module(void)
 		rx_frag_size = 2048;
 	}
 
-	if (num_vfs > 32) {
-		printk(KERN_WARNING DRV_NAME
-			" : Module param num_vfs must not be greater than 32."
-			"Using 32\n");
-		num_vfs = 32;
-	}
+	init_poll_procfs();
 
 	return pci_register_driver(&be_driver);
 }
@@ -2738,5 +2763,7 @@ module_init(be_init_module);
 static void __exit be_exit_module(void)
 {
 	pci_unregister_driver(&be_driver);
+
+	cleanup_procfs();
 }
 module_exit(be_exit_module);
diff --git a/drivers/pci/dmar.c b/drivers/pci/dmar.c
index 0a19708..750b824 100644
--- a/drivers/pci/dmar.c
+++ b/drivers/pci/dmar.c
@@ -831,6 +831,51 @@ void free_iommu(struct intel_iommu *iommu)
 /*
  * Reclaim all the submitted descriptors which have completed its work.
  */
+
+#ifdef IOMMU_DEFERRED_LIST
+
+int get_q_length(struct intel_iommu *iommu)
+{
+	struct q_inval *qi = iommu->qi;
+	return ((qi->free_head + 256) - qi->free_tail) % 256;
+}
+
+#define MAX_PROCESSED_DEFERRED_ENTRIES	1
+static inline void reclaim_free_desc(struct q_inval *qi,
+	struct list_head* iovas)
+{
+	int i, group_descs;
+	int processed = 0;
+	while ((qi->desc_data[qi->free_tail].status == QI_DONE ||
+	       qi->desc_data[qi->free_tail].status == QI_ABORT) &&
+		processed < MAX_PROCESSED_DEFERRED_ENTRIES) {
+		group_descs = qi->desc_data[qi->free_tail].group_descs;
+/*		if (!list_empty(&qi->desc_data[qi->free_tail].iovas)) {
+			while (!list_empty(&qi->desc_data[qi->free_tail].iovas)) {
+				struct iova *iova = list_first_entry(
+					&qi->desc_data[qi->free_tail].iovas, 
+					struct iova, link);
+				printk(KERN_ERR "%s : pfn_lo = %lx, pfn_hi = %lx\n", 
+					__func__, iova->pfn_lo, iova->pfn_hi);
+				list_del_init(&iova->link);
+				list_add(&iova->link, iovas);
+			}
+		}
+*/
+		list_splice_init(&qi->desc_data[qi->free_tail].iovas, iovas);
+
+		for (i = 0; i < group_descs; i++) {
+			qi->desc_data[qi->free_tail].status = QI_FREE;
+/*			qi->desc[qi->free_tail].low = 0;
+			qi->desc[qi->free_tail].high = 0;*/
+			qi->free_tail = (qi->free_tail + 1) % QI_LENGTH;
+		}
+		qi->free_cnt += group_descs;
+		processed++;
+	}
+}
+
+#else
 static inline void reclaim_free_desc(struct q_inval *qi)
 {
 	while (qi->desc_status[qi->free_tail] == QI_DONE ||
@@ -840,7 +885,74 @@ static inline void reclaim_free_desc(struct q_inval *qi)
 		qi->free_cnt++;
 	}
 }
+#endif
 
+#ifdef IOMMU_DEFERRED_LIST
+static int qi_check_fault(struct intel_iommu *iommu, int index)
+{
+	u32 fault;
+	int head, tail;
+	struct q_inval *qi = iommu->qi;
+	int wait_index = (index + 1) % QI_LENGTH;
+	
+	if (qi->desc_data[wait_index].status == QI_ABORT)
+		return -EAGAIN;
+	
+
+	fault = readl(iommu->reg + DMAR_FSTS_REG);
+
+	/*
+	 * If IQE happens, the head points to the descriptor associated
+	 * with the error. No new descriptors are fetched until the IQE
+	 * is cleared.
+	 */
+	if (fault & DMA_FSTS_IQE) {
+		panic("DMA_FSTS_IQE");
+		head = readl(iommu->reg + DMAR_IQH_REG);
+		if ((head >> DMAR_IQ_SHIFT) == index) {
+			printk(KERN_ERR "VT-d detected invalid descriptor: "
+				"low=%llx, high=%llx\n",
+				(unsigned long long)qi->desc[index].low,
+				(unsigned long long)qi->desc[index].high);
+			memcpy(&qi->desc[index], &qi->desc[wait_index],
+					sizeof(struct qi_desc));
+			__iommu_flush_cache(iommu, &qi->desc[index],
+					sizeof(struct qi_desc));
+			writel(DMA_FSTS_IQE, iommu->reg + DMAR_FSTS_REG);
+			return -EINVAL;
+		}
+	}
+
+	/*
+	 * If ITE happens, all pending wait_desc commands are aborted.
+	 * No new descriptors are fetched until the ITE is cleared.
+	 */
+	if (fault & DMA_FSTS_ITE) {
+		panic("DMA_FSTS_ITE");
+		head = readl(iommu->reg + DMAR_IQH_REG);
+		head = ((head >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;
+		head |= 1;
+		tail = readl(iommu->reg + DMAR_IQT_REG);
+		tail = ((tail >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;
+
+		writel(DMA_FSTS_ITE, iommu->reg + DMAR_FSTS_REG);
+
+		do {
+			if (qi->desc_data[head].status == QI_IN_USE)
+				qi->desc_data[head].status = QI_ABORT;
+			head = (head - 2 + QI_LENGTH) % QI_LENGTH;
+		} while (head != tail);
+
+		if (qi->desc_data[wait_index].status == QI_ABORT)
+			return -EAGAIN;
+	}
+
+	if (fault & DMA_FSTS_ICE)
+		writel(DMA_FSTS_ICE, iommu->reg + DMAR_FSTS_REG);
+
+	return 0;
+}
+#else
 static int qi_check_fault(struct intel_iommu *iommu, int index)
 {
 	u32 fault;
@@ -903,11 +1015,144 @@ static int qi_check_fault(struct intel_iommu *iommu, int index)
 	return 0;
 }
 
+#endif
+
+
+#ifdef IOMMU_DEFERRED_LIST
+int qi_submit(struct qi_desc *desc, struct intel_iommu *iommu,
+	uint32_t *done_ptr, struct list_head *iovas,
+	struct list_head *to_free_iovas)
+{
+	int rc;
+	struct q_inval *qi = iommu->qi;
+	struct qi_desc *hw, write_desc, wait_desc;
+	int wait_index, write_index, index;
+	volatile unsigned long flags;
+	struct qi_data *act_meta, *write_meta, *wait_meta;
+	uint64_t wait_cl, act_cl, write_cl;
+
+	if (!qi)
+		return 0;
+/*
+	BUG_ON(!iovas);
+*/
+	hw = qi->desc;
+
+	rc = 0;
+
+	spin_lock_irqsave(&qi->q_lock, flags);
+	while (qi->free_cnt < 5) {
+		reclaim_free_desc(qi, to_free_iovas);
+		if (qi->free_cnt >= 5)
+			break;
+	/*	spin_unlock_irqrestore(&qi->q_lock, flags);*/
+/*		cpu_relax();*/
+/*		spin_lock_irqsave(&qi->q_lock, flags); */
+	}
+/*	BUG_ON(flags != flags2);*/
+
+	index = qi->free_head;
+	act_meta = &qi->desc_data[index];
+/*	if (act_meta->status != QI_FREE)
+		printk(KERN_ERR "%s : status is %d", __func__, act_meta->status);	*/
+	INIT_LIST_HEAD(&act_meta->iovas);
+	list_splice(iovas, &act_meta->iovas);
+	write_index = (index + 1) % QI_LENGTH; /* Calc even if not req. for clflush calculations */
+	if (done_ptr) {
+		write_meta = &qi->desc_data[write_index];
+		write_meta->status = QI_IN_USE;
+		/* TODO : Following two lines to ease debug */
+	/*	write_meta->group_descs = 0;
+		INIT_LIST_HEAD(&write_meta->iovas);*/
+		/* -- Cut here -- */
+		wait_index = (write_index + 1) % QI_LENGTH;
+		qi->desc_data[write_index].status = QI_IN_USE;
+		act_meta->group_descs = 3;
+		write_desc.low = QI_IWD_STATUS_DATA(0) | 
+			QI_IWD_STATUS_WRITE | QI_IWD_TYPE | QI_IWD_FENCE;
+		write_desc.high = virt_to_phys(done_ptr);
+	/*	BUG_ON(write_desc.high & 0x3);*/
+		qi->desc[write_index] = write_desc;
+	} else {
+		wait_index = (index + 1) % QI_LENGTH;
+		act_meta->group_descs = 2;
+	}
+	
+	wait_meta = &qi->desc_data[wait_index];
+	wait_meta->status = act_meta->status = QI_IN_USE;
+	/* Following two lines to ease debug; remove later */
+	/*INIT_LIST_HEAD(&wait_meta->iovas);
+	wait_meta->group_descs = 0;*/
+	
+
+	wait_desc.low = QI_IWD_STATUS_DATA(QI_DONE) |
+			QI_IWD_STATUS_WRITE | QI_IWD_TYPE | QI_IWD_FENCE;
+	wait_desc.high = virt_to_phys(&qi->desc_data[index]);
+	BUG_ON(wait_desc.high & 0x3);
+	
+	qi->desc[wait_index] = wait_desc;
+	qi->desc[index] = *desc;
+
+	qi->desc_data[index].status = qi->desc_data[wait_index].status = QI_IN_USE;
+
+	act_cl = ((uint64_t)&hw[index]) % boot_cpu_data.x86_clflush_size;
+	write_cl = ((uint64_t)&hw[write_index]) % boot_cpu_data.x86_clflush_size;
+	wait_cl = ((uint64_t)&hw[wait_index]) % boot_cpu_data.x86_clflush_size;
+
+	__iommu_flush_cache(iommu, &hw[index], sizeof(struct qi_desc));
+
+	/* Trying to avoid unnecassary clflushes */
+	if (act_cl != write_cl)
+		__iommu_flush_cache(iommu, &hw[write_index], sizeof(struct qi_desc));
+	
+	if (write_cl != wait_cl)
+		__iommu_flush_cache(iommu, &hw[wait_index], sizeof(struct qi_desc));
+
+	qi->free_head = (qi->free_head + act_meta->group_descs) % QI_LENGTH;
+	qi->free_cnt -= act_meta->group_descs;
+	
+	writel(qi->free_head << DMAR_IQ_SHIFT, iommu->reg + DMAR_IQT_REG);
+
+	reclaim_free_desc(qi, to_free_iovas);
+	
+	spin_unlock_irqrestore(&qi->q_lock, flags);
+	return 0;
+}
+/*
+uint32_t __attribute__ ((aligned(8))) iq_done[NR_CPUS];
+*/
+
+int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu, int counter)
+{
+	int rc = 0;
+	uint32_t __attribute__ ((aligned(8))) iq_done = 1;
+	volatile uint32_t* done_ptr = &iq_done;/*[smp_processor_id()];*/
+
+	struct q_inval *qi = iommu->qi;
+	LIST_HEAD(to_free_iovas);
+	LIST_HEAD(empty_iovas);
+	clflush_cache_range((void*)done_ptr, sizeof(*done_ptr));
+	rc = qi_submit(desc, iommu, (uint32_t*)done_ptr, &empty_iovas, &to_free_iovas);
+	if (rc)
+		return rc;
+
+/*	TODO: return the next line - disabled temporarily*/
+	free_iovas(&to_free_iovas);
+
+	while (*done_ptr) {
+/*		rc = qi_check_fault(iommu, index);*/
+/*		spin_unlock(&qi->q_lock);
+		cpu_relax();
+		spin_lock(&qi->q_lock);
+*/	}
+	return 0;
+}
+#else
 /*
  * Submit the queued invalidation descriptor to the remapping
  * hardware unit and wait for its completion.
  */
-int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)
+int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu, int counter)
 {
 	int rc;
 	struct q_inval *qi = iommu->qi;
@@ -923,6 +1168,7 @@ int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)
 restart:
 	rc = 0;
 
+
 	spin_lock_irqsave(&qi->q_lock, flags);
 	while (qi->free_cnt < 3) {
 		spin_unlock_irqrestore(&qi->q_lock, flags);
@@ -953,6 +1199,10 @@ restart:
 	 * update the HW tail register indicating the presence of
 	 * new descriptors.
 	 */
+
+	if (counter >= 0)
+		update_counter(counter, 1);	
+
 	writel(qi->free_head << DMAR_IQ_SHIFT, iommu->reg + DMAR_IQT_REG);
 
 	while (qi->desc_status[wait_index] != QI_DONE) {
@@ -972,16 +1222,21 @@ restart:
 		spin_lock(&qi->q_lock);
 	}
 
+	if (counter >= 0)
+		update_counter(counter, 0);	
+
 	qi->desc_status[index] = QI_DONE;
 
 	reclaim_free_desc(qi);
 	spin_unlock_irqrestore(&qi->q_lock, flags);
 
+
 	if (rc == -EAGAIN)
 		goto restart;
 
 	return rc;
 }
+#endif
 
 /*
  * Flush the global interrupt entry cache.
@@ -994,7 +1249,7 @@ void qi_global_iec(struct intel_iommu *iommu)
 	desc.high = 0;
 
 	/* should never fail */
-	qi_submit_sync(&desc, iommu);
+	qi_submit_sync(&desc, iommu, -1);
 }
 
 void qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,
@@ -1005,18 +1260,29 @@ void qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,
 	desc.low = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)
 			| QI_CC_GRAN(type) | QI_CC_TYPE;
 	desc.high = 0;
-
-	qi_submit_sync(&desc, iommu);
+	printk(KERN_ERR "%s : start flush context %d\n", __func__, did);
+	qi_submit_sync(&desc, iommu, -1);
+	printk(KERN_ERR "%s : end flush context %d\n", __func__, did);
 }
 
+#ifdef IOMMU_DEFERRED_LIST
 void qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,
-		    unsigned int size_order, u64 type)
+		    unsigned int size_order, u64 type,
+		    struct list_head *iovas,
+		    struct list_head *to_free_iovas)
 {
 	u8 dw = 0, dr = 0;
+	int local_free = 0;
+	LIST_HEAD(tmp);
 
 	struct qi_desc desc;
 	int ih = 0;
 
+	if (to_free_iovas == NULL) {
+		local_free = 1;
+		to_free_iovas = &tmp;
+	}
+
 	if (cap_write_drain(iommu->cap))
 		dw = 1;
 
@@ -1028,9 +1294,58 @@ void qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,
 	desc.high = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)
 		| QI_IOTLB_AM(size_order);
 
-	qi_submit_sync(&desc, iommu);
+	if (!iovas) 
+		qi_submit_sync(&desc, iommu, -1);
+	else
+		qi_submit(&desc, iommu, /*done_ptr*/NULL, iovas,
+			to_free_iovas);
+
+	if (local_free)
+		free_iovas(to_free_iovas);
+}
+
+void qi_flush_process(struct intel_iommu *iommu) 
+{
+	unsigned long flags;
+	struct q_inval *qi = iommu->qi;
+	LIST_HEAD(iovas);
+	spin_lock_irqsave(&qi->q_lock, flags);
+	reclaim_free_desc(qi, &iovas);
+	spin_unlock_irqrestore(&qi->q_lock, flags);
+	free_iovas(&iovas);
+}
+
+#else
+void qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,
+		    unsigned int size_order, u64 type,
+		    struct list_head *iovas,
+		    struct list_head *to_free_iovas)
+{
+	u8 dw = 0, dr = 0;
+
+	struct qi_desc desc;
+	int ih = 0;
+
+	if (cap_write_drain(iommu->cap))
+		dw = 1;
+
+	if (cap_read_drain(iommu->cap))
+		dr = 1;
+
+	desc.low = QI_IOTLB_DID(did&0x7FFF) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)
+		| QI_IOTLB_GRAN(type) | QI_IOTLB_TYPE;
+	desc.high = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)
+		| QI_IOTLB_AM(size_order);
+
+	qi_submit_sync(&desc, iommu, (did&0x8000)?IOMMU_MINVD_COUNTER:IOMMU_UINVD_COUNTER);
+}
+
+void qi_flush_process(struct intel_iommu *iommu) 
+{
 }
 
+#endif
+
 void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 qdep,
 			u64 addr, unsigned mask)
 {
@@ -1049,7 +1364,7 @@ void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 qdep,
 	desc.low = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |
 		   QI_DIOTLB_TYPE;
 
-	qi_submit_sync(&desc, iommu);
+	qi_submit_sync(&desc, iommu, -1);
 }
 
 /*
@@ -1150,6 +1465,15 @@ int dmar_enable_qi(struct intel_iommu *iommu)
 
 	qi->desc = page_address(desc_page);
 
+#ifdef IOMMU_DEFERRED_LIST
+	qi->desc_data = kmalloc(QI_LENGTH * sizeof(struct qi_data), GFP_ATOMIC);
+	if (!qi->desc_data) {
+		free_page((unsigned long) qi->desc);
+		kfree(qi);
+		iommu->qi = 0;
+		return -ENOMEM;
+	}
+#else
 	qi->desc_status = kmalloc(QI_LENGTH * sizeof(int), GFP_ATOMIC);
 	if (!qi->desc_status) {
 		free_page((unsigned long) qi->desc);
@@ -1157,6 +1481,7 @@ int dmar_enable_qi(struct intel_iommu *iommu)
 		iommu->qi = 0;
 		return -ENOMEM;
 	}
+#endif
 
 	qi->free_head = qi->free_tail = 0;
 	qi->free_cnt = QI_LENGTH;
diff --git a/drivers/pci/intel-iommu.c b/drivers/pci/intel-iommu.c
index c9171be..3c9cbf4 100644
--- a/drivers/pci/intel-iommu.c
+++ b/drivers/pci/intel-iommu.c
@@ -41,8 +41,224 @@
 #include <linux/dmi.h>
 #include <asm/cacheflush.h>
 #include <asm/iommu.h>
+#include <asm/sync_bitops.h>
+#include <asm/timer.h>
 #include "pci.h"
 
+#define PROCFS_MAX_SIZE         512
+#define PROCFS_NAME             "iommu"
+
+/**
+ * This structure hold information about the /proc file
+ *
+ */
+static struct proc_dir_entry *Our_Proc_File;
+
+/**
+ * The buffer used to store character for this module
+ *
+ */
+static char procfs_buffer[PROCFS_MAX_SIZE];
+
+/**
+ * The size of the buffer
+ *
+ */
+static unsigned long procfs_buffer_size = 0;
+
+struct counter_data {
+        uint64_t time_counter;
+        uint64_t temp_start_counter;
+        uint64_t events_counter;
+        uint64_t min_time_counter;
+        uint64_t max_time_counter;
+};
+
+static const char* proc_output_names[] = {"map", "unmap", "minvd", "uinvd", "shared_hits", "victim_hits", "misses"};
+static const char* proc_control_names[] = {"vsize", "vtime", "dtime", "dsize", NULL};
+
+enum {IOMMU_VSIZE_CONTROL, IOMMU_VTIME_CONTROL, IOMMU_DTIME_CONTROL,
+	IOMMU_DSIZE_CONTROL, IOMMU_CONTROLS_NUM};
+static int proc_control_vals[IOMMU_CONTROLS_NUM];
+
+static struct counter_data counters_data[IOMMU_COUNTERS_NUM];
+
+static u64 total_q_len;
+static u64 q_count;
+static u64 max_q_size;
+#if 0
+void update_q_size(struct intel_iommu* iommu)
+{
+	int q_size = get_q_length(iommu);
+	total_q_len += q_size;
+	if (max_q_size < q_size)
+		max_q_size = q_size;
+	q_count++;
+}
+#endif
+
+/* global iommu list, set NULL for ignored DMAR units */
+static struct intel_iommu **g_iommus;
+
+static void iommu_clear_counters(void)
+{
+        int i;
+	uint64_t temp;
+        for (i=0; i < IOMMU_COUNTERS_NUM; i++) {
+                counters_data[i].time_counter = 0;
+                counters_data[i].temp_start_counter = 0;
+                counters_data[i].events_counter = 0;
+		counters_data[i].min_time_counter = 0xFFFFFFFF;
+		counters_data[i].max_time_counter = 0;
+		temp = xchg(&counters_data[i].events_counter, 0);
+        }
+	q_count = max_q_size = total_q_len = 0;
+}
+
+#define VIOMMU_SAMPLE 0x100
+
+static void print_counters(char* buf)
+{
+        int i;
+        char temp_buf[1000];
+        buf[0] = '\0';
+        for (i=0; i<IOMMU_COUNTERS_NUM; i++) {
+                uint64_t samples = counters_data[i].events_counter / VIOMMU_SAMPLE;
+                sprintf(temp_buf, "%s -  Total: %llx   Avg: %llx  Min:%llx  Max: %llx   Event: %llx\n",
+                        proc_output_names[i],
+                        counters_data[i].time_counter, (samples == 0) ? 0 :
+                        (counters_data[i].time_counter / samples),
+                        counters_data[i].min_time_counter,
+                        counters_data[i].max_time_counter,
+                        counters_data[i].events_counter);
+
+                strcat(buf, temp_buf);
+        }
+}
+
+inline void update_counter(int counter, int start)
+{
+#if 0
+        int sample_time = ((counters_data[counter].events_counter % VIOMMU_SAMPLE) == 0);
+
+        if (start && sample_time) {
+                counters_data[counter].temp_start_counter = get_cycles();
+        } else if (!start) {
+                if (sample_time) {
+                        uint64_t cycles = get_cycles() - counters_data[counter].temp_start_counter;
+                        counters_data[counter].time_counter += cycles;
+                        if ((cycles < counters_data[counter].min_time_counter ||
+                                counters_data[counter].min_time_counter == 0) &&
+                                cycles > 0)
+                                counters_data[counter].min_time_counter = cycles;
+                        if (cycles > counters_data[counter].max_time_counter)
+                                counters_data[counter].max_time_counter = cycles;
+                }
+                counters_data[counter].events_counter++;
+        }
+#endif
+}
+EXPORT_SYMBOL(update_counter);
+
+/**
+ * This function is called then the /proc file is read
+ *
+ */
+int
+procfile_read(char *buffer,
+              char **buffer_location,
+              off_t offset, int buffer_length, int *eof, void *data)
+{
+        int ret;
+	struct intel_iommu *iommu;
+	if (g_iommus == NULL)
+		return 0;
+	iommu = g_iommus[0];
+	if (iommu == NULL)
+		return 0;
+
+        if (offset > 0) {
+                /* we have finished to read, return 0 */
+                ret  = 0;
+        } else {
+                /* fill the buffer, return the buffer size */
+                procfs_buffer[0] = '\0';
+		ret = sprintf(procfs_buffer, "1.active cycles: %llx\n2.inactive cycles: %llx\n3.avg_q: %lx, max_q: %lx\n", 
+			readq(iommu->reg + DMAR_SIDECORE_ACTIVE_CNT),
+			readq(iommu->reg + DMAR_SIDECORE_INACTIVE_CNT),
+			total_q_len/(q_count?q_count:1), max_q_size);
+                memcpy(buffer, procfs_buffer, strlen(procfs_buffer));
+
+/*                print_counters(procfs_buffer);*/
+/*                ret = strlen(procfs_buffer);*/
+
+        }
+
+        return ret;
+}
+
+
+
+int procfile_write(struct file *file, const char *buffer, unsigned long count,
+                   void *data)
+{
+	int cmd_index = 0;
+	int val_int = 0;
+	const char *val_ptr = strchr(buffer, '=');
+	const char **cmd = proc_control_names;
+	char *temp_ptr;
+
+	if (val_ptr == NULL) {
+		printk(KERN_ERR "Clearing IOMMU counters\n");
+		iommu_clear_counters();
+		return count;
+	}
+
+	while (*cmd != NULL) {
+		if (!strncmp(*cmd, buffer, val_ptr-buffer)) 
+			break;
+		cmd_index++;
+		cmd++;
+	}
+	
+	val_int = (int)simple_strtol(val_ptr+1, &temp_ptr, 0);
+	
+	if (*cmd == NULL) {
+		printk(KERN_ERR "iommu proc write : could not find the command %s", buffer);
+		return count;
+	}
+
+	proc_control_vals[cmd_index] = val_int;
+	printk(KERN_ERR
+		"VIOMMU : Setting control %s to val %d\n", *cmd, val_int);
+
+	return count;
+}
+
+static void init_procfs(void)
+{
+        /* create the /proc file */
+        Our_Proc_File = create_proc_entry(PROCFS_NAME, 0644 ,NULL);
+
+        if (Our_Proc_File == NULL) {
+                remove_proc_entry(PROCFS_NAME, Our_Proc_File);
+                printk(KERN_ALERT "Error: Could not initialize /proc/%s\n",
+                        PROCFS_NAME);
+                return;
+        }
+
+        Our_Proc_File->read_proc  = procfile_read;
+        Our_Proc_File->write_proc = procfile_write;
+/*      Our_Proc_File->owner      = THIS_MODULE;*/
+/*      Our_Proc_File->mode       = S_IFREG | S_IRUGO;
+        Our_Proc_File->uid        = 0;
+        Our_Proc_File->gid        = 0;
+        Our_Proc_File->size       = 37;*/
+        Our_Proc_File->data = NULL;
+	iommu_clear_counters();
+}
+
+
 #define ROOT_SIZE		VTD_PAGE_SIZE
 #define CONTEXT_SIZE		VTD_PAGE_SIZE
 
@@ -70,7 +286,14 @@
 #define IOVA_PFN(addr)		((addr) >> PAGE_SHIFT)
 #define DMA_32BIT_PFN		IOVA_PFN(DMA_BIT_MASK(32))
 #define DMA_64BIT_PFN		IOVA_PFN(DMA_BIT_MASK(64))
+#define UL_SIZE 		unsigned long
 
+static void flush_unmaps_lock(void);
+
+static inline int is_error(int val)
+{
+	return (val < 0);
+}
 
 /* VT-d pages must always be _smaller_ than MM pages. Otherwise things
    are never going to work. */
@@ -92,8 +315,6 @@ static inline unsigned long virt_to_dma_pfn(void *p)
 	return page_to_dma_pfn(virt_to_page(p));
 }
 
-/* global iommu list, set NULL for ignored DMAR units */
-static struct intel_iommu **g_iommus;
 
 static void __init check_tylersburg_isoch(void);
 static int rwbf_quirk;
@@ -239,6 +460,24 @@ static inline u64 dma_pte_addr(struct dma_pte *pte)
 	return  __cmpxchg64(pte, 0ULL, 0ULL) & VTD_PAGE_MASK;
 #endif
 }
+static inline void dma_set_pte_need_invd(struct dma_pte *pte)
+{
+/*
+	TODO: Check if atomic is required ; I really do not think so
+*/
+	sync_set_bit(8, (volatile unsigned long *)(&pte->val));
+/*	pte->val |= DMA_PTE_NEED_INVD;*/
+}
+
+static inline bool dma_pte_need_invd(struct dma_pte *pte)
+{
+	return test_bit(8, (unsigned long*)&pte->val);
+}
+
+static inline void dma_clear_pte_need_invd(struct dma_pte *pte)
+{
+	sync_clear_bit(8, (volatile unsigned long *)(&pte->val));
+}
 
 static inline void dma_set_pte_pfn(struct dma_pte *pte, unsigned long pfn)
 {
@@ -275,6 +514,17 @@ static int hw_pass_through = 1;
 /* si_domain contains mulitple devices */
 #define DOMAIN_FLAG_STATIC_IDENTITY	(1 << 2)
 
+#define DMAR_SHARED_CACHE_SETS 4096 
+#define DMAR_SHARED_CACHE_WAYS 4
+#define DMAR_SHARED_CACHE_INTERVAL 10000
+
+struct dmar_shared_cache_set {
+	struct iova* 	iova[DMAR_SHARED_CACHE_WAYS];
+} __attribute__((packed, aligned (8)));
+
+
+
+
 struct dmar_domain {
 	int	id;			/* domain id */
 	int	nid;			/* node id */
@@ -296,6 +546,11 @@ struct dmar_domain {
 	int		iommu_count;	/* reference count of iommu */
 	spinlock_t	iommu_lock;	/* protect iommu set in domain */
 	u64		max_addr;	/* maximum mapped address */
+
+	struct list_head iova_cache; 
+	int iova_cache_size;
+	struct dmar_shared_cache_set shared_cache[DMAR_SHARED_CACHE_SETS]; 	
+
 };
 
 /* PCI domain-device relationship */
@@ -315,6 +570,16 @@ static void flush_unmaps_timeout(unsigned long data);
 DEFINE_TIMER(unmap_timer,  flush_unmaps_timeout, 0, 0);
 
 #define HIGH_WATER_MARK 250
+#define LOW_WATER_MARK 250
+
+#ifdef IOMMU_DEFERRED_LIST
+struct deferred_flush_entry {
+	struct list_head list_head;
+	u32 size;
+};
+
+static struct deferred_flush_entry *deferred_flush_lists;
+#else
 struct deferred_flush_tables {
 	int next;
 	struct iova *iova[HIGH_WATER_MARK];
@@ -322,6 +587,7 @@ struct deferred_flush_tables {
 };
 
 static struct deferred_flush_tables *deferred_flush;
+#endif
 
 /* bitmap for indexing intel_iommus */
 static int g_num_of_iommus;
@@ -343,6 +609,9 @@ int dmar_disabled = 1;
 static int dmar_map_gfx = 1;
 static int dmar_forcedac;
 static int intel_iommu_strict;
+static int intel_iommu_alh;
+static int intel_iommu_no_shared;
+static int intel_iommu_no_victim;
 
 #define DUMMY_DEVICE_DOMAIN_INFO ((struct device_domain_info *)(-1))
 static DEFINE_SPINLOCK(device_domain_lock);
@@ -373,8 +642,21 @@ static int __init intel_iommu_setup(char *str)
 			printk(KERN_INFO
 				"Intel-IOMMU: disable batched IOTLB flush\n");
 			intel_iommu_strict = 1;
+		} else if (!strncmp(str, "alh", 3)) {
+			printk(KERN_INFO
+				"Intel-IOMMU: enable ALH");
+			intel_iommu_alh = 1;
+		} else if (!strncmp(str, "no_shared", 9)) {
+			printk(KERN_INFO
+				"Intel-IOMMU: shared IOVAs off");
+			intel_iommu_no_shared = 1;
+		} else if (!strncmp(str, "no_victim", 9)) {
+			printk(KERN_INFO
+				"Intel-IOMMU: victim off");
+			intel_iommu_no_victim = 1;
 		}
 
+
 		str += strcspn(str, ",");
 		while (*str == ',')
 			str++;
@@ -433,8 +715,15 @@ void free_iova_mem(struct iova *iova)
 	kmem_cache_free(iommu_iova_cache, iova);
 }
 
+void free_iova_unmap(struct intel_iommu *iommu, struct dmar_domain *domain, 
+	struct iova *iova, int lock);
 
 static inline int width_to_agaw(int width);
+static void flush_unmap_lock(struct intel_iommu* iommu);
+static void flush_unmap_func(void *iommu)
+{
+	flush_unmap_lock((struct intel_iommu*)iommu);
+}
 
 static int __iommu_calculate_agaw(struct intel_iommu *iommu, int max_gaw)
 {
@@ -475,7 +764,7 @@ static struct intel_iommu *domain_get_iommu(struct dmar_domain *domain)
 	int iommu_id;
 
 	/* si_domain and vm domain should not get here. */
-	BUG_ON(domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE);
+	/*BUG_ON(domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE);*/
 	BUG_ON(domain->flags & DOMAIN_FLAG_STATIC_IDENTITY);
 
 	iommu_id = find_first_bit(&domain->iommu_bmp, g_num_of_iommus);
@@ -692,7 +981,7 @@ static inline unsigned long align_to_level(unsigned long pfn, int level)
 }
 
 static struct dma_pte *pfn_to_dma_pte(struct dmar_domain *domain,
-				      unsigned long pfn)
+				      unsigned long pfn, int alloc)
 {
 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
 	struct dma_pte *parent, *pte = NULL;
@@ -700,6 +989,11 @@ static struct dma_pte *pfn_to_dma_pte(struct dmar_domain *domain,
 	int offset;
 
 	BUG_ON(!domain->pgd);
+	if (!alloc &&
+      		(!domain->pgd
+       		|| (addr_width < BITS_PER_LONG && pfn >> addr_width)))
+        	return NULL;
+
 	BUG_ON(addr_width < BITS_PER_LONG && pfn >> addr_width);
 	parent = domain->pgd;
 
@@ -713,7 +1007,8 @@ static struct dma_pte *pfn_to_dma_pte(struct dmar_domain *domain,
 
 		if (!dma_pte_present(pte)) {
 			uint64_t pteval;
-
+			if (!alloc)
+				return NULL;
 			tmp_page = alloc_pgtable_page(domain->nid);
 
 			if (!tmp_page)
@@ -760,10 +1055,9 @@ static struct dma_pte *dma_pfn_level_pte(struct dmar_domain *domain,
 	return NULL;
 }
 
-/* clear last level pte, a tlb flush should be followed */
-static void dma_pte_clear_range(struct dmar_domain *domain,
+static int dma_pte_clear_range(struct dmar_domain *domain,
 				unsigned long start_pfn,
-				unsigned long last_pfn)
+				unsigned long last_pfn, int flush)
 {
 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
 	struct dma_pte *first_pte, *pte;
@@ -779,16 +1073,56 @@ static void dma_pte_clear_range(struct dmar_domain *domain,
 			start_pfn = align_to_level(start_pfn + 1, 2);
 			continue;
 		}
-		do { 
+		do {
 			dma_clear_pte(pte);
 			start_pfn++;
 			pte++;
 		} while (start_pfn <= last_pfn && !first_pte_in_page(pte));
-
-		domain_flush_cache(domain, first_pte,
+		/* Maybe no need to clear if already unmapped */
+		if (flush)
+			domain_flush_cache(domain, first_pte,
 				   (void *)pte - (void *)first_pte);
+	} while (start_pfn && start_pfn <= last_pfn);
+
+	return 0;
+}
+
+/* clear last level pte, a tlb flush should be followed */
+static int dma_pte_clear_range_need_invd(struct dmar_domain *domain,
+				unsigned long start_pfn,
+				unsigned long last_pfn)
+{
+	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
+	struct dma_pte *first_pte, *pte;
+	struct dma_pte new_pte, old_pte;
+	int any_mapped = 0;
+
+	BUG_ON(addr_width < BITS_PER_LONG && start_pfn >> addr_width);
+	BUG_ON(addr_width < BITS_PER_LONG && last_pfn >> addr_width);
+	BUG_ON(start_pfn > last_pfn);
+
+	/* we don't need lock here; nobody else touches the iova range */
+	do {
+		first_pte = pte = dma_pfn_level_pte(domain, start_pfn, 1);
+		if (!pte) {
+			start_pfn = align_to_level(start_pfn + 1, 2);
+			continue;
+		}
+		do {
+			new_pte.val = (1ULL << 8);
+			old_pte.val = xchg(&pte->val, new_pte.val);
+				
+			any_mapped |= dma_pte_present(&old_pte) ||
+				dma_pte_need_invd(&old_pte); /* Need to be nicer */
+			start_pfn++;
+			pte++;
+		} while (start_pfn <= last_pfn && !first_pte_in_page(pte));
+	
+			domain_flush_cache(domain, first_pte,
+					   (void *)pte - (void *)first_pte);
 
 	} while (start_pfn && start_pfn <= last_pfn);
+	return 1/*any_mapped*/;
 }
 
 /* free page table pages. last level pte should already be cleared */
@@ -936,8 +1270,11 @@ static void __iommu_flush_context(struct intel_iommu *iommu,
 }
 
 /* return value determine if we need a write buffer flush */
+/* TODO: Does not free the iovas */
 static void __iommu_flush_iotlb(struct intel_iommu *iommu, u16 did,
-				u64 addr, unsigned int size_order, u64 type)
+				u64 addr, unsigned int size_order, u64 type,
+				struct list_head *iovas, 
+				struct list_head *to_free_iovas)
 {
 	int tlb_offset = ecap_iotlb_offset(iommu->ecap);
 	u64 val = 0, val_iva = 0;
@@ -1030,10 +1367,12 @@ static struct device_domain_info *iommu_support_dev_iotlb(
 
 static void iommu_enable_dev_iotlb(struct device_domain_info *info)
 {
+	int res;
 	if (!info)
 		return;
 
-	pci_enable_ats(info->dev, VTD_PAGE_SHIFT);
+	res = pci_enable_ats(info->dev, VTD_PAGE_SHIFT);
+	printk(KERN_ERR "%s : result of enable - %d", __func__, res);
 }
 
 static void iommu_disable_dev_iotlb(struct device_domain_info *info)
@@ -1051,6 +1390,8 @@ static void iommu_flush_dev_iotlb(struct dmar_domain *domain,
 	unsigned long flags;
 	struct device_domain_info *info;
 
+	return;
+
 	spin_lock_irqsave(&device_domain_lock, flags);
 	list_for_each_entry(info, &domain->devices, link) {
 		if (!info->dev || !pci_ats_enabled(info->dev))
@@ -1063,14 +1404,65 @@ static void iommu_flush_dev_iotlb(struct dmar_domain *domain,
 	spin_unlock_irqrestore(&device_domain_lock, flags);
 }
 
+#ifdef IOMMU_DEFERRED_LIST
 static void iommu_flush_iotlb_psi(struct intel_iommu *iommu, u16 did,
-				  unsigned long pfn, unsigned int pages, int map)
-{
-	unsigned int mask = ilog2(__roundup_pow_of_two(pages));
+				  unsigned long pfn, unsigned int pages, int map,
+				  struct iova *iova,
+				  struct list_head *to_free_iovas)
+{
+	unsigned long roundup_pages = __roundup_pow_of_two(pages);
+	unsigned int mask = ilog2(roundup_pages);
+	unsigned long first_pfn = pfn;
+	unsigned long last_pfn = (pfn + pages - 1);
 	uint64_t addr = (uint64_t)pfn << VTD_PAGE_SHIFT;
+	LIST_HEAD(iovas);
+	if (iova) {
+		list_add(&iova->link, &iovas);
+	}
+	BUG_ON(pages == 0);
+
+	while ((first_pfn >> mask) != (last_pfn >> mask)) {
+		mask++;
+	}	
 
+	/*
+	 * Fallback to domain selective flush if no PSI support or the size is
+	 * too big.
+	 * PSI requires page size to be 2 ^ x, and the base address is naturally
+	 * aligned to the size
+	 */
+	/*update_q_size(iommu);*/
+	if (!cap_pgsel_inv(iommu->cap) || mask > cap_max_amask_val(iommu->cap))
+		iommu->flush.flush_iotlb(iommu, did, 0, 0,
+						DMA_TLB_DSI_FLUSH, iova?(&iovas):NULL, NULL);
+	else
+		iommu->flush.flush_iotlb(iommu, did, addr, mask,
+						DMA_TLB_PSI_FLUSH, iova?(&iovas):NULL, NULL);
+
+	/*
+	 * In caching mode, domain ID 0 is reserved for non-present to present
+	 * mapping flush. Device IOTLB doesn't need to be flushed in this case.
+	 */
+	if (!cap_caching_mode(iommu->cap) || !map)
+		iommu_flush_dev_iotlb(iommu->domains[did], addr, mask);
+}
+#else
+static void iommu_flush_iotlb_psi(struct intel_iommu *iommu, u16 did,
+				  unsigned long pfn, unsigned int pages, int map,
+				  struct iova * iova,
+				  struct list_head *to_free_iovas)
+{
+	unsigned long roundup_pages = __roundup_pow_of_two(pages);
+	unsigned int mask = ilog2(roundup_pages);
+	unsigned long first_pfn = pfn;
+	unsigned long last_pfn = (pfn + pages - 1);
+	uint64_t addr = (uint64_t)pfn << VTD_PAGE_SHIFT;
 	BUG_ON(pages == 0);
 
+	while ((first_pfn >> mask) != (last_pfn >> mask)) {
+		mask++;
+	}	
+
 	/*
 	 * Fallback to domain selective flush if no PSI support or the size is
 	 * too big.
@@ -1079,18 +1471,19 @@ static void iommu_flush_iotlb_psi(struct intel_iommu *iommu, u16 did,
 	 */
 	if (!cap_pgsel_inv(iommu->cap) || mask > cap_max_amask_val(iommu->cap))
 		iommu->flush.flush_iotlb(iommu, did, 0, 0,
-						DMA_TLB_DSI_FLUSH);
+						DMA_TLB_DSI_FLUSH, NULL, NULL);
 	else
 		iommu->flush.flush_iotlb(iommu, did, addr, mask,
-						DMA_TLB_PSI_FLUSH);
+						DMA_TLB_PSI_FLUSH, NULL, NULL);
 
 	/*
-	 * In caching mode, changes of pages from non-present to present require
-	 * flush. However, device IOTLB doesn't need to be flushed in this case.
+	 * In caching mode, domain ID 0 is reserved for non-present to present
+	 * mapping flush. Device IOTLB doesn't need to be flushed in this case.
 	 */
 	if (!cap_caching_mode(iommu->cap) || !map)
 		iommu_flush_dev_iotlb(iommu->domains[did], addr, mask);
 }
+#endif 
 
 static void iommu_disable_protect_mem_regions(struct intel_iommu *iommu)
 {
@@ -1197,9 +1590,9 @@ void free_dmar_iommu(struct intel_iommu *iommu)
 
 			spin_lock_irqsave(&domain->iommu_lock, flags);
 			if (--domain->iommu_count == 0) {
-				if (domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE)
+/*				if (domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE)
 					vm_domain_exit(domain);
-				else
+				else*/
 					domain_exit(domain);
 			}
 			spin_unlock_irqrestore(&domain->iommu_lock, flags);
@@ -1244,6 +1637,7 @@ static struct dmar_domain *alloc_domain(void)
 
 	domain->nid = -1;
 	memset(&domain->iommu_bmp, 0, sizeof(unsigned long));
+/*	memset(domain->invd_range, 0, sizeof(domain->invd_range));*/
 	domain->flags = 0;
 
 	return domain;
@@ -1364,8 +1758,14 @@ static int domain_init(struct dmar_domain *domain, int guest_width)
 	unsigned long sagaw;
 
 	init_iova_domain(&domain->iovad, DMA_32BIT_PFN);
+
 	spin_lock_init(&domain->iommu_lock);
 
+	memset(domain->shared_cache, 0, DMAR_SHARED_CACHE_SETS *
+		sizeof(struct dmar_shared_cache_set));
+	INIT_LIST_HEAD(&domain->iova_cache);
+	domain->iova_cache_size = 0;
+
 	domain_reserve_special_ranges(domain);
 
 	/* calculate AGAW */
@@ -1420,11 +1820,17 @@ static void domain_exit(struct dmar_domain *domain)
 	/* destroy iovas */
 	put_iova_domain(&domain->iovad);
 
+	iommu = domain_get_iommu(domain);
+
+	if (!intel_iommu_strict)
+		flush_unmap_lock(iommu);
+
 	/* clear ptes */
-	dma_pte_clear_range(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
+	dma_pte_clear_range(domain, 0, DOMAIN_MAX_PFN(domain->gaw), 1);
 
 	/* free page tables */
-	dma_pte_free_pagetable(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
+	if (intel_iommu_strict)
+		dma_pte_free_pagetable(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
 
 	for_each_active_iommu(iommu, drhd)
 		if (test_bit(iommu->seq_id, &domain->iommu_bmp))
@@ -1545,7 +1951,8 @@ static int domain_context_mapping_one(struct dmar_domain *domain, int segment,
 					   (((u16)bus) << 8) | devfn,
 					   DMA_CCMD_MASK_NOBIT,
 					   DMA_CCMD_DEVICE_INVL);
-		iommu->flush.flush_iotlb(iommu, domain->id, 0, 0, DMA_TLB_DSI_FLUSH);
+		iommu->flush.flush_iotlb(iommu, domain->id, 0, 0, DMA_TLB_DSI_FLUSH,
+					NULL, NULL);
 	} else {
 		iommu_flush_write_buffer(iommu);
 	}
@@ -1641,7 +2048,7 @@ static int domain_context_mapped(struct pci_dev *pdev)
 
 /* Returns a number of VTD pages, but aligned to MM page size */
 static inline unsigned long aligned_nrpages(unsigned long host_addr,
-					    size_t size)
+					    UL_SIZE size)
 {
 	host_addr &= ~PAGE_MASK;
 	return PAGE_ALIGN(host_addr + size) >> VTD_PAGE_SHIFT;
@@ -1655,6 +2062,7 @@ static int __domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 	phys_addr_t uninitialized_var(pteval);
 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
 	unsigned long sg_res;
+	int need_invd = 0;
 
 	BUG_ON(addr_width < BITS_PER_LONG && (iov_pfn + nr_pages - 1) >> addr_width);
 
@@ -1662,12 +2070,14 @@ static int __domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 		return -EINVAL;
 
 	prot &= DMA_PTE_READ | DMA_PTE_WRITE | DMA_PTE_SNP;
+	
+	/*printk(KERN_ERR "%s : sg = %d\n", __func__, sg==NULL);*/
 
 	if (sg)
 		sg_res = 0;
 	else {
 		sg_res = nr_pages + 1;
-		pteval = ((phys_addr_t)phys_pfn << VTD_PAGE_SHIFT) | prot;
+		pteval = ((phys_addr_t)phys_pfn << VTD_PAGE_SHIFT) | prot | (1ULL << 8);
 	}
 
 	while (nr_pages--) {
@@ -1677,26 +2087,34 @@ static int __domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 			sg_res = aligned_nrpages(sg->offset, sg->length);
 			sg->dma_address = ((dma_addr_t)iov_pfn << VTD_PAGE_SHIFT) + sg->offset;
 			sg->dma_length = sg->length;
-			pteval = page_to_phys(sg_page(sg)) | prot;
+			pteval = page_to_phys(sg_page(sg)) | prot | (1ULL << 8);
 		}
 		if (!pte) {
-			first_pte = pte = pfn_to_dma_pte(domain, iov_pfn);
+			first_pte = pte = pfn_to_dma_pte(domain, iov_pfn, 1);
 			if (!pte)
 				return -ENOMEM;
 		}
 		/* We don't need lock here, nobody else
 		 * touches the iova range
 		 */
-		tmp = cmpxchg64_local(&pte->val, 0ULL, pteval);
-		if (tmp) {
-			static int dumps = 5;
-			printk(KERN_CRIT "ERROR: DMA PTE for vPFN 0x%lx already set (to %llx not %llx)\n",
-			       iov_pfn, tmp, (unsigned long long)pteval);
-			if (dumps) {
-				dumps--;
-				debug_dma_dump_mappings(NULL);
+		/*printk(KERN_ERR "%s : iov_pfn %lx    pteval %lx  nr_pages %lx  prot %d\n", __func__, iov_pfn, pteval, nr_pages, prot);*/
+		tmp = xchg(&pte->val, pteval);
+
+		/* Check if the entry marks that invalidation was not completed for it */
+		if (
+#ifndef IOMMU_DEFERRED_LIST
+			!intel_iommu_strict &&
+#endif			
+			(domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE) &&
+			(dma_pte_need_invd((struct dma_pte*)&tmp))) {
+			struct intel_iommu *iommu = domain_get_iommu(domain);
+
+			flush_unmap_lock(iommu);
+			while (dma_pte_need_invd((struct dma_pte *)&tmp)) {
+				iommu->flush.flush_process(iommu);
+				tmp = xchg(&pte->val, pteval); /* Put again since it is cleared */
+				/* TODO: in unmap we can free pagetables and only afterwards set */
 			}
-			WARN_ON(1);
 		}
 		pte++;
 		if (!nr_pages || first_pte_in_page(pte)) {
@@ -1710,7 +2128,7 @@ static int __domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
 		if (!sg_res)
 			sg = sg_next(sg);
 	}
-	return 0;
+	return need_invd;
 }
 
 static inline int domain_sg_mapping(struct dmar_domain *domain, unsigned long iov_pfn,
@@ -1735,7 +2153,7 @@ static void iommu_detach_dev(struct intel_iommu *iommu, u8 bus, u8 devfn)
 	clear_context_table(iommu, bus, devfn);
 	iommu->flush.flush_context(iommu, 0, 0, 0,
 					   DMA_CCMD_GLOBAL_INVL);
-	iommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH);
+	iommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH, NULL, NULL);
 }
 
 static void domain_remove_dev_info(struct dmar_domain *domain)
@@ -1927,6 +2345,8 @@ static int iommu_domain_identity_map(struct dmar_domain *domain,
 {
 	unsigned long first_vpfn = start >> VTD_PAGE_SHIFT;
 	unsigned long last_vpfn = end >> VTD_PAGE_SHIFT;
+	struct intel_iommu *iommu;	
+	int res;
 
 	if (!reserve_iova(&domain->iovad, dma_to_mm_pfn(first_vpfn),
 			  dma_to_mm_pfn(last_vpfn))) {
@@ -1940,11 +2360,15 @@ static int iommu_domain_identity_map(struct dmar_domain *domain,
 	 * RMRR range might have overlap with physical memory range,
 	 * clear it first
 	 */
-	dma_pte_clear_range(domain, first_vpfn, last_vpfn);
+	dma_pte_clear_range(domain, first_vpfn, last_vpfn, 1);
 
-	return domain_pfn_mapping(domain, first_vpfn, first_vpfn,
+	res = domain_pfn_mapping(domain, first_vpfn, first_vpfn,
 				  last_vpfn - first_vpfn + 1,
 				  DMA_PTE_READ|DMA_PTE_WRITE);
+
+	iommu = domain_get_iommu(domain);
+
+	return is_error(res) ? res : 0;
 }
 
 static int iommu_prepare_identity_map(struct pci_dev *pdev,
@@ -1971,7 +2395,7 @@ static int iommu_prepare_identity_map(struct pci_dev *pdev,
 	printk(KERN_INFO
 	       "IOMMU: Setting identity map for device %s [0x%Lx - 0x%Lx]\n",
 	       pci_name(pdev), start, end);
-	
+
 	if (end < start) {
 		WARN(1, "Your BIOS is broken; RMRR ends before it starts!\n"
 			"BIOS vendor: %s; Ver: %s; Product Version: %s\n",
@@ -2217,6 +2641,12 @@ static int __init iommu_prepare_static_identity_mapping(int hw)
 	return 0;
 }
 
+static void __iommu_flush_process(struct intel_iommu* iommu) 
+{
+	BUG();
+}
+
+
 int __init init_dmars(void)
 {
 	struct dmar_drhd_unit *drhd;
@@ -2248,12 +2678,24 @@ int __init init_dmars(void)
 		goto error;
 	}
 
+#ifdef IOMMU_DEFERRED_LIST
+	deferred_flush_lists = kzalloc(g_num_of_iommus * 
+		sizeof(struct deferred_flush_entry), GFP_KERNEL);
+	if (!deferred_flush_lists) {
+		ret = -ENOMEM;
+		goto error;
+	}
+	for (i = 0; i < g_num_of_iommus; i++)
+		INIT_LIST_HEAD(&deferred_flush_lists[i].list_head);
+#else
 	deferred_flush = kzalloc(g_num_of_iommus *
 		sizeof(struct deferred_flush_tables), GFP_KERNEL);
+
 	if (!deferred_flush) {
 		ret = -ENOMEM;
 		goto error;
 	}
+#endif
 
 	for_each_drhd_unit(drhd) {
 		if (drhd->ignored)
@@ -2278,6 +2720,8 @@ int __init init_dmars(void)
 		}
 		if (!ecap_pass_through(iommu->ecap))
 			hw_pass_through = 0;
+
+		iommu->pending_invd = 0;
 	}
 
 	/*
@@ -2321,16 +2765,16 @@ int __init init_dmars(void)
 			 */
 			iommu->flush.flush_context = __iommu_flush_context;
 			iommu->flush.flush_iotlb = __iommu_flush_iotlb;
-			printk(KERN_INFO "IOMMU %d 0x%Lx: using Register based "
+			iommu->flush.flush_process = __iommu_flush_process;
+			printk(KERN_INFO "IOMMU 0x%Lx: using Register based "
 			       "invalidation\n",
-				iommu->seq_id,
 			       (unsigned long long)drhd->reg_base_addr);
 		} else {
 			iommu->flush.flush_context = qi_flush_context;
 			iommu->flush.flush_iotlb = qi_flush_iotlb;
-			printk(KERN_INFO "IOMMU %d 0x%Lx: using Queued "
+			iommu->flush.flush_process = qi_flush_process;
+			printk(KERN_INFO "IOMMU 0x%Lx: using Queued "
 			       "invalidation\n",
-				iommu->seq_id,
 			       (unsigned long long)drhd->reg_base_addr);
 		}
 	}
@@ -2410,7 +2854,8 @@ int __init init_dmars(void)
 		iommu_set_root_entry(iommu);
 
 		iommu->flush.flush_context(iommu, 0, 0, 0, DMA_CCMD_GLOBAL_INVL);
-		iommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH);
+		iommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH, 
+			NULL, NULL);
 
 		ret = iommu_enable_translation(iommu);
 		if (ret)
@@ -2431,6 +2876,84 @@ error:
 	return ret;
 }
 
+#define DEFERRED_NS (10000000UL)
+
+static void free_deferred_iovas(struct intel_iommu* iommu, struct dmar_domain* domain)
+{
+	struct iova* iova;
+	int i,j;
+	struct dmar_shared_cache_set* set;
+	unsigned long flags;
+	static cycles_t prev_ns = 0;
+	uint64_t ns;
+	cycles_t cycles;
+	struct list_head to_remove_list, *pos;
+
+	INIT_LIST_HEAD(&to_remove_list);
+	
+	if (intel_iommu_no_shared && intel_iommu_no_victim)
+		return;
+
+	if (list_empty(&domain->iova_cache)) 
+		return;
+
+	ns = __cycles_2_ns(cycles = get_cycles());
+	
+	if (ns - prev_ns <= proc_control_vals[IOMMU_VTIME_CONTROL] &&
+		domain->iova_cache_size < proc_control_vals[IOMMU_VSIZE_CONTROL])
+		return;
+
+	spin_lock_irqsave(&domain->iovad.iova_rbtree_lock, flags);
+	if (list_empty(&domain->iova_cache)) 
+		goto out_unlock;
+
+	list_for_each(pos, &domain->iova_cache) {
+		iova = list_entry(pos, struct iova, link);
+		if (ns - iova->free_time <= proc_control_vals[IOMMU_VTIME_CONTROL]
+			&& domain->iova_cache_size < 
+				proc_control_vals[IOMMU_VSIZE_CONTROL]) 
+			break;
+		domain->iova_cache_size--;
+	}
+
+	BUG_ON(domain->iova_cache_size < 0);
+	
+	if (pos->prev == &domain->iova_cache) 
+		goto out_unlock;
+
+	pos = pos->prev;
+	list_cut_position(&to_remove_list, &domain->iova_cache,	pos);
+
+	list_for_each(pos, &to_remove_list) {
+		iova=list_entry(pos, struct iova, link);
+		set = &domain->shared_cache[iova->ppfn_lo % DMAR_SHARED_CACHE_SETS];
+		for (i=0; i<DMAR_SHARED_CACHE_WAYS; i++) 
+			if (iova == set->iova[i])
+				break;
+
+		/* Remove from cache if found */
+		if (i != DMAR_SHARED_CACHE_WAYS) {
+			for (j=i; j<DMAR_SHARED_CACHE_WAYS-1; j++) 
+				set->iova[j] = set->iova[j+1];
+			set->iova[DMAR_SHARED_CACHE_WAYS-1] = NULL;
+		}
+	}
+
+out_unlock:
+	prev_ns = ns;
+	spin_unlock_irqrestore(&domain->iovad.iova_rbtree_lock, flags);
+
+	while (!list_empty(&to_remove_list)) {
+		/* Free IOVA */
+		iova=list_first_entry(&to_remove_list, struct iova, link);
+		list_del_init(&iova->link);
+		free_iova_unmap(iommu, domain, iova, 1);
+	}
+}
+
+
+
+
 /* This takes a number of _MM_ pages, not VTD pages */
 static struct iova *intel_alloc_iova(struct device *dev,
 				     struct dmar_domain *domain,
@@ -2449,11 +2972,12 @@ static struct iova *intel_alloc_iova(struct device *dev,
 		 * from higher range
 		 */
 		iova = alloc_iova(&domain->iovad, nrpages,
-				  IOVA_PFN(DMA_BIT_MASK(32)), 1);
+				  IOVA_PFN(DMA_BIT_MASK(32)), 1, domain);
 		if (iova)
 			return iova;
 	}
-	iova = alloc_iova(&domain->iovad, nrpages, IOVA_PFN(dma_mask), 1);
+	iova = alloc_iova(&domain->iovad, nrpages, IOVA_PFN(dma_mask), 1,
+		domain);
 	if (unlikely(!iova)) {
 		printk(KERN_ERR "Allocating %ld-page iova for %s failed",
 		       nrpages, pci_name(pdev));
@@ -2560,20 +3084,178 @@ static int iommu_no_mapping(struct device *dev)
 	return 0;
 }
 
+struct iova* add_shared_iova(struct intel_iommu* iommu,
+	struct dmar_domain *domain, unsigned long pfn,
+	struct iova* iova, uint8_t prot, int lock)
+{
+	unsigned long flags = 0;
+	struct dmar_shared_cache_set* set;
+	struct iova* to_free_iova = NULL;
+	int i;
+
+	if (iova->pfn_hi != iova->pfn_lo)
+		return NULL;
+
+	INIT_LIST_HEAD(&iova->link);
+	iova->ppfn_lo = pfn;
+	iova->prot = prot;
+	set = &domain->shared_cache[pfn % DMAR_SHARED_CACHE_SETS];
+	if (lock)
+		spin_lock_irqsave(&domain->iovad.iova_rbtree_lock, flags);
+
+	if (lock)
+		iova->share_cnt = 1;
+	to_free_iova = set->iova[DMAR_SHARED_CACHE_WAYS-1];
+	if (to_free_iova && to_free_iova->share_cnt > 0)
+		to_free_iova = NULL;
+	for (i=DMAR_SHARED_CACHE_WAYS-1; i>0; i--) {
+		set->iova[i]=set->iova[i-1];
+	}
+	set->iova[0] = iova;
+	
+	if (to_free_iova && !list_empty(&to_free_iova->link)) {
+		domain->iova_cache_size--;
+		list_del_init(&to_free_iova->link);
+		to_free_iova->free_time = 0;
+	}
+
+	if (lock)
+		spin_unlock_irqrestore(&domain->iovad.iova_rbtree_lock, flags);
+
+	if (to_free_iova && lock) {
+		free_iova_unmap(iommu, domain, to_free_iova, 0);
+	}
+
+	return lock ? NULL: to_free_iova;
+}
+
+
+
+static struct iova* get_shared_iova(struct intel_iommu *iommu,
+					struct dmar_domain* domain,
+					unsigned long pfn, uint64_t size, uint8_t prot,
+					uint64_t dma_mask)
+{
+	unsigned long flags;
+	struct dmar_shared_cache_set* set = &domain->shared_cache[pfn % DMAR_SHARED_CACHE_SETS];
+	struct iova* iova = NULL;
+	int i,j;
+	if (size != 1)
+		return NULL;
+	if (intel_iommu_no_shared && intel_iommu_no_victim)
+		return NULL;
+
+	spin_lock_irqsave(&domain->iovad.iova_rbtree_lock, flags);
+	for (i=0; i < DMAR_SHARED_CACHE_WAYS; i++) {
+		if (set->iova[i] == NULL)
+			continue;/*break;*/ /* Vacant slot */
+		if (set->iova[i]->ppfn_lo == pfn && set->iova[i]->prot == prot &&
+			set->iova[i]->pfn_lo <= dma_mask) {
+			iova = set->iova[i];
+			break; /* found match */
+		}
+	}
+	
+	if (iova) {
+		if (!intel_iommu_no_shared) {
+			/* shifting according to LRU */
+			for (j=i; j>0; j--) 
+				set->iova[j]=set->iova[j-1];	
+			set->iova[0] = iova;
+		}
+		if (iova->share_cnt == 0 && !list_empty(&iova->link)) {
+			domain->iova_cache_size--;
+			list_del_init(&iova->link);
+			iova->free_time = 0;
+			counters_data[IOMMU_VICTIM_HITS].events_counter++;	
+		} else {
+			counters_data[IOMMU_SHARED_HITS].events_counter++;	
+		}
+		iova->share_cnt++;
+	}
+
+
+	if (!iova)
+		counters_data[IOMMU_MISSES].events_counter++;	
+	spin_unlock_irqrestore(&domain->iovad.iova_rbtree_lock, flags);
+
+	return iova;
+}
+
+static void free_shared_iova(struct intel_iommu* iommu, struct dmar_domain *domain, 
+	struct iova* iova)
+{
+	unsigned long flags, pfn = iova->ppfn_lo;
+	struct dmar_shared_cache_set* set = &domain->shared_cache[pfn % DMAR_SHARED_CACHE_SETS];
+	int j, i, found = 0;
+	struct iova *to_free_iova = NULL;
+	if (iova->pfn_hi != iova->pfn_lo ||
+		(intel_iommu_no_shared && intel_iommu_no_victim)) {
+		free_iova_unmap(iommu, domain, iova, 1);
+		return;
+	}
+	spin_lock_irqsave(&domain->iovad.iova_rbtree_lock, flags);
+	if (iova->share_cnt==0)
+		printk(KERN_ERR "%s : iova->share_cnt < 0", __func__);
+		
+	iova->share_cnt--;
+	if (iova->share_cnt==0) {
+		for (i=0; i<DMAR_SHARED_CACHE_WAYS; i++)
+			if (set->iova[i] == iova) {
+				found = 1;
+				break;
+			}
+
+		if (!intel_iommu_no_victim) {
+				/* victim is enabled */
+			if (found || intel_iommu_no_shared) {
+				/* found in the cache - need to add to deferred cleanup */
+				if (!found)
+					to_free_iova = 
+						add_shared_iova(iommu, domain, pfn,
+						iova, iova->prot, 0);
+
+				iova->free_time = __cycles_2_ns(get_cycles());
+				list_add_tail(&iova->link, &domain->iova_cache);
+				/* increase size */
+				domain->iova_cache_size++;
+			} else 
+				to_free_iova = iova;  /* victim, shared, !found */
+		} else {
+			to_free_iova = iova;
+			if (found) {
+				/* intel_iommu_no_victim == 1 */
+				/* This is for the case of no deferred IOVA unmappings */
+				for (j=i; j<DMAR_SHARED_CACHE_WAYS-1; j++)
+					set->iova[j]=set->iova[j+1];
+				set->iova[DMAR_SHARED_CACHE_WAYS-1] = NULL;
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&domain->iovad.iova_rbtree_lock, flags);
+
+	if (to_free_iova) 
+		free_iova_unmap(iommu, domain, to_free_iova, 0); /* not found*/
+}
+
+
+
 static dma_addr_t __intel_map_single(struct device *hwdev, phys_addr_t paddr,
-				     size_t size, int dir, u64 dma_mask)
+				     UL_SIZE size, int dir, u64 dma_mask)
 {
 	struct pci_dev *pdev = to_pci_dev(hwdev);
 	struct dmar_domain *domain;
 	phys_addr_t start_paddr;
-	struct iova *iova;
+	struct iova *iova = NULL;
 	int prot = 0;
 	int ret;
 	struct intel_iommu *iommu;
 	unsigned long paddr_pfn = paddr >> PAGE_SHIFT;
+	int shared = 0;
 
 	BUG_ON(dir == DMA_NONE);
-
+	
 	if (iommu_no_mapping(hwdev))
 		return paddr;
 
@@ -2584,11 +3266,6 @@ static dma_addr_t __intel_map_single(struct device *hwdev, phys_addr_t paddr,
 	iommu = domain_get_iommu(domain);
 	size = aligned_nrpages(paddr, size);
 
-	iova = intel_alloc_iova(hwdev, domain, dma_to_mm_pfn(size),
-				pdev->dma_mask);
-	if (!iova)
-		goto error;
-
 	/*
 	 * Check if DMAR supports zero-length reads on write only
 	 * mappings..
@@ -2598,22 +3275,49 @@ static dma_addr_t __intel_map_single(struct device *hwdev, phys_addr_t paddr,
 		prot |= DMA_PTE_READ;
 	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)
 		prot |= DMA_PTE_WRITE;
+
+	free_deferred_iovas(iommu, domain);
+ 	if (!intel_iommu_no_shared || !intel_iommu_no_victim)
+ 		iova = get_shared_iova(iommu, domain, 
+			paddr_pfn, size, prot, pdev->dma_mask);
+
+ 	if (!iova) { 
+ 		iova = intel_alloc_iova(hwdev, domain, dma_to_mm_pfn(size),
+ 					pdev->dma_mask);
+ 		BUG_ON(iova==NULL);
+ 		if (!intel_iommu_no_shared)
+ 			add_shared_iova(iommu, domain, paddr_pfn, iova, prot, 1);
+ 		else if (iova->pfn_hi == iova->pfn_lo)
+ 			iova->share_cnt = 1;			
+ 	} else {
+ 		shared = 1;
+ 	}
+
+ 	if (!iova) {
+		printk(KERN_ERR "%s : no iova\n", __func__);
+ 		goto error;
+	}
+
+
 	/*
 	 * paddr - (paddr + size) might be partial page, we should map the whole
 	 * page.  Note: if two part of one page are separately mapped, we
 	 * might have two guest_addr mapping to the same host paddr, but this
 	 * is not a big problem
 	 */
-	ret = domain_pfn_mapping(domain, mm_to_dma_pfn(iova->pfn_lo),
+ 	if (!shared) {
+		ret = domain_pfn_mapping(domain, mm_to_dma_pfn(iova->pfn_lo),
 				 mm_to_dma_pfn(paddr_pfn), size, prot);
-	if (ret)
-		goto error;
+		if (is_error(ret))
+			goto error;
 
 	/* it's a non-present to present mapping. Only flush if caching mode */
-	if (cap_caching_mode(iommu->cap))
-		iommu_flush_iotlb_psi(iommu, domain->id, mm_to_dma_pfn(iova->pfn_lo), size, 1);
-	else
-		iommu_flush_write_buffer(iommu);
+		if (cap_caching_mode(iommu->cap))
+			iommu_flush_iotlb_psi(iommu, domain->id, 
+				mm_to_dma_pfn(iova->pfn_lo), size, 1, NULL, NULL);
+		else
+			iommu_flush_write_buffer(iommu);
+	}
 
 	start_paddr = (phys_addr_t)iova->pfn_lo << PAGE_SHIFT;
 	start_paddr += paddr & ~PAGE_MASK;
@@ -2621,71 +3325,282 @@ static dma_addr_t __intel_map_single(struct device *hwdev, phys_addr_t paddr,
 
 error:
 	if (iova)
-		__free_iova(&domain->iovad, iova);
+		__free_iova(&domain->iovad, iova, 1);
 	printk(KERN_ERR"Device %s request: %zx@%llx dir %d --- failed\n",
 		pci_name(pdev), size, (unsigned long long)paddr, dir);
 	return 0;
 }
 
 static dma_addr_t intel_map_page(struct device *dev, struct page *page,
-				 unsigned long offset, size_t size,
+				 unsigned long offset, UL_SIZE size,
 				 enum dma_data_direction dir,
 				 struct dma_attrs *attrs)
 {
-	return __intel_map_single(dev, page_to_phys(page) + offset, size,
+	dma_addr_t res;
+
+	update_counter(IOMMU_MAP_COUNTER, 1);	
+	
+	res = __intel_map_single(dev, page_to_phys(page) + offset, size,
 				  dir, to_pci_dev(dev)->dma_mask);
+	update_counter(IOMMU_MAP_COUNTER, 0);	
+
+	return res;
 }
 
-static void flush_unmaps(void)
+#ifdef IOMMU_DEFERRED_LIST
+static inline void flush_unmap(struct intel_iommu *iommu,
+			struct list_head *to_free_iovas)
 {
-	int i, j;
-
-	timer_on = 0;
-
-	/* just flush them all */
-	for (i = 0; i < g_num_of_iommus; i++) {
-		struct intel_iommu *iommu = g_iommus[i];
-		if (!iommu)
-			continue;
-
-		if (!deferred_flush[i].next)
-			continue;
+	int i;
+	struct iova *iova, *temp;
+	if (!iommu) 
+		return;
+	i = iommu->seq_id;
+	if (deferred_flush_lists[i].size == 0)
+		return;
 
-		/* In caching mode, global flushes turn emulation expensive */
-		if (!cap_caching_mode(iommu->cap))
-			iommu->flush.flush_iotlb(iommu, 0, 0, 0,
-					 DMA_TLB_GLOBAL_FLUSH);
-		for (j = 0; j < deferred_flush[i].next; j++) {
+	/* In caching mode, global flushes turn emulation expensive */
+	if (ecap_dev_iotlb_support(iommu->ecap) || 
+		cap_caching_mode(iommu->cap))
+	{
+		list_for_each_entry_safe(iova, temp, &deferred_flush_lists[i].list_head, link) {
 			unsigned long mask;
-			struct iova *iova = deferred_flush[i].iova[j];
-			struct dmar_domain *domain = deferred_flush[i].domain[j];
+	/*		struct iova *iova = list_first_entry(&deferred_flush_lists[i], struct iova, link); */
+			struct dmar_domain *domain;
+			struct iova_domain *iovad;
+			domain  = iova->domain;
+			iovad = &domain->iovad;
 
 			/* On real hardware multiple invalidations are expensive */
-			if (cap_caching_mode(iommu->cap))
+			if (cap_caching_mode(iommu->cap)) {
+		/*		LIST_HEAD(temp_head);
+				printk(KERN_ERR "%s : flush %lx %lx\n", __func__, iova->pfn_lo, iova->pfn_hi);
+				list_add(&iova->link, &temp_head);*/
+				list_del_init(&iova->link);
 				iommu_flush_iotlb_psi(iommu, domain->id,
-				iova->pfn_lo, iova->pfn_hi - iova->pfn_lo + 1, 0);
-			else {
+					iova->pfn_lo, iova->pfn_hi - iova->pfn_lo + 1, 0, iova,
+					to_free_iovas);
+			} else {
 				mask = ilog2(mm_to_dma_pfn(iova->pfn_hi - iova->pfn_lo + 1));
-				iommu_flush_dev_iotlb(deferred_flush[i].domain[j],
-						(uint64_t)iova->pfn_lo << PAGE_SHIFT, mask);
+				iommu_flush_dev_iotlb(domain,
+					(uint64_t)iova->pfn_lo << PAGE_SHIFT, mask);
 			}
-			__free_iova(&deferred_flush[i].domain[j]->iovad, iova);
 		}
-		deferred_flush[i].next = 0;
 	}
+	list_size -= deferred_flush_lists[i].size;
+
+	if (!cap_caching_mode(iommu->cap)) {
+		iommu->flush.flush_iotlb(iommu, 0, 0, 0,
+				 DMA_TLB_GLOBAL_FLUSH, &deferred_flush_lists[i].list_head,
+				 to_free_iovas);
+	}
+
+	INIT_LIST_HEAD(&deferred_flush_lists[i].list_head);
+	deferred_flush_lists[i].size = 0;
+}
+
+inline void free_iovas(struct list_head* iovas)
+{
+	if (!iovas)
+		return;
+
+	while(!list_empty(iovas)) {
+		struct iova *iova = list_first_entry(iovas, struct iova, link); 
+		BUG_ON(!iova);
+		BUG_ON(!iova->domain);
+		if (iova->orphan) {
+			dma_pte_clear_range(iova->domain, iova->pfn_lo,
+				iova->pfn_hi, 0);
+               	}
+		__free_iova(&iova->domain->iovad, iova, 1);
+	}
+}
+
+#else
+static inline void flush_unmap(struct intel_iommu *iommu,
+			struct list_head *to_free_iovas)
+{
+	int j,i;
+	if (!iommu) 
+		return;
+	if (!deferred_flush[iommu->seq_id].next)
+		return;
+	i = iommu->seq_id;
+
+	/* In caching mode, global flushes turn emulation expensive */
+	if (!cap_caching_mode(iommu->cap)) {
+		iommu->flush.flush_iotlb(iommu, 0, 0, 0,
+				 DMA_TLB_GLOBAL_FLUSH, NULL, NULL);
+	}
+	for (j = 0; j < deferred_flush[i].next; j++) {
+		unsigned long mask;
+		struct iova *iova = deferred_flush[i].iova[j];
+		struct dmar_domain *domain = deferred_flush[i].domain[j];
+
+               	/* On real hardware multiple invalidations are expensive */
+                if (cap_caching_mode(iommu->cap))
+                	iommu_flush_iotlb_psi(iommu, domain->id,
+        			iova->pfn_lo, iova->pfn_hi - iova->pfn_lo + 1, 
+			0, NULL, NULL);
+              	else if (iova) {
+                      	mask = ilog2(mm_to_dma_pfn(iova->pfn_hi - iova->pfn_lo + 1));
+                       	iommu_flush_dev_iotlb(deferred_flush[i].domain[j],
+         			(uint64_t)iova->pfn_lo << PAGE_SHIFT, mask);
+                       	}
+/*
+		if (domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE)
+			printk(KERN_ERR "%s : should unmap %lx - %lx", __func__, iova->pfn_lo, iova->pfn_hi);
+*/
+              	if (iova) {
+			if (iova->orphan) {
+/*
+				printk(KERN_ERR "%s : clearing %lx - %lx", __func__, iova->pfn_lo, iova->pfn_hi);
+*/
+
+				dma_pte_clear_range(domain, iova->pfn_lo,
+					iova->pfn_hi, 0);
+			}
+				
+			__free_iova(&deferred_flush[i].domain[j]->
+               	                iovad, iova, 1);
+			deferred_flush[i].domain[j] = NULL;
+			deferred_flush[i].iova[j] = NULL;
+               	}
+	}
+
+	list_size -= deferred_flush[i].next;
+	deferred_flush[i].next = 0;
+}
+#endif
+
+static void flush_unmaps(struct list_head *to_free_iovas)
+{
+	int i;
+
+	timer_on = 0;
+
+	/* just flush them all */
+	for (i = 0; i < g_num_of_iommus; i++) {
+		flush_unmap(g_iommus[i], to_free_iovas);
+	}
+}
+
+static void flush_unmaps_lock(void)
+{
+	unsigned long flags;
+	LIST_HEAD(to_free_iovas);
+	spin_lock_irqsave(&async_umap_flush_lock, flags);
+	flush_unmaps(&to_free_iovas);
+	spin_unlock_irqrestore(&async_umap_flush_lock, flags);
+#ifdef IOMMU_DEFERRED_LIST
+	free_iovas(&to_free_iovas);
+#endif
+}
+
+static void flush_unmap_lock(struct intel_iommu *iommu)
+{
+	unsigned long flags;
+	LIST_HEAD(to_free_iovas);
+	spin_lock_irqsave(&async_umap_flush_lock, flags);
+	flush_unmap(iommu, &to_free_iovas);
+	spin_unlock_irqrestore(&async_umap_flush_lock, flags);
+#ifdef IOMMU_DEFERRED_LIST
+	free_iovas(&to_free_iovas);
+#endif
+}
+
+static inline void flush_unmaps_timeout(unsigned long data)
+{
+	unsigned long flags;
+	LIST_HEAD(to_free_iovas);
+	if (!spin_trylock_irqsave(&async_umap_flush_lock, flags)) {
+		mod_timer(&unmap_timer, jiffies + 
+			nsecs_to_jiffies(proc_control_vals[IOMMU_DTIME_CONTROL]));
+		timer_on = 1;
+		return;
+	}
+	flush_unmaps(&to_free_iovas);
+	spin_unlock_irqrestore(&async_umap_flush_lock, flags);
+#ifdef IOMMU_DEFERRED_LIST
+	free_iovas(&to_free_iovas);
+#endif
+}
+
+
+#ifdef IOMMU_DEFERRED_LIST
+static int iommu_assistant_cpu = 0;
+static int iommu_assistant_recalc;
+#define IOMMU_ASSSITANT_RECALC_INTERVAL 	100
+
+
+static int choose_async_cpu(void)
+{
+	int checked_cpu;
+	unsigned long checked_load, load;
+	const struct cpumask *nodemask = cpumask_of_node(cpu_to_node(smp_processor_id()));
+	/*if (iommu_assistant_recalc-- > 0)
+		return iommu_assistant_cpu;*/
 
-	list_size = 0;
+	load = cpu_load(iommu_assistant_cpu);
+	iommu_assistant_cpu = smp_processor_id();
+	for_each_cpu_and(checked_cpu, nodemask, cpu_active_mask) {
+		checked_load = cpu_load(checked_cpu);
+
+		if (cpumask_test_cpu(checked_cpu, &current->cpus_allowed) && 
+			checked_load < load) {
+				iommu_assistant_cpu = checked_cpu;
+				load = checked_load;
+		}
+	}
+	return iommu_assistant_cpu;
 }
 
-static void flush_unmaps_timeout(unsigned long data)
+
+static void add_unmap(struct dmar_domain *dom, struct iova *iova)
 {
 	unsigned long flags;
+	int iommu_id;
+	int call_unmap;
+	struct intel_iommu *iommu;
+	LIST_HEAD(to_free_iovas);
+	
+	BUG_ON(dom != iova->domain);
 
 	spin_lock_irqsave(&async_umap_flush_lock, flags);
-	flush_unmaps();
+	if (list_size >= proc_control_vals[IOMMU_DSIZE_CONTROL] /*HIGH_WATER_MARK*/) 
+		flush_unmaps(&to_free_iovas);
+
+	iommu = domain_get_iommu(dom);
+	iommu_id = iommu->seq_id;
+
+	list_add(&iova->link, &deferred_flush_lists[iommu_id].list_head);
+	deferred_flush_lists[iommu_id].size++;
+/*
+	call_unmap = (list_size >= LOW_WATER_MARK);
+*/
+	if (!timer_on) {
+		mod_timer(&unmap_timer, jiffies + 
+			nsecs_to_jiffies(proc_control_vals[IOMMU_DTIME_CONTROL]));
+		timer_on = 1;
+	}
+	
+	list_size++;
 	spin_unlock_irqrestore(&async_umap_flush_lock, flags);
+	free_iovas(&to_free_iovas);
+
+#if 0
+	if (call_unmap) {
+/*		int preferred_cpu = get_nohz_load_balancer();*/
+/*		int preferred_cpu = 3;*/
+/*		if (preferred_cpu < 0)*/
+		int preferred_cpu = choose_async_cpu();
+		smp_call_function_single(preferred_cpu, flush_unmap_func, iommu, 0);
+	}
+#endif
 }
 
+
+#else
 static void add_unmap(struct dmar_domain *dom, struct iova *iova)
 {
 	unsigned long flags;
@@ -2694,8 +3609,11 @@ static void add_unmap(struct dmar_domain *dom, struct iova *iova)
 
 	spin_lock_irqsave(&async_umap_flush_lock, flags);
 	if (list_size == HIGH_WATER_MARK)
-		flush_unmaps();
-
+		flush_unmaps(NULL);
+/*
+	if (dom->flags & DOMAIN_FLAG_VIRTUAL_MACHINE)
+		printk(KERN_ERR "%s : unmap %lx - %lx", __func__, iova->pfn_lo, iova->pfn_hi);
+*/
 	iommu = domain_get_iommu(dom);
 	iommu_id = iommu->seq_id;
 
@@ -2705,13 +3623,57 @@ static void add_unmap(struct dmar_domain *dom, struct iova *iova)
 	deferred_flush[iommu_id].next++;
 
 	if (!timer_on) {
-		mod_timer(&unmap_timer, jiffies + msecs_to_jiffies(10));
+		mod_timer(&unmap_timer, jiffies + 
+			nsecs_to_jiffies(proc_control_vals[IOMMU_DTIME_CONTROL]));
 		timer_on = 1;
 	}
 	list_size++;
 	spin_unlock_irqrestore(&async_umap_flush_lock, flags);
 }
 
+#endif
+
+static void add_unmap_range(struct dmar_domain *dom, unsigned long s_pfn,
+	unsigned long size)
+{
+	struct iova *iova = alloc_orphan_iova(&dom->iovad, s_pfn, 
+		s_pfn+size-1, dom);
+	/*printk(KERN_ERR "%s : adding %lx - %lx", __func__, s_pfn, s_pfn+size-1);*/
+	add_unmap(dom, iova);
+}
+
+void free_iova_unmap(struct intel_iommu *iommu, struct dmar_domain *domain, 
+ 	struct iova *iova, int lock)
+{
+	unsigned long start_pfn, last_pfn;
+ 	start_pfn = mm_to_dma_pfn(iova->pfn_lo);
+ 	last_pfn = mm_to_dma_pfn(iova->pfn_hi + 1) - 1;
+	/* clear the whole page */
+	dma_pte_clear_range_need_invd(domain, start_pfn, last_pfn);
+
+	/* free page tables */
+	dma_pte_free_pagetable(domain, start_pfn, last_pfn);
+	
+	if (intel_iommu_strict) {
+#ifdef IOMMU_DEFERRED_LIST
+		iommu_flush_iotlb_psi(iommu, domain->id, start_pfn,
+				      last_pfn - start_pfn + 1, 0, 
+				      iova, NULL);
+#else
+		iommu_flush_iotlb_psi(iommu, domain->id, start_pfn,
+				      last_pfn - start_pfn + 1, 0, NULL, NULL);
+		/* free iova */
+		__free_iova(&domain->iovad, iova, 1);
+#endif
+	} else {
+		add_unmap(domain, iova);
+		/*
+		 * queue up the release of the unmap to save the 1/6th of the
+		 * cpu used up by the iotlb flush operation...
+		 */
+	}
+}
+  
 static void intel_unmap_page(struct device *dev, dma_addr_t dev_addr,
 			     size_t size, enum dma_data_direction dir,
 			     struct dma_attrs *attrs)
@@ -2725,6 +3687,8 @@ static void intel_unmap_page(struct device *dev, dma_addr_t dev_addr,
 	if (iommu_no_mapping(dev))
 		return;
 
+	update_counter(IOMMU_UNMAP_COUNTER, 1);	
+
 	domain = find_domain(pdev);
 	BUG_ON(!domain);
 
@@ -2738,30 +3702,15 @@ static void intel_unmap_page(struct device *dev, dma_addr_t dev_addr,
 	start_pfn = mm_to_dma_pfn(iova->pfn_lo);
 	last_pfn = mm_to_dma_pfn(iova->pfn_hi + 1) - 1;
 
-	pr_debug("Device %s unmapping: pfn %lx-%lx\n",
-		 pci_name(pdev), start_pfn, last_pfn);
-
 	/*  clear the whole page */
-	dma_pte_clear_range(domain, start_pfn, last_pfn);
-
-	/* free page tables */
-	dma_pte_free_pagetable(domain, start_pfn, last_pfn);
+	/*dma_pte_clear_range_need_invd(domain, start_pfn, last_pfn);*/
+ 
+	free_shared_iova(iommu, domain, iova);
 
-	if (intel_iommu_strict) {
-		iommu_flush_iotlb_psi(iommu, domain->id, start_pfn,
-				      last_pfn - start_pfn + 1, 0);
-		/* free iova */
-		__free_iova(&domain->iovad, iova);
-	} else {
-		add_unmap(domain, iova);
-		/*
-		 * queue up the release of the unmap to save the 1/6th of the
-		 * cpu used up by the iotlb flush operation...
-		 */
-	}
+	update_counter(IOMMU_UNMAP_COUNTER, 0);	
 }
 
-static void *intel_alloc_coherent(struct device *hwdev, size_t size,
+static void *intel_alloc_coherent(struct device *hwdev, UL_SIZE size,
 				  dma_addr_t *dma_handle, gfp_t flags)
 {
 	void *vaddr;
@@ -2793,7 +3742,7 @@ static void *intel_alloc_coherent(struct device *hwdev, size_t size,
 	return NULL;
 }
 
-static void intel_free_coherent(struct device *hwdev, size_t size, void *vaddr,
+static void intel_free_coherent(struct device *hwdev, UL_SIZE size, void *vaddr,
 				dma_addr_t dma_handle)
 {
 	int order;
@@ -2814,7 +3763,7 @@ static void intel_unmap_sg(struct device *hwdev, struct scatterlist *sglist,
 	unsigned long start_pfn, last_pfn;
 	struct iova *iova;
 	struct intel_iommu *iommu;
-
+	
 	if (iommu_no_mapping(hwdev))
 		return;
 
@@ -2826,22 +3775,28 @@ static void intel_unmap_sg(struct device *hwdev, struct scatterlist *sglist,
 	iova = find_iova(&domain->iovad, IOVA_PFN(sglist[0].dma_address));
 	if (WARN_ONCE(!iova, "Driver unmaps unmatched sglist at PFN %llx\n",
 		      (unsigned long long)sglist[0].dma_address))
-		return;
+		BUG(); /*return;*/
 
 	start_pfn = mm_to_dma_pfn(iova->pfn_lo);
 	last_pfn = mm_to_dma_pfn(iova->pfn_hi + 1) - 1;
 
 	/*  clear the whole page */
-	dma_pte_clear_range(domain, start_pfn, last_pfn);
+	dma_pte_clear_range_need_invd(domain, start_pfn, last_pfn);
 
 	/* free page tables */
+/*
 	dma_pte_free_pagetable(domain, start_pfn, last_pfn);
-
+*/
 	if (intel_iommu_strict) {
+#ifdef IOMMU_DEFERRED_LIST
+		iommu_flush_iotlb_psi(iommu, domain->id, start_pfn,
+				      last_pfn - start_pfn + 1, 0, iova, NULL);
+#else	
 		iommu_flush_iotlb_psi(iommu, domain->id, start_pfn,
-				      last_pfn - start_pfn + 1, 0);
+				      last_pfn - start_pfn + 1, 0, NULL, NULL);
 		/* free iova */
-		__free_iova(&domain->iovad, iova);
+		__free_iova(&domain->iovad, iova, 1);
+#endif
 	} else {
 		add_unmap(domain, iova);
 		/*
@@ -2912,23 +3867,25 @@ static int intel_map_sg(struct device *hwdev, struct scatterlist *sglist, int ne
 	start_vpfn = mm_to_dma_pfn(iova->pfn_lo);
 
 	ret = domain_sg_mapping(domain, start_vpfn, sglist, size, prot);
-	if (unlikely(ret)) {
+	if (unlikely(is_error(ret))) {
+		printk(KERN_ERR "%s : Error allocating", __func__);
 		/*  clear the page */
 		dma_pte_clear_range(domain, start_vpfn,
-				    start_vpfn + size - 1);
+				    start_vpfn + size - 1, 1);
 		/* free page tables */
 		dma_pte_free_pagetable(domain, start_vpfn,
 				       start_vpfn + size - 1);
 		/* free iova */
-		__free_iova(&domain->iovad, iova);
+		__free_iova(&domain->iovad, iova, 1);
 		return 0;
 	}
 
 	/* it's a non-present to present mapping. Only flush if caching mode */
 	if (cap_caching_mode(iommu->cap))
-		iommu_flush_iotlb_psi(iommu, domain->id, start_vpfn, size, 1);
+		iommu_flush_iotlb_psi(iommu, domain->id, start_vpfn, size, 1, NULL, NULL);
 	else
 		iommu_flush_write_buffer(iommu);
+	/*printk(KERN_ERR "%s : %lx - %lx", __func__, iova->pfn_lo, iova->pfn_hi);*/
 
 	return nelems;
 }
@@ -3090,7 +4047,7 @@ static int init_iommu_hw(void)
 		iommu->flush.flush_context(iommu, 0, 0, 0,
 					   DMA_CCMD_GLOBAL_INVL);
 		iommu->flush.flush_iotlb(iommu, 0, 0, 0,
-					 DMA_TLB_GLOBAL_FLUSH);
+					 DMA_TLB_GLOBAL_FLUSH, NULL);
 		iommu_enable_translation(iommu);
 		iommu_disable_protect_mem_regions(iommu);
 	}
@@ -3107,7 +4064,7 @@ static void iommu_flush_all(void)
 		iommu->flush.flush_context(iommu, 0, 0, 0,
 					   DMA_CCMD_GLOBAL_INVL);
 		iommu->flush.flush_iotlb(iommu, 0, 0, 0,
-					 DMA_TLB_GLOBAL_FLUSH);
+					 DMA_TLB_GLOBAL_FLUSH, NULL);
 	}
 }
 
@@ -3265,6 +4222,8 @@ int __init intel_iommu_init(void)
 			panic("tboot: Failed to initialize DMAR device scope\n");
 		return 	-ENODEV;
 	}
+	
+	init_procfs();
 
 	/*
 	 * Check the need for DMA-remapping initialization now.
@@ -3302,6 +4261,11 @@ int __init intel_iommu_init(void)
 
 	bus_register_notifier(&pci_bus_type, &device_nb);
 
+	proc_control_vals[IOMMU_VSIZE_CONTROL] = 256;
+	proc_control_vals[IOMMU_VTIME_CONTROL] = 10000000; /* ns */
+	proc_control_vals[IOMMU_DTIME_CONTROL] = 10000000; /* ns */
+	proc_control_vals[IOMMU_DSIZE_CONTROL] = HIGH_WATER_MARK; 
+
 	return 0;
 }
 
@@ -3477,6 +4441,7 @@ static int md_domain_init(struct dmar_domain *domain, int guest_width)
 	if (!domain->pgd)
 		return -ENOMEM;
 	domain_flush_cache(domain, domain->pgd, PAGE_SIZE);
+
 	return 0;
 }
 
@@ -3515,9 +4480,12 @@ static void vm_domain_exit(struct dmar_domain *domain)
 	vm_domain_remove_all_dev_info(domain);
 	/* destroy iovas */
 	put_iova_domain(&domain->iovad);
-
+/*	put_iova_domain(&domain->defer_unmap_iovad);
+*/
 	/* clear ptes */
-	dma_pte_clear_range(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
+	dma_pte_clear_range(domain, 0, DOMAIN_MAX_PFN(domain->gaw), 1);
+
+	printk(KERN_ERR "%s : domain exit!", __func__);
 
 	/* free page tables */
 	dma_pte_free_pagetable(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
@@ -3569,7 +4537,7 @@ static int intel_iommu_attach_device(struct iommu_domain *domain,
 
 		old_domain = find_domain(pdev);
 		if (old_domain) {
-			if (dmar_domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE ||
+			if (/*dmar_domain->flags & DOMAIN_FLAG_VIRTUAL_MACHINE ||*/
 			    dmar_domain->flags & DOMAIN_FLAG_STATIC_IDENTITY)
 				domain_remove_one_dev_info(old_domain, pdev);
 			else
@@ -3623,14 +4591,17 @@ static void intel_iommu_detach_device(struct iommu_domain *domain,
 }
 
 static int intel_iommu_map(struct iommu_domain *domain,
-			   unsigned long iova, phys_addr_t hpa,
-			   int gfp_order, int iommu_prot)
+				 unsigned long iova, phys_addr_t hpa,
+				 UL_SIZE size, int iommu_prot)
 {
 	struct dmar_domain *dmar_domain = domain->priv;
 	u64 max_addr;
 	int prot = 0;
-	size_t size;
 	int ret;
+ 	struct intel_iommu *iommu;
+	unsigned long s_pfn, e_pfn;
+
+	iommu = domain_get_iommu(dmar_domain);
 
 	if (iommu_prot & IOMMU_READ)
 		prot |= DMA_PTE_READ;
@@ -3639,7 +4610,6 @@ static int intel_iommu_map(struct iommu_domain *domain,
 	if ((iommu_prot & IOMMU_CACHE) && dmar_domain->iommu_snooping)
 		prot |= DMA_PTE_SNP;
 
-	size     = PAGE_SIZE << gfp_order;
 	max_addr = iova + size;
 	if (dmar_domain->max_addr < max_addr) {
 		u64 end;
@@ -3657,36 +4627,89 @@ static int intel_iommu_map(struct iommu_domain *domain,
 	/* Round up size to next multiple of PAGE_SIZE, if it and
 	   the low bits of hpa would take us onto the next page */
 	size = aligned_nrpages(hpa, size);
-	ret = domain_pfn_mapping(dmar_domain, iova >> VTD_PAGE_SHIFT,
+	s_pfn = iova >> VTD_PAGE_SHIFT;
+	e_pfn = s_pfn + size - 1;
+	ret = domain_pfn_mapping(dmar_domain, s_pfn,
 				 hpa >> VTD_PAGE_SHIFT, size, prot);
+
+	/* it's a non-present to present mapping. Only flush if caching mode */
+	if (cap_caching_mode(iommu->cap))
+   		iommu_flush_iotlb_psi(iommu, 0, mm_to_dma_pfn(iova), size, 1, NULL, NULL);
+      	else
+        	iommu_flush_write_buffer(iommu);
+
+	/* need to flush unmaps, we may be mapping something with stale data */
 	return ret;
 }
 
 static int intel_iommu_unmap(struct iommu_domain *domain,
-			     unsigned long iova, int gfp_order)
+				    unsigned long iova_addr, size_t size)
 {
 	struct dmar_domain *dmar_domain = domain->priv;
-	size_t size = PAGE_SIZE << gfp_order;
+	struct intel_iommu *iommu;
+	int actual_cleared;
 
-	dma_pte_clear_range(dmar_domain, iova >> VTD_PAGE_SHIFT,
-			    (iova + size - 1) >> VTD_PAGE_SHIFT);
+	struct iova* iova = NULL;
+	unsigned long s_pfn = iova_addr >> VTD_PAGE_SHIFT;
+	
+
+	if (!size)
+		return 0;
 
-	if (dmar_domain->max_addr == iova + size)
-		dmar_domain->max_addr = iova;
+	iommu = domain_get_iommu(dmar_domain);
+	size = aligned_nrpages(iova_addr, size);
+/*	printk(KERN_ERR "%s : unmaping %llx %llx", __func__, s_pfn, s_pfn+size-1);
+	dump_stack();*/
+	actual_cleared = dma_pte_clear_range_need_invd(dmar_domain, s_pfn,
+			    s_pfn + size - 1);
 
-	return gfp_order;
+/*
+	TODO: Check later - might improve
+	dma_pte_free_pagetable(dmar_domain, iova >> VTD_PAGE_SHIFT,
+                           (iova + size - 1) >> VTD_PAGE_SHIFT);
+*/
+
+	if (dmar_domain->max_addr == iova_addr + size)
+		dmar_domain->max_addr = iova_addr;
+
+	/* TODO: Check if size is bigger than 1 page, if not, we do not need */
+#ifdef IOMMU_DEFERRED_LIST
+	iova = alloc_orphan_iova(&dmar_domain->iovad, s_pfn, 
+		s_pfn+size-1, dmar_domain);
+#endif
+
+       	/* Adding flush */
+	if (actual_cleared) {
+	       	if (intel_iommu_strict) {
+    			iommu_flush_iotlb_psi(iommu, dmar_domain->id, 
+				s_pfn,
+    				size, 0, iova, NULL);
+	  	} else {
+               		/*UL_SIZE i;
+	           	for (i =0 ; i < size; i++)*/
+ 			add_unmap_range(dmar_domain, s_pfn, size);
+               /*
+                * queue up the release of the unmap to save the 1/6th of the
+                * cpu used up by the iotlb flush operation...
+                */
+	       }
+	}
+	return size;
 }
 
 static phys_addr_t intel_iommu_iova_to_phys(struct iommu_domain *domain,
-					    unsigned long iova)
+					    unsigned long iova, int *mapped)
 {
 	struct dmar_domain *dmar_domain = domain->priv;
 	struct dma_pte *pte;
 	u64 phys = 0;
+	*mapped = 0;
 
-	pte = pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT);
-	if (pte)
+	pte = pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT, 0);
+	if (pte) {
 		phys = dma_pte_addr(pte);
+		*mapped = dma_pte_present(pte);
+	}
 
 	return phys;
 }
@@ -3694,11 +4717,45 @@ static phys_addr_t intel_iommu_iova_to_phys(struct iommu_domain *domain,
 static int intel_iommu_domain_has_cap(struct iommu_domain *domain,
 				      unsigned long cap)
 {
-	struct dmar_domain *dmar_domain = domain->priv;
+	struct dmar_domain *dmar_domain = (domain == NULL) ? NULL : domain->priv;
 
 	if (cap == IOMMU_CAP_CACHE_COHERENCY)
 		return dmar_domain->iommu_snooping;
 
+	/* TODO: Why 30? Why dmar_domain might be NULL? */
+	if (cap == IOMMU_CAP_ADDRESS_WIDTH)
+		return dmar_domain ? agaw_to_width(dmar_domain->agaw) : 30;
+
+	if (cap == IOMMU_CAP_STRICT)
+		return intel_iommu_strict;
+
+	return 0;
+}
+
+static 
+int intel_iommu_get_iommu_domain_for_dev(struct iommu_domain* domain,
+                                struct pci_dev *dev)
+{
+	domain->priv = get_valid_domain_for_dev(dev);
+	return 0;
+}
+
+static
+int intel_iommu_vdomain_init(struct iommu_domain* domain)
+{
+	struct dmar_domain *dmar_domain = (domain == NULL) ? NULL : domain->priv;
+	BUG_ON(dmar_domain == NULL);
+	dmar_domain->flags |= DOMAIN_FLAG_VIRTUAL_MACHINE;
+/*	printk(KERN_ERR "%s : init vdomain", __func__);*/
+	return 0;
+}
+
+static
+int intel_iommu_vdomain_destroy(struct iommu_domain* domain)
+{
+	struct dmar_domain *dmar_domain = (domain == NULL) ? NULL : domain->priv;
+	BUG_ON(dmar_domain == NULL);
+	dmar_domain->flags &= ~DOMAIN_FLAG_VIRTUAL_MACHINE;
 	return 0;
 }
 
@@ -3711,6 +4768,9 @@ static struct iommu_ops intel_iommu_ops = {
 	.unmap		= intel_iommu_unmap,
 	.iova_to_phys	= intel_iommu_iova_to_phys,
 	.domain_has_cap = intel_iommu_domain_has_cap,
+	.get_iommu	= intel_iommu_get_iommu_domain_for_dev,
+	.vdomain_init  	= intel_iommu_vdomain_init,
+	.vdomain_destroy = intel_iommu_vdomain_destroy,
 };
 
 static void __devinit quirk_iommu_rwbf(struct pci_dev *dev)
diff --git a/drivers/pci/iova.c b/drivers/pci/iova.c
index 7914951..4fcc69b 100644
--- a/drivers/pci/iova.c
+++ b/drivers/pci/iova.c
@@ -128,6 +128,7 @@ move_left:
 	/* pfn_lo will point to size aligned address if size_aligned is set */
 	new->pfn_lo = limit_pfn - (size + pad_size) + 1;
 	new->pfn_hi = new->pfn_lo + size - 1;
+	new->orphan = 0;
 
 	/* Insert the new_iova into domain rbtree by holding writer lock */
 	/* Add new node and rebalance tree. */
@@ -188,6 +189,26 @@ iova_insert_rbtree(struct rb_root *root, struct iova *iova)
 	rb_insert_color(&iova->node, root);
 }
 
+struct iova *
+alloc_orphan_iova(struct iova_domain *iovad, unsigned long pfn_lo,
+	unsigned long pfn_hi, struct dmar_domain *dom)
+{
+	struct iova *new_iova;
+
+	new_iova = alloc_iova_mem();
+	if (!new_iova)
+		return NULL;
+	
+	new_iova->pfn_lo = pfn_lo;
+	new_iova->pfn_hi = pfn_hi;
+	new_iova->orphan = 1;
+#ifdef IOMMU_DEFERRED_LIST
+	INIT_LIST_HEAD(&new_iova->link);
+	new_iova->domain = dom;
+#endif
+	return new_iova;
+}
+
 /**
  * alloc_iova - allocates an iova
  * @iovad - iova domain in question
@@ -202,7 +223,7 @@ iova_insert_rbtree(struct rb_root *root, struct iova *iova)
 struct iova *
 alloc_iova(struct iova_domain *iovad, unsigned long size,
 	unsigned long limit_pfn,
-	bool size_aligned)
+	bool size_aligned, struct dmar_domain *dom)
 {
 	struct iova *new_iova;
 	int ret;
@@ -211,6 +232,11 @@ alloc_iova(struct iova_domain *iovad, unsigned long size,
 	if (!new_iova)
 		return NULL;
 
+#ifdef IOMMU_DEFERRED_LIST
+	INIT_LIST_HEAD(&new_iova->link);
+	new_iova->domain = dom;
+#endif
+
 	/* If size aligned is set then round the size to
 	 * to next power of two.
 	 */
@@ -275,14 +301,25 @@ struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn)
  * Frees the given iova belonging to the giving domain
  */
 void
-__free_iova(struct iova_domain *iovad, struct iova *iova)
+__free_iova(struct iova_domain *iovad, struct iova *iova, int lock)
 {
-	unsigned long flags;
+	unsigned long flags = 0;
+	BUG_ON(iovad==NULL);
+	BUG_ON(iova==NULL);
+
+
+	if (!iova->orphan) {
+		if (lock)
+			spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
+		__cached_rbnode_delete_update(iovad, iova);
+		rb_erase(&iova->node, &iovad->rbroot);
+		if (lock)
+			spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+	}
+#ifdef IOMMU_DEFERRED_LIST
+	list_del(&iova->link);
+#endif
 
-	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
-	__cached_rbnode_delete_update(iovad, iova);
-	rb_erase(&iova->node, &iovad->rbroot);
-	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
 	free_iova_mem(iova);
 }
 
@@ -298,7 +335,7 @@ free_iova(struct iova_domain *iovad, unsigned long pfn)
 {
 	struct iova *iova = find_iova(iovad, pfn);
 	if (iova)
-		__free_iova(iovad, iova);
+		__free_iova(iovad, iova, 1);
 
 }
 
@@ -425,3 +462,58 @@ copy_reserved_iova(struct iova_domain *from, struct iova_domain *to)
 	}
 	spin_unlock_irqrestore(&from->iova_rbtree_lock, flags);
 }
+
+
+
+/***** ADDITIONS *****/
+int insert_iova_range(struct iova_domain *iovad,
+		uint64_t pfn_lo, uint64_t pfn_hi)
+{
+	unsigned long flags;
+	struct iova *new_iova;
+	struct rb_node **entry, *parent = NULL;
+
+	new_iova = alloc_iova_mem();
+	if (!new_iova)
+		return -1;
+
+	/* Walk the tree backwards */
+	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
+
+	/* pfn_lo will point to size aligned address if size_aligned is set */
+	new_iova->pfn_lo = pfn_lo;
+	new_iova->pfn_hi = pfn_hi;
+
+	/* Insert the new_iova into domain rbtree by holding writer lock */
+	/* Add new node and rebalance tree. */
+	/* If we have 'prev', it's a valid place to start the
+	   insertion. Otherwise, start from the root. */
+
+	entry = &iovad->rbroot.rb_node;
+
+	/* Figure out where to put new node */
+	while (*entry) {
+		struct iova *this = container_of(*entry,
+						struct iova, node);
+		parent = *entry;
+			if (new_iova->pfn_lo < this->pfn_lo)
+			entry = &((*entry)->rb_left);
+		else if (new_iova->pfn_lo > this->pfn_lo)
+			entry = &((*entry)->rb_right);
+		else
+			BUG(); /* this might happen, but we will avoid it right now */
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&new_iova->node, parent, entry);
+	rb_insert_color(&new_iova->node, &iovad->rbroot);
+
+	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+
+	return 0;
+}
+
+
+
+
+
diff --git a/include/linux/dma_remapping.h b/include/linux/dma_remapping.h
index 5619f85..a47db7b 100644
--- a/include/linux/dma_remapping.h
+++ b/include/linux/dma_remapping.h
@@ -12,11 +12,16 @@
 #define DMA_PTE_READ (1)
 #define DMA_PTE_WRITE (2)
 #define DMA_PTE_SNP (1 << 11)
+#define DMA_PTE_NEED_INVD_BIT 	8
 
 #define CONTEXT_TT_MULTI_LEVEL	0
 #define CONTEXT_TT_DEV_IOTLB	1
 #define CONTEXT_TT_PASS_THROUGH 2
 
+/*
+#define IOMMU_DEFERRED_LIST
+*/
+
 struct intel_iommu;
 struct dmar_domain;
 struct root_entry;
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 9310c69..5c9cba1 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -58,6 +58,10 @@
 #define DMAR_ICS_REG	0x98	/* Invalidation complete status register */
 #define DMAR_IRTA_REG	0xb8    /* Interrupt remapping table addr register */
 
+#define DMAR_SIDECORE_ACTIVE_CNT 0x110
+#define DMAR_SIDECORE_INACTIVE_CNT 0x118
+
+
 #define OFFSET_STRIDE		(9)
 /*
 #define dmar_readl(dmar, reg) readl(dmar + reg)
@@ -246,6 +250,7 @@ enum {
 
 #define QI_IWD_STATUS_DATA(d)	(((u64)d) << 32)
 #define QI_IWD_STATUS_WRITE	(((u64)1) << 5)
+#define QI_IWD_FENCE		(((u64)1) << 6)
 
 #define QI_IOTLB_DID(did) 	(((u64)did) << 16)
 #define QI_IOTLB_DR(dr) 	(((u64)dr) << 7)
@@ -270,6 +275,25 @@ struct qi_desc {
 	u64 low, high;
 };
 
+#ifdef IOMMU_DEFERRED_LIST
+struct qi_data {
+	int		status;
+	int		group_descs;
+	struct list_head 	iovas;
+};
+
+struct q_inval {
+	spinlock_t      q_lock;
+	struct qi_desc  *desc;          /* invalidation queue */
+	struct qi_data  *desc_data;   	/* desc status */
+	int             free_head;      /* first free entry */
+	int             free_tail;      /* last free entry */
+	int             free_cnt;
+};
+
+
+#else
+
 struct q_inval {
 	spinlock_t      q_lock;
 	struct qi_desc  *desc;          /* invalidation queue */
@@ -279,6 +303,8 @@ struct q_inval {
 	int             free_cnt;
 };
 
+#endif
+
 #ifdef CONFIG_INTR_REMAP
 /* 1MB - maximum possible interrupt remapping table size */
 #define INTR_REMAP_PAGE_ORDER	8
@@ -295,7 +321,10 @@ struct iommu_flush {
 	void (*flush_context)(struct intel_iommu *iommu, u16 did, u16 sid,
 			      u8 fm, u64 type);
 	void (*flush_iotlb)(struct intel_iommu *iommu, u16 did, u64 addr,
-			    unsigned int size_order, u64 type);
+			    unsigned int size_order, u64 type, 
+			    struct list_head *iovas,
+			    struct list_head *to_free_iovas);
+	void (*flush_process)(struct intel_iommu *iommu);
 };
 
 enum {
@@ -328,6 +357,7 @@ struct intel_iommu {
 #endif
 	struct q_inval  *qi;            /* Queued invalidation info */
 	u32 *iommu_state; /* Store iommu states between suspend and resume.*/
+	unsigned long pending_invd;
 
 #ifdef CONFIG_INTR_REMAP
 	struct ir_table *ir_table;	/* Interrupt remapping info */
@@ -342,6 +372,8 @@ static inline void __iommu_flush_cache(
 		clflush_cache_range(addr, size);
 }
 
+void free_iovas(struct list_head* iovas);
+
 extern struct dmar_drhd_unit * dmar_find_matched_drhd_unit(struct pci_dev *dev);
 extern int dmar_find_matched_atsr_unit(struct pci_dev *dev);
 
@@ -355,12 +387,24 @@ extern void qi_global_iec(struct intel_iommu *iommu);
 extern void qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid,
 			     u8 fm, u64 type);
 extern void qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,
-			  unsigned int size_order, u64 type);
+			  unsigned int size_order, u64 type, 
+			  struct list_head *iovas, 
+			  struct list_head *to_free_iovas);
 extern void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 qdep,
 			       u64 addr, unsigned mask);
+extern void qi_flush_process(struct intel_iommu *iommu);
 
-extern int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu);
+extern int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu, int counter);
 
 extern int dmar_ir_support(void);
 
+extern int get_q_length(struct intel_iommu *iommu);
+
+enum {IOMMU_MAP_COUNTER, IOMMU_UNMAP_COUNTER, IOMMU_MINVD_COUNTER,
+	IOMMU_UINVD_COUNTER, IOMMU_SHARED_HITS, IOMMU_VICTIM_HITS,
+	IOMMU_MISSES, IOMMU_COUNTERS_NUM};
+
+extern void update_counter(int counter, int start);
+
+
 #endif
diff --git a/include/linux/iommu.h b/include/linux/iommu.h
index be22ad8..b3297d4 100644
--- a/include/linux/iommu.h
+++ b/include/linux/iommu.h
@@ -25,11 +25,18 @@
 
 struct device;
 
+/* TODO: upcasting to pci_dev and deliver device */
+struct pci_dev;
+
 struct iommu_domain {
 	void *priv;
+	int iommu_flags;
+	struct list_head link;
 };
 
 #define IOMMU_CAP_CACHE_COHERENCY	0x1
+#define IOMMU_CAP_ADDRESS_WIDTH 	0x2
+#define IOMMU_CAP_STRICT		0x3
 
 struct iommu_ops {
 	int (*domain_init)(struct iommu_domain *domain);
@@ -41,16 +48,20 @@ struct iommu_ops {
 	int (*unmap)(struct iommu_domain *domain, unsigned long iova,
 		     int gfp_order);
 	phys_addr_t (*iova_to_phys)(struct iommu_domain *domain,
-				    unsigned long iova);
+				    unsigned long iova, int *mapped);
 	int (*domain_has_cap)(struct iommu_domain *domain,
 			      unsigned long cap);
+       	int (*get_iommu)(struct iommu_domain* domain,
+               	struct pci_dev *dev);
+	int (*vdomain_init)(struct iommu_domain *domain);
+	int (*vdomain_destroy)(struct iommu_domain *domain);
 };
 
 #ifdef CONFIG_IOMMU_API
 
 extern void register_iommu(struct iommu_ops *ops);
 extern bool iommu_found(void);
-extern struct iommu_domain *iommu_domain_alloc(void);
+extern struct iommu_domain *iommu_domain_alloc(struct pci_dev* dev);
 extern void iommu_domain_free(struct iommu_domain *domain);
 extern int iommu_attach_device(struct iommu_domain *domain,
 			       struct device *dev);
@@ -61,9 +72,13 @@ extern int iommu_map(struct iommu_domain *domain, unsigned long iova,
 extern int iommu_unmap(struct iommu_domain *domain, unsigned long iova,
 		       int gfp_order);
 extern phys_addr_t iommu_iova_to_phys(struct iommu_domain *domain,
-				      unsigned long iova);
+			       unsigned long iova, int *mapped);
 extern int iommu_domain_has_cap(struct iommu_domain *domain,
 				unsigned long cap);
+extern int iommu_get_iommu_domain_for_dev(struct iommu_domain* domain,
+                               struct pci_dev* dev);
+extern int iommu_vdomain_init(struct iommu_domain *domain);
+extern int iommu_vdomain_destroy(struct iommu_domain *domain);
 
 #else /* CONFIG_IOMMU_API */
 
@@ -76,7 +91,7 @@ static inline bool iommu_found(void)
 	return false;
 }
 
-static inline struct iommu_domain *iommu_domain_alloc(void)
+static inline struct iommu_domain *iommu_domain_alloc(struct pci_dev *dev)
 {
 	return NULL;
 }
@@ -120,6 +135,17 @@ static inline int domain_has_cap(struct iommu_domain *domain,
 	return 0;
 }
 
+static inline int iommu_get_iommu_domain_for_dev(struct iommu_domain* domain,
+                                               struct pci_dev *dev)
+{
+       return 0;
+}
+
+static inline void iommu_flush_unmap(void)
+{
+}
+
+
 #endif /* CONFIG_IOMMU_API */
 
 #endif /* __LINUX_IOMMU_H */
diff --git a/include/linux/iova.h b/include/linux/iova.h
index 76a0759..7b94957 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -15,15 +15,28 @@
 #include <linux/kernel.h>
 #include <linux/rbtree.h>
 #include <linux/dma-mapping.h>
+#include <linux/dma_remapping.h>
 
 /* IO virtual address start page frame number */
 #define IOVA_START_PFN		(1)
 
+struct dmar_domain;
+
 /* iova structure */
 struct iova {
 	struct rb_node	node;
 	unsigned long	pfn_hi; /* IOMMU dish out addr hi */
 	unsigned long	pfn_lo; /* IOMMU dish out addr lo */
+#ifdef IOMMU_DEFERRED_LIST
+	struct dmar_domain* domain;
+#endif
+	struct list_head link;	/* link to the deferred list */
+	int orphan;	/* True if not in the RB-tree */
+	/* For sharing and victim */
+	unsigned long 	ppfn_lo; /* physical address low */
+	uint16_t	share_cnt;
+	uint16_t	prot;
+	uint64_t 	free_time;
 };
 
 /* holds all the iova translations for a domain */
@@ -37,15 +50,19 @@ struct iova_domain {
 struct iova *alloc_iova_mem(void);
 void free_iova_mem(struct iova *iova);
 void free_iova(struct iova_domain *iovad, unsigned long pfn);
-void __free_iova(struct iova_domain *iovad, struct iova *iova);
+void __free_iova(struct iova_domain *iovad, struct iova *iova, int lock);
+struct iova *alloc_orphan_iova(struct iova_domain *iovad, 
+	unsigned long pfn_lo, unsigned long pfn_hi, struct dmar_domain *dom);
 struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
 	unsigned long limit_pfn,
-	bool size_aligned);
+	bool size_aligned, struct dmar_domain *dom);
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
 void init_iova_domain(struct iova_domain *iovad, unsigned long pfn_32bit);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
+int insert_iova_range(struct iova_domain *iovad,
+                uint64_t pfn_lo, uint64_t pfn_hi);
 
 #endif
diff --git a/include/linux/kvm.h b/include/linux/kvm.h
index 23ea022..fd9e8af 100644
--- a/include/linux/kvm.h
+++ b/include/linux/kvm.h
@@ -11,6 +11,7 @@
 #include <linux/compiler.h>
 #include <linux/ioctl.h>
 #include <asm/kvm.h>
+#include <linux/viommu.h>
 
 #define KVM_API_VERSION 12
 
@@ -657,6 +658,11 @@ struct kvm_clock_data {
 /* Available with KVM_CAP_PIT_STATE2 */
 #define KVM_GET_PIT2              _IOR(KVMIO,  0x9f, struct kvm_pit_state2)
 #define KVM_SET_PIT2              _IOW(KVMIO,  0xa0, struct kvm_pit_state2)
+#define KVM_ENABLE_VIOMMU	  _IOR(KVMIO,  0xa5, struct viommu_enable)
+#define KVM_VIOMMU_TRANSLATE      _IOWR(KVMIO, 0xa6, struct iommu_translate)
+#define KVM_VIOMMU_SET_DEVFN_MAP  _IOR(KVMIO, 0xa7, struct devfn_mapping)
+#define KVM_DISABLE_VIOMMU	  _IO(KVMIO, 0xa8)
+#define KVM_VIOMMU_WRITE          _IOR(KVMIO, 0xa9, struct viommu_write)
 
 /*
  * ioctls for vcpu fds
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 7cb116a..6ec66ca 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -175,6 +175,7 @@ struct kvm {
 	raw_spinlock_t requests_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
+	struct task_struct *tsk;
 	struct kvm_memslots *memslots;
 	struct srcu_struct srcu;
 #ifdef CONFIG_KVM_APIC_ARCHITECTURE
@@ -213,6 +214,7 @@ struct kvm {
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 #endif
+	struct viommu* viommu;
 };
 
 /* The guest did something we don't support. */
@@ -294,6 +296,7 @@ void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
+pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr);
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 pfn_t gfn_to_pfn_memslot(struct kvm *kvm,
 			 struct kvm_memory_slot *slot, gfn_t gfn);
@@ -418,6 +421,7 @@ struct kvm_assigned_dev_kernel {
 	struct work_struct interrupt_work;
 	struct list_head list;
 	int assigned_dev_id;
+	struct iommu_domain *iommu_domain; /* TODO : check if required */
 	int host_segnr;
 	int host_busnr;
 	int host_devfn;
@@ -465,14 +469,42 @@ void kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);
 #define KVM_IOMMU_CACHE_COHERENCY	0x1
 
 #ifdef CONFIG_IOMMU_API
-int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
+int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot, struct iommu_domain *domain);
+int kvm_iommu_map_pages_all(struct kvm *kvm, gfn_t base_gfn, unsigned long npages);
 int kvm_iommu_map_guest(struct kvm *kvm);
 int kvm_iommu_unmap_guest(struct kvm *kvm);
 int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev);
 int kvm_deassign_device(struct kvm *kvm,
 			struct kvm_assigned_dev_kernel *assigned_dev);
+ /*TODO: Check if the following are required 
+
+int iommu_map_range(struct iommu_domain *domain, unsigned long iova,
+	phys_addr_t paddr, size_t size, int prot);
+void iommu_unmap_range(struct iommu_domain *domain, unsigned long iova,
+	size_t size);
+int kvm_iommu_unmap_memslots_plain(struct kvm* kvm,
+	struct iommu_domain* domain);
+*/
 #else /* CONFIG_IOMMU_API */
+/*
+int iommu_map_range(struct iommu_domain *domain, unsigned long iova,
+                   phys_addr_t paddr, size_t size, int prot)
+{
+       return 0;
+}
+
+void iommu_unmap_range(struct iommu_domain *domain, unsigned long iova,
+                     size_t size)
+{
+}
+
+int iommu_map_range(struct iommu_domain *domain, unsigned long iova,
+                   phys_addr_t paddr, size_t size, int prot)
+{
+}
+*/
+
 static inline int kvm_iommu_map_pages(struct kvm *kvm,
 				      gfn_t base_gfn,
 				      unsigned long npages)
@@ -628,5 +660,8 @@ static inline long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
 
 #endif
 
+struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,
+						      int assigned_dev_id);
+
 #endif
 
diff --git a/include/linux/pm.h b/include/linux/pm.h
index 8e258c7..76b54ec 100644
--- a/include/linux/pm.h
+++ b/include/linux/pm.h
@@ -34,6 +34,7 @@
 extern void (*pm_idle)(void);
 extern void (*pm_power_off)(void);
 extern void (*pm_power_off_prepare)(void);
+extern void (*pm_cl_monitoring)(u64* monitored, u64 prev_val, u32 cstate);
 
 /*
  * Device power management
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0478888..9ad1002 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -141,6 +141,7 @@ extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
 extern unsigned long this_cpu_load(void);
+extern unsigned long cpu_load(int cpu);
 
 
 extern void calc_global_load(void);
@@ -2292,6 +2293,7 @@ static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
 static inline void set_tsk_need_resched(struct task_struct *tsk)
 {
 	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
+	wake_cl_monitoring(task_thread_info(tsk));
 }
 
 static inline void clear_tsk_need_resched(struct task_struct *tsk)
diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h
index a8cc4e1..e45286c 100644
--- a/include/linux/thread_info.h
+++ b/include/linux/thread_info.h
@@ -98,7 +98,12 @@ static inline int test_ti_thread_flag(struct thread_info *ti, int flag)
 #define test_thread_flag(flag) \
 	test_ti_thread_flag(current_thread_info(), flag)
 
-#define set_need_resched()	set_thread_flag(TIF_NEED_RESCHED)
+static inline void set_need_resched(void) 
+{ 							
+	set_thread_flag(TIF_NEED_RESCHED);
+	wake_cl_monitoring(current_thread_info()); /* TODO: Is it really needed? */
+}
+
 #define clear_need_resched()	clear_thread_flag(TIF_NEED_RESCHED)
 
 #if defined TIF_RESTORE_SIGMASK && !defined HAVE_SET_RESTORE_SIGMASK
diff --git a/include/linux/viommu.h b/include/linux/viommu.h
new file mode 100644
index 0000000..d59e780
--- /dev/null
+++ b/include/linux/viommu.h
@@ -0,0 +1,71 @@
+#ifndef MMONITOR_H_
+#define MMONITOR_H_
+#if 0
+#include <linux/ioctl.h> 
+#endif
+#include <linux/kvm_host.h>
+#include <linux/kvm_types.h>
+
+/* user space visible part */
+
+struct iommu_translate {
+        uint64_t iova;
+        uint64_t gpa;
+        int err;
+        uint8_t g_busnr;
+        uint8_t g_devfn;
+        int is_write;
+};
+
+struct devfn_mapping {
+	uint8_t g_busnr;
+	uint8_t g_devfn;
+	uint8_t h_busnr;
+	uint8_t h_devfn;
+};
+
+struct viommu_enable {
+	uint64_t addr;
+	uint8_t sidecore;
+};
+
+struct viommu_write {
+	uint32_t offset;
+	uint32_t value;
+};
+
+struct kvm;
+
+int alloc_viommu(struct kvm* kvm, uint64_t addr, int sidecore);
+void kvm_viommu_exit(struct kvm *kvm);
+
+
+uint64_t iommu_phy_addr_translate(struct kvm* kvm, uint64_t addr,
+                            int is_write, uint8_t g_busnr, uint8_t g_devfn, int* err);
+void intel_iommu_add_dev(struct kvm *kvm,
+        struct devfn_mapping *mapping);
+
+void viommu_perform_write(struct kvm *kvm, struct viommu_write *viommu_write);
+
+void viommu_mark_vmexit(struct kvm *kvm);
+
+#ifdef __KERNEL__ /* kernel only */ 
+/*
+void set_kvm(struct kvm* kvm);
+*/
+/*
+struct timer_interrupt_hook
+{
+	void (*func)(void*); 
+	void* data; 
+}; 
+
+
+int register_timer_interrupt(struct timer_interrupt_hook* hook); 
+void unregister_timer_interrupt(struct timer_interrupt_hook* hook); 
+*/
+
+#endif /* __KERNEL__ */ 
+
+#endif /* MMONITOR_H_ */ 
+
diff --git a/kernel/perf_event.c b/kernel/perf_event.c
index ff86c55..2a9a547 100644
--- a/kernel/perf_event.c
+++ b/kernel/perf_event.c
@@ -92,12 +92,14 @@ void perf_disable(void)
 	if (!__get_cpu_var(perf_disable_count)++)
 		hw_perf_disable();
 }
+EXPORT_SYMBOL_GPL(perf_disable);
 
 void perf_enable(void)
 {
 	if (!--__get_cpu_var(perf_disable_count))
 		hw_perf_enable();
 }
+EXPORT_SYMBOL_GPL(perf_enable);
 
 static void get_ctx(struct perf_event_context *ctx)
 {
@@ -633,6 +635,7 @@ void perf_event_disable(struct perf_event *event)
 
 	raw_spin_unlock_irq(&ctx->lock);
 }
+EXPORT_SYMBOL_GPL(perf_event_disable);
 
 static int
 event_sched_in(struct perf_event *event,
@@ -1053,6 +1056,7 @@ void perf_event_enable(struct perf_event *event)
  out:
 	raw_spin_unlock_irq(&ctx->lock);
 }
+EXPORT_SYMBOL_GPL(perf_event_enable);
 
 static int perf_event_refresh(struct perf_event *event, int refresh)
 {
@@ -2146,6 +2150,7 @@ static void perf_event_reset(struct perf_event *event)
 	atomic64_set(&event->count, 0);
 	perf_event_update_userpage(event);
 }
+EXPORT_SYMBOL_GPL(perf_event_reset);
 
 /*
  * Holding the top-level event's child_mutex means that any
@@ -2309,6 +2314,7 @@ int perf_event_task_enable(void)
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(perf_event_task_enable);
 
 int perf_event_task_disable(void)
 {
@@ -2321,6 +2327,7 @@ int perf_event_task_disable(void)
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(perf_event_task_disable);
 
 #ifndef PERF_EVENT_INDEX_OFFSET
 # define PERF_EVENT_INDEX_OFFSET 0
diff --git a/kernel/sched.c b/kernel/sched.c
index f52a880..1383d1f 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -2885,6 +2885,11 @@ unsigned long this_cpu_load(void)
 	return this->cpu_load[0];
 }
 
+unsigned long cpu_load(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	return rq->cpu_load[0];
+}
 
 /* Variables and functions for calc_load */
 static atomic_long_t calc_load_tasks;
diff --git a/virt/kvm/assigned-dev.c b/virt/kvm/assigned-dev.c
index 4d10b1e..d9f05f5 100644
--- a/virt/kvm/assigned-dev.c
+++ b/virt/kvm/assigned-dev.c
@@ -19,7 +19,7 @@
 #include <linux/slab.h>
 #include "irq.h"
 
-static struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,
+struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,
 						      int assigned_dev_id)
 {
 	struct list_head *ptr;
@@ -569,11 +569,13 @@ static int kvm_vm_ioctl_assign_device(struct kvm *kvm,
 	list_add(&match->list, &kvm->arch.assigned_dev_head);
 
 	if (assigned_dev->flags & KVM_DEV_ASSIGN_ENABLE_IOMMU) {
+#if 0
 		if (!kvm->arch.iommu_domain) {
 			r = kvm_iommu_map_guest(kvm);
 			if (r)
 				goto out_list_del;
 		}
+#endif
 		r = kvm_assign_device(kvm, match);
 		if (r)
 			goto out_list_del;
diff --git a/virt/kvm/iommu.c b/virt/kvm/iommu.c
index 96048ee..e36f9c8 100644
--- a/virt/kvm/iommu.c
+++ b/virt/kvm/iommu.c
@@ -28,9 +28,11 @@
 #include <linux/iommu.h>
 #include <linux/intel-iommu.h>
 
-static int kvm_iommu_unmap_memslots(struct kvm *kvm);
+static int kvm_iommu_unmap_memslots(struct kvm *kvm,
+				struct iommu_domain *domain);
 static void kvm_iommu_put_pages(struct kvm *kvm,
-				gfn_t base_gfn, unsigned long npages);
+				gfn_t base_gfn, unsigned long npages,
+				struct iommu_domain *domain);
 
 static pfn_t kvm_pin_pages(struct kvm *kvm, struct kvm_memory_slot *slot,
 			   gfn_t gfn, unsigned long size)
@@ -51,108 +53,114 @@ static pfn_t kvm_pin_pages(struct kvm *kvm, struct kvm_memory_slot *slot,
 	return pfn;
 }
 
-int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
+int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot,
+			struct iommu_domain *domain)
 {
-	gfn_t gfn, end_gfn;
+	gfn_t gfn = slot->base_gfn;
+	unsigned long npages = slot->npages;
 	pfn_t pfn;
-	int r = 0;
-	struct iommu_domain *domain = kvm->arch.iommu_domain;
+	int i, r = 0;
 	int flags;
 
 	/* check if iommu exists and in use */
-	if (!domain)
+	if (!domain) {
+		printk (KERN_ERR "%s : no domain found", __func__);
 		return 0;
-
-	gfn     = slot->base_gfn;
-	end_gfn = gfn + slot->npages;
+	}
 
 	flags = IOMMU_READ | IOMMU_WRITE;
-	if (kvm->arch.iommu_flags & KVM_IOMMU_CACHE_COHERENCY)
+	if (domain->iommu_flags & KVM_IOMMU_CACHE_COHERENCY)
 		flags |= IOMMU_CACHE;
 
-
-	while (gfn < end_gfn) {
-		unsigned long page_size;
-
-		/* Check if already mapped */
-		if (iommu_iova_to_phys(domain, gfn_to_gpa(gfn))) {
-			gfn += 1;
-			continue;
-		}
-
-		/* Get the page size we could use to map */
-		page_size = kvm_host_page_size(kvm, gfn);
-
-		/* Make sure the page_size does not exceed the memslot */
-		while ((gfn + (page_size >> PAGE_SHIFT)) > end_gfn)
-			page_size >>= 1;
-
-		/* Make sure gfn is aligned to the page size we want to map */
-		while ((gfn << PAGE_SHIFT) & (page_size - 1))
-			page_size >>= 1;
-
-		/*
-		 * Pin all pages we are about to map in memory. This is
-		 * important because we unmap and unpin in 4kb steps later.
-		 */
-		pfn = kvm_pin_pages(kvm, slot, gfn, page_size);
-		if (is_error_pfn(pfn)) {
-			gfn += 1;
+	for (i = 0; i < npages; i++) {
+		/* check if already mapped */
+/*		if (iommu_iova_to_phys(domain, gfn_to_gpa(gfn)))
 			continue;
-		}
-
-		/* Map into IO address space */
-		r = iommu_map(domain, gfn_to_gpa(gfn), pfn_to_hpa(pfn),
-			      get_order(page_size), flags);
+*/
+		pfn = gfn_to_pfn_memslot(kvm, slot, gfn);
+		r = iommu_map(domain,
+				    gfn_to_gpa(gfn),
+				    pfn_to_hpa(pfn),
+				    PAGE_SIZE, flags);
 		if (r) {
 			printk(KERN_ERR "kvm_iommu_map_address:"
 			       "iommu failed to map pfn=%lx\n", pfn);
 			goto unmap_pages;
 		}
-
-		gfn += page_size >> PAGE_SHIFT;
-
-
+		gfn++;
 	}
-
 	return 0;
 
 unmap_pages:
-	kvm_iommu_put_pages(kvm, slot->base_gfn, gfn);
+	kvm_iommu_put_pages(kvm, slot->base_gfn, i, domain);
 	return r;
 }
 
-static int kvm_iommu_map_memslots(struct kvm *kvm)
+int kvm_iommu_map_pages_all(struct kvm *kvm,
+                       gfn_t base_gfn, unsigned long npages)
 {
+	int res = 0;
+/*	struct iommu_domain* sp;
+	int tmp_res;
+	printk(KERN_ERR "IOMMU: entered kvm_iommu_map_pages_all: %llx, %lx\n",
+		(u64)base_gfn, npages);
+	list_for_each_entry(sp, &kvm->arch.iommu_domain, link) {
+		tmp_res = kvm_iommu_map_pages(kvm, base_gfn, npages, sp);
+               	if (tmp_res != 0)
+            		res = tmp_res;
+       	}
+*/
+       	return res;
+}
+
+static int kvm_iommu_map_memslots(struct kvm *kvm, struct iommu_domain *domain)
+{
+	return 0;
+#if 0
 	int i, r = 0;
 	struct kvm_memslots *slots;
 
-	slots = kvm_memslots(kvm);
+	slots = rcu_dereference(kvm->memslots);
 
 	for (i = 0; i < slots->nmemslots; i++) {
-		r = kvm_iommu_map_pages(kvm, &slots->memslots[i]);
+		r = kvm_iommu_map_pages(kvm, &slots->memslots[i], domain);
 		if (r)
 			break;
 	}
 
 	return r;
+#endif
 }
 
 int kvm_assign_device(struct kvm *kvm,
 		      struct kvm_assigned_dev_kernel *assigned_dev)
 {
 	struct pci_dev *pdev = NULL;
-	struct iommu_domain *domain = kvm->arch.iommu_domain;
+	struct iommu_domain *domain = NULL;
 	int r, last_flags;
 
-	/* check if iommu exists and in use */
-	if (!domain)
-		return 0;
-
 	pdev = assigned_dev->dev;
 	if (pdev == NULL)
 		return -ENODEV;
 
+	domain = kmalloc(sizeof(*domain), GFP_KERNEL); /* TODO: free */
+	iommu_get_iommu_domain_for_dev(domain, pdev); /*iommu_domain_alloc(pdev);*/
+	if (!domain) {
+	printk(KERN_ERR "%s : cannot get iommu domain for dev %x \n",
+		__func__, assigned_dev->host_devfn);
+		return 0;
+	}
+
+       /* change: find the domain relevant */
+	if (!domain->priv)
+               printk("%s : cannot find domain\n", __func__);
+
+	printk(KERN_ERR "assign device %x:%x.%x succeeded",
+                       pdev->bus->number,
+                       PCI_SLOT(pdev->devfn),
+                       PCI_FUNC(pdev->devfn));
+       	list_add(&(domain->link), &(kvm->arch.iommu_domain));
+#if 0
 	r = iommu_attach_device(domain, &pdev->dev);
 	if (r) {
 		printk(KERN_ERR "assign device %x:%x:%x.%x failed",
@@ -162,20 +170,16 @@ int kvm_assign_device(struct kvm *kvm,
 			PCI_FUNC(pdev->devfn));
 		return r;
 	}
+#endif
+
+	/* Update the domain of the device */
+	assigned_dev->iommu_domain = domain;
 
 	last_flags = kvm->arch.iommu_flags;
-	if (iommu_domain_has_cap(kvm->arch.iommu_domain,
+	if (iommu_domain_has_cap(domain,
 				 IOMMU_CAP_CACHE_COHERENCY))
-		kvm->arch.iommu_flags |= KVM_IOMMU_CACHE_COHERENCY;
-
-	/* Check if need to update IOMMU page table for guest memory */
-	if ((last_flags ^ kvm->arch.iommu_flags) ==
-			KVM_IOMMU_CACHE_COHERENCY) {
-		kvm_iommu_unmap_memslots(kvm);
-		r = kvm_iommu_map_memslots(kvm);
-		if (r)
-			goto out_unmap;
-	}
+		domain->iommu_flags |= KVM_IOMMU_CACHE_COHERENCY;
+	kvm->arch.iommu_flags |= domain->iommu_flags; /* TODO: Remove this completely */
 
 	printk(KERN_DEBUG "assign device %x:%x:%x.%x\n",
 		assigned_dev->host_segnr,
@@ -185,14 +189,14 @@ int kvm_assign_device(struct kvm *kvm,
 
 	return 0;
 out_unmap:
-	kvm_iommu_unmap_memslots(kvm);
+	kvm_iommu_unmap_memslots(kvm, domain);
 	return r;
 }
 
 int kvm_deassign_device(struct kvm *kvm,
 			struct kvm_assigned_dev_kernel *assigned_dev)
 {
-	struct iommu_domain *domain = kvm->arch.iommu_domain;
+	struct iommu_domain *domain = assigned_dev->iommu_domain;
 	struct pci_dev *pdev = NULL;
 
 	/* check if iommu exists and in use */
@@ -203,8 +207,6 @@ int kvm_deassign_device(struct kvm *kvm,
 	if (pdev == NULL)
 		return -ENODEV;
 
-	iommu_detach_device(domain, &pdev->dev);
-
 	printk(KERN_DEBUG "deassign device %x:%x:%x.%x\n",
 		assigned_dev->host_segnr,
 		assigned_dev->host_busnr,
@@ -216,13 +218,13 @@ int kvm_deassign_device(struct kvm *kvm,
 
 int kvm_iommu_map_guest(struct kvm *kvm)
 {
-	int r;
-
 	if (!iommu_found()) {
 		printk(KERN_ERR "%s: iommu not found\n", __func__);
 		return -ENODEV;
 	}
 
+	INIT_LIST_HEAD(&kvm->arch.iommu_domain);
+#if 0
 	kvm->arch.iommu_domain = iommu_domain_alloc();
 	if (!kvm->arch.iommu_domain)
 		return -ENOMEM;
@@ -236,6 +238,8 @@ int kvm_iommu_map_guest(struct kvm *kvm)
 out_unmap:
 	kvm_iommu_unmap_memslots(kvm);
 	return r;
+#endif
+	return 0;
 }
 
 static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
@@ -247,8 +251,11 @@ static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
 }
 
 static void kvm_iommu_put_pages(struct kvm *kvm,
-				gfn_t base_gfn, unsigned long npages)
+				gfn_t base_gfn, unsigned long npages,
+				struct iommu_domain *domain)
 {
+	return;
+#if 0
 	struct iommu_domain *domain;
 	gfn_t end_gfn, gfn;
 	pfn_t pfn;
@@ -279,10 +286,14 @@ static void kvm_iommu_put_pages(struct kvm *kvm,
 
 		gfn += unmap_pages;
 	}
+#endif
 }
 
-static int kvm_iommu_unmap_memslots(struct kvm *kvm)
+static int kvm_iommu_unmap_memslots(struct kvm *kvm,
+				struct iommu_domain *domain)
 {
+	return 0;
+#if 0
 	int i;
 	struct kvm_memslots *slots;
 
@@ -294,17 +305,34 @@ static int kvm_iommu_unmap_memslots(struct kvm *kvm)
 	}
 
 	return 0;
+#endif 
 }
 
 int kvm_iommu_unmap_guest(struct kvm *kvm)
 {
-	struct iommu_domain *domain = kvm->arch.iommu_domain;
+	struct iommu_domain *domain, *tmp;
 
 	/* check if iommu exists and in use */
-	if (!domain)
+	if (list_empty(&kvm->arch.iommu_domain))
 		return 0;
 
-	kvm_iommu_unmap_memslots(kvm);
-	iommu_domain_free(domain);
+       	list_for_each_entry_safe(domain, tmp, &kvm->arch.iommu_domain, link)
+	{
+     		list_del(&domain->link);
+    		kvm_iommu_unmap_memslots(kvm, domain);
+     		kfree(domain);
+        	/*iommu_domain_free(domain);*/
+	}
+
+
+	return 0;
+}
+
+#if 0
+int kvm_iommu_unmap_memslots_plain(struct kvm* kvm,
+               struct iommu_domain* domain) 
+{
 	return 0;
 }
+
+#endif
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index f032806..c97a3dd 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -389,6 +389,10 @@ static struct kvm *kvm_create_vm(void)
 	INIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);
 #endif
 
+#ifdef KVM_CAP_DEVICE_ASSIGNMENT
+	INIT_LIST_HEAD(&kvm->arch.iommu_domain);
+#endif
+
 	r = -ENOMEM;
 	kvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);
 	if (!kvm->memslots)
@@ -411,6 +415,7 @@ static struct kvm *kvm_create_vm(void)
 	}
 
 	kvm->mm = current->mm;
+	kvm->tsk = current;
 	atomic_inc(&kvm->mm->mm_count);
 	spin_lock_init(&kvm->mmu_lock);
 	raw_spin_lock_init(&kvm->requests_lock);
@@ -699,6 +704,7 @@ skip_lpage:
 
 #ifdef CONFIG_DMAR
 	/* map the pages in iommu page table */
+#if 0
 	if (npages) {
 		r = kvm_iommu_map_pages(kvm, &new);
 		if (r)
@@ -706,6 +712,8 @@ skip_lpage:
 	}
 #endif
 
+#endif
+
 	r = -ENOMEM;
 	slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);
 	if (!slots)
@@ -932,7 +940,7 @@ unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_hva);
 
-static pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr)
+pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr)
 {
 	struct page *page[1];
 	int npages;
@@ -940,29 +948,36 @@ static pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr)
 
 	might_sleep();
 
+/*
+	TODO: Check later perhaps path according to current = kvm->tsk
 	npages = get_user_pages_fast(addr, 1, 1, page);
+*/
+	down_read(&kvm->mm->mmap_sem);
+	npages = get_user_pages(kvm->tsk, kvm->mm, addr, 1, 1, 1, page, NULL);
+	up_read(&kvm->mm->mmap_sem);
 
 	if (unlikely(npages != 1)) {
 		struct vm_area_struct *vma;
 
-		down_read(&current->mm->mmap_sem);
-		vma = find_vma(current->mm, addr);
+		down_read(&kvm->mm->mmap_sem);
+		vma = find_vma(kvm->mm, addr);
 
 		if (vma == NULL || addr < vma->vm_start ||
 		    !(vma->vm_flags & VM_PFNMAP)) {
-			up_read(&current->mm->mmap_sem);
+			up_read(&kvm->mm->mmap_sem);
 			get_page(bad_page);
 			return page_to_pfn(bad_page);
 		}
 
 		pfn = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
-		up_read(&current->mm->mmap_sem);
+		up_read(&kvm->mm->mmap_sem);
 		BUG_ON(!kvm_is_mmio_pfn(pfn));
 	} else
 		pfn = page_to_pfn(page[0]);
 
 	return pfn;
 }
+EXPORT_SYMBOL_GPL(hva_to_pfn);
 
 pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
 {
@@ -1375,9 +1390,10 @@ static long kvm_vcpu_ioctl(struct file *filp,
 	int r;
 	struct kvm_fpu *fpu = NULL;
 	struct kvm_sregs *kvm_sregs = NULL;
-
+/*
 	if (vcpu->kvm->mm != current->mm)
 		return -EIO;
+*/
 	switch (ioctl) {
 	case KVM_RUN:
 		r = -EINVAL;
@@ -1566,9 +1582,11 @@ static long kvm_vm_ioctl(struct file *filp,
 	struct kvm *kvm = filp->private_data;
 	void __user *argp = (void __user *)arg;
 	int r;
-
+/*
+	TODO: Check if current is the expected current
 	if (kvm->mm != current->mm)
 		return -EIO;
+*/
 	switch (ioctl) {
 	case KVM_CREATE_VCPU:
 		r = kvm_vm_ioctl_create_vcpu(kvm, arg);
@@ -1652,6 +1670,55 @@ static long kvm_vm_ioctl(struct file *filp,
 		mutex_unlock(&kvm->lock);
 		break;
 #endif
+	case KVM_ENABLE_VIOMMU: {
+		struct viommu_enable data;
+		r = -EFAULT;
+		if (copy_from_user(&data, argp, sizeof data))
+			goto out;
+		r = alloc_viommu(kvm, data.addr, data.sidecore);
+		break;
+	}
+	case KVM_VIOMMU_TRANSLATE: {
+		struct iommu_translate translate;
+		r = 0;
+                if (copy_from_user(&translate, argp, sizeof(translate))) {
+			r = -EFAULT;
+			break;
+             	} 
+                translate.gpa = iommu_phy_addr_translate(kvm,
+                	translate.iova, translate.is_write, 
+                        translate.g_busnr, translate.g_devfn, &translate.err);
+                if (copy_to_user(argp, &translate, sizeof(translate))) {
+                	r = -EFAULT;
+              	}
+		break;
+	}
+	case KVM_VIOMMU_SET_DEVFN_MAP: {
+		struct devfn_mapping data;
+		r = 0;
+                printk(KERN_ERR "%s: ##################### Start SET_DEVFN_MAP\n", __func__);
+                if (copy_from_user(&data, (void*)argp, sizeof(data))) {
+             		r = -EFAULT;
+            		break;
+		}
+           	intel_iommu_add_dev(kvm, &data);
+		break;
+	}
+	case KVM_DISABLE_VIOMMU: {
+		r = 0;
+		kvm_viommu_exit(kvm);
+		break;
+	}
+	case KVM_VIOMMU_WRITE: {
+		struct viommu_write data;
+		r = 0;
+		if (copy_from_user(&data, (void*)argp, sizeof(data))) {
+			r = -EFAULT;
+			break;
+		}
+		viommu_perform_write(kvm, &data);
+		break;
+	}
 	default:
 		r = kvm_arch_vm_ioctl(filp, ioctl, arg);
 		if (r == -ENOTTY)
@@ -1719,8 +1786,10 @@ static int kvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (kvm_is_error_hva(addr))
 		return VM_FAULT_SIGBUS;
 
-	npages = get_user_pages(current, current->mm, addr, 1, 1, 0, page,
+	npages = get_user_pages(kvm->tsk, kvm->mm, addr, 1, 1, 0, page,
 				NULL);
+
+
 	if (unlikely(npages != 1))
 		return VM_FAULT_SIGBUS;
 
diff --git a/virt/kvm/viommu.c b/virt/kvm/viommu.c
new file mode 100644
index 0000000..7507912
--- /dev/null
+++ b/virt/kvm/viommu.c
@@ -0,0 +1,3143 @@
+#define DEBUG 1 /* for pr_debug */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/semaphore.h>
+#include <linux/smp_lock.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/cacheflush.h>
+#include <asm/mtrr.h>
+#include <asm/unistd.h>
+#include <asm/timer.h>
+
+#include <linux/kvm_host.h>
+#include <linux/kvm_types.h>
+#include <linux/kvm.h>
+
+#include <linux/highmem.h>
+#include <linux/delay.h>
+#include <linux/pm.h>
+
+#include <stddef.h>
+
+#include <linux/intel-iommu.h>
+#include <linux/iommu.h>
+#include <linux/viommu.h>
+#include <linux/proc_fs.h>	/* Necessary because we use the proc fs */
+#include <linux/perf_event.h>
+#include <linux/smp.h>
+
+
+MODULE_AUTHOR("Nadav Amit <namit@cs.technion.ac.il>");
+MODULE_DESCRIPTION("Uncachable memory region");
+MODULE_LICENSE("GPL");
+#define MODULE_NAME "viommu"
+
+#define DMAR_DEVICES_NUM 1
+#define DMAR_REG_BASE 0xFED10000
+#define INVALIDATION_QUEUE 1
+#define IOMMU_PWC 0
+#define IOMMU_DEFERRED_UNMAP 1
+#define MONITOR_CYCLES 1
+
+enum {CIRG_GLOBAL_INVALIDATION = 1, 
+	CIRG_DOMAIN_SELECTIVE_INVALIDATION = 2,
+	CIRG_DEVICE_SELECTIVE_INVALIDATION = 3};
+
+enum {IIRG_GLOBAL_INVALIDATION = 1, 
+	IIRG_DOMAIN_SELECTIVE_INVALIDATION = 2,
+	IIRG_DOMAIN_PAGE_SELECTIVE_INVALIDATION	= 3};
+
+
+
+/*
+typedef enum { IOMMU_READ = 1, IOMMU_WRITE = 2 } IommuAccessType;
+*/
+#define print_err( msg, pr_err, err ) 					\
+	do {								\
+		*(err) = 1; 						\
+			if (pr_err) { 					\
+			printk(KERN_ERR "intel_iommu.c: %s \n", (msg)); \
+		} 							\
+	} while (0)
+	
+
+#define IOMMU_CONTEXT_ENTRIES 	256
+#define MGAW 			32
+#define FRCD_REG_NUM 		4
+#define DMAR_REGS_PAGE 		0x1000
+#define DMAR_REGS_PAGE_SHIFT 	12
+#define LEVEL_STRIDE		9
+/*
+#define VTD_PAGE_SHIFT		12
+*/
+#define DEVFN_ENTRIES_NUM 	256
+#define IOMMU_CACHE_COHERENCY 	0x1
+
+
+enum {IOMMU_MAP_FRAME = 1, IOMMU_UNMAP_FRAME = 2};
+
+/* Mapping related structures */
+struct IommuRootEntry {
+	uint32_t p 		: 1 ;
+	uint32_t res1 		: 11 ;
+	uint64_t ctp 		: 52 ;
+	uint64_t res2		: 64 ;
+};
+
+struct IommuContextEntry {
+	uint32_t p		: 1 ;
+	uint32_t fpd		: 1 ;
+	uint32_t t		: 2 ;
+	uint32_t eh		: 1 ;
+	uint32_t alh 		: 1 ;
+	uint32_t res1		: 6 ;
+	uint64_t asr		: 52 ;
+	uint32_t aw		: 3 ;
+	uint32_t avail		: 4 ;
+	uint32_t res2		: 1;
+	uint32_t did		: 16;
+	uint64_t res3		: 40;
+};
+
+struct bus_root {
+	uint16_t busnr;
+	struct IommuRootEntry root;
+	struct list_head node;/* QLIST_ENTRY(bus_root) node;*/
+};
+
+struct devfn_context {
+	uint8_t g_busnr;
+	uint8_t g_devfn;
+	uint8_t h_busnr;
+	uint8_t h_devfn;
+	struct IommuContextEntry context;
+     	struct iommu_domain iommu_domain; 
+	uint8_t iommu_width;
+	struct list_head node; 
+};
+
+
+enum { TRASNLATE_UNTRANSLATED = 0, TRANSLATE_ALL = 1, TRANSLATE_PASS_THROUGH = 2, TRANSLATE_RESERVED = 3};
+
+struct IommuPageTableEntry {
+	uint32_t r		: 1 ;
+	uint32_t w		: 1 ;
+	uint32_t avail1		: 5 ;
+	uint32_t sp		: 1 ;
+	uint32_t avail2		: 3 ;
+	uint32_t snp		: 1 ;
+	uint64_t addr		: 40 ;
+	uint32_t avail3		: 10 ;
+	uint32_t tm		: 1 ;
+	uint32_t avail4 	: 1 ;
+};
+
+
+/* IOMMU Registers */
+struct gcmd_reg
+{
+	uint32_t res		: 23;
+	uint32_t cfi		: 1;
+	uint32_t sirtp		: 1;
+	uint32_t ire		: 1;
+	uint32_t qie		: 1;
+	uint32_t wbf		: 1;
+	uint32_t eafl		: 1;
+	uint32_t sfl		: 1;
+	uint32_t srtp		: 1;
+	uint32_t te		: 1;
+}	__attribute__((__packed__));
+	
+struct gsts_reg
+{
+	uint32_t res		: 23;
+	uint32_t cfis		: 1;
+	uint32_t irtps		: 1;
+	uint32_t ires		: 1;
+	uint32_t qies		: 1;
+	uint32_t wbfs		: 1;
+	uint32_t afls		: 1;
+	uint32_t fls		: 1;
+	uint32_t rtps		: 1;
+	uint32_t tes		: 1;
+}	__attribute__((__packed__));
+
+struct ecap_reg 
+{
+	uint32_t c		: 1;
+	uint32_t qi		: 1;
+	uint32_t di		: 1;
+	uint32_t ir		: 1;
+	uint32_t eim		: 1;
+	uint32_t ch		: 1;
+	uint32_t pt		: 1;
+	uint32_t sc		: 1;
+	uint32_t iro		: 10;
+	uint32_t res1		: 2;
+	uint32_t mhmv		: 4;
+	uint64_t res2		: 40;	
+}	__attribute__((__packed__));
+
+struct ver_reg
+{
+	uint32_t min		: 4;
+	uint32_t max		: 4;
+	uint32_t res		: 24;
+}	__attribute__((__packed__));
+
+struct cap_reg
+{
+	uint32_t nd		: 3;
+	uint32_t afl		: 1;
+	uint32_t rwbf		: 1;
+	uint32_t plmr		: 1;
+	uint32_t phmr		: 1;
+	uint32_t cm		: 1;
+	uint32_t sagaw		: 5;
+	uint32_t res1		: 3;
+	uint32_t mgaw		: 6;
+	uint32_t zlr		: 1;
+	uint32_t isoch		: 1;
+	uint32_t fro		: 10;
+	uint32_t sps		: 4;
+	uint32_t res2		: 1;
+	uint32_t psi		: 1;
+	uint32_t nfr		: 8;
+	uint32_t mamv		: 6;
+	uint32_t dwd		: 1;
+	uint32_t drd		: 1;
+	uint32_t res3		: 8;
+}	__attribute__((__packed__));
+
+enum {SPS_21_BIT = 1, SPS_30_BIT = 2, SPS_39_BIT = 4, SPS_48_BIT = 8}; /* sagaw values */
+enum {AGAW_30_BIT = 1, AGAW_39_BIT = 2, AGAW_48_BIT = 4, AGAW_57_BIT = 8, AGAW_64_BIT = 16}; /* agaw values */
+
+
+struct ccmd_reg {
+	uint32_t did		: 16;
+	uint32_t sid		: 16;
+	uint32_t fm		: 2;
+	uint32_t res		: 25;
+	uint32_t caig		: 2;
+	uint32_t cirg		: 2;
+	uint32_t icc		: 1;
+}	__attribute__((__packed__));
+	
+struct fsts_reg {
+	uint32_t pfo		: 1;
+	uint32_t ppf		: 1;
+	uint32_t afo		: 1;
+	uint32_t apf		: 1;
+	uint32_t iqe		: 1;
+	uint32_t ice		: 1;
+	uint32_t ite		: 1;
+	uint32_t res1		: 1;
+	uint32_t fri		: 8;
+	uint32_t res2		: 16;
+}	__attribute__((__packed__));
+	
+
+struct fectl_reg {
+	uint32_t res		: 30;
+	uint32_t ip		: 1;
+	uint32_t im		: 1;
+}	__attribute__((__packed__));
+
+struct fedata_reg {
+	uint32_t imd		: 16;
+	uint32_t eimd		: 16;
+}	__attribute__((__packed__));
+
+struct feaddr_reg {
+	uint32_t res		: 2;
+	uint32_t ma		: 30;
+}	__attribute__((__packed__)); 
+
+struct aflog_reg {
+	uint32_t res		: 9;
+	uint32_t fls		: 3;
+	uint64_t fla		: 52;
+}	__attribute__((__packed__)); 
+
+struct pmen_reg {
+	uint32_t prs		: 1;
+	uint32_t res		: 30;
+	uint32_t epm		: 1;
+}	__attribute__((__packed__)); 
+
+
+struct iqh_reg {
+	uint32_t res1		: 4;
+	uint32_t qh		: 15;
+	uint64_t res2		: 45;
+}	__attribute__((__packed__)); 
+
+struct iqt_reg {
+	uint32_t res1		: 4;
+	uint32_t qt		: 15;
+	uint64_t res2		: 45;
+}	__attribute__((__packed__)); 
+
+struct iqa_reg {
+	uint32_t qs		: 3;
+	uint32_t res		: 9;
+	uint64_t iqa		: 52;
+}	__attribute__((__packed__)); 
+
+struct ics_reg {
+	uint32_t iwc		: 1;
+	uint32_t res		: 31;
+}	__attribute__((__packed__)); 
+
+struct iectl_reg {
+	uint32_t res		: 30;
+	uint32_t ip		: 1;
+	uint32_t im		: 1;
+}	__attribute__((__packed__)); 
+
+struct iedata_reg {
+	uint32_t imd 		: 16;
+	uint32_t eimd		: 16;
+}	__attribute__((__packed__)); 
+
+struct ieaddr_reg {
+	uint32_t res 		: 2;
+	uint32_t ma		: 30;
+}	__attribute__((__packed__)); 
+
+/* IOTLB Registers */
+struct iotlb_reg {
+	uint32_t res1 		: 32;
+	uint32_t did 		: 16;
+	uint32_t dw		: 1;
+	uint32_t dr		: 1;
+	uint32_t res2		: 7;
+	uint32_t iaig		: 2;
+	uint32_t res3		: 1;
+	uint32_t iirg		: 2;
+	uint32_t res4		: 1;
+	uint32_t ivt		: 1;
+}	__attribute__((__packed__)); 
+
+struct iva_reg {
+	uint32_t am		: 6;
+	uint32_t ih		: 1;
+	uint32_t res		: 5;
+	uint64_t addr		: 52;
+}	__attribute__((__packed__));
+
+/* Faults reporting register */
+struct frcd_reg {
+	uint32_t res1		: 12;
+	uint64_t fi		: 52;
+	uint32_t sid		: 16;
+	uint32_t res2		: 16;
+	uint32_t fr		: 8;
+	uint32_t res3		: 20;
+	uint32_t at		: 2;
+	uint32_t t		: 1;
+	uint32_t f		: 1;
+}	__attribute__((__packed__));
+	
+struct dmar_regs_region {
+	struct ver_reg ver;	/* Arch version supported by this IOMMU */
+	uint32_t res1;		/* Reserved */
+	struct cap_reg cap;	/* Hardware supported capabilities */
+	struct ecap_reg ecap;	/* Extended capabilities supported */
+	struct gcmd_reg gcmd;	/* Global command register */
+	struct gsts_reg gsts;	/* Global status register */
+	uint64_t readdr;	/* Root entry table */
+	struct ccmd_reg ccmd;	/* Context command reg */
+	uint32_t res2;		/* Reserved */
+	uint32_t fsts;		/* Fault Status register */
+	uint32_t fectl;		/* Fault control register */
+	uint32_t fedata;	/* Fault event interrupt data register */
+	uint32_t feaddr;	/* Fault event interrupt addr register */
+	uint32_t feuaddr; 	/* Upper address register */
+	uint64_t res3[2];	/* Reserved */
+	uint64_t aflog;		/* Advanced Fault control */
+	uint32_t res4; 		/* Reserved */
+	struct pmen_reg pmen;	/* Enable Protected Memory Region */
+	uint32_t plmbase;	/* PMRR Low addr */
+	uint32_t plmlimit;	/* PMRR low limit */
+	uint64_t phmbase;	/* pmrr high base addr */
+	uint64_t phmlimit;	/* pmrr high limit */
+	struct iqh_reg iqh;	/* Invalidation queue head register */
+	struct iqt_reg iqt;		/* Invalidation queue tail register */
+	struct iqa_reg iqa;		/* Invalidation queue addr register */
+	uint32_t res5;		/* Reserved */
+	uint32_t ics;		/* Invalidation complete status register */
+	uint32_t icec;		/* Invalidation Completion Event Control Register */
+	uint32_t iced; 		/* Invalidation Queue Event message data register */
+	uint32_t icea;		/* Invalidation Completion Event Address Register */
+	uint32_t iceua; 	/* Invalidation Completion Event Upper Address Register */
+	uint64_t res6;		/* Reserved */
+	uint64_t irtar;		/* Interrupt Remapping Table Address Register */
+	struct iva_reg iva;	/* Invalidate Address Register */
+	struct iotlb_reg iotlb; /* IOTLB Invalidate Register */
+	struct frcd_reg frr[FRCD_REG_NUM];	/* Fault recording register */
+#if MONITOR_CYCLES
+	uint64_t active_cycles;
+	uint64_t inactive_cycles;
+#endif
+} __attribute__((__packed__));
+
+/* Invalidation queue descriptors */
+enum { 	IQ_CONTEXT_INVD_DESC_ID = 1, 
+	IQ_IOTLB_INVD_DESC_ID = 2, 
+	IQ_DEV_IOTLB_INVD_DESC_ID = 3,
+	IQ_INT_CACHE_INV_DESC_ID = 4,
+	IQ_INVD_WAIT_DESC_ID = 5 };
+
+struct iommu_generic_desc {
+	uint32_t id		: 4;
+	uint64_t res1		: 60;
+	uint64_t res2		: 64; 
+} __attribute__((__packed__));
+
+struct iotlb_invd_desc {
+	uint32_t id		: 4;
+	uint32_t g		: 2;
+	uint32_t dw		: 1;
+	uint32_t dr		: 1;
+	uint32_t res1		: 8;
+	uint32_t did		: 16;
+	uint32_t res2		: 32;
+	uint32_t am		: 6;
+	uint32_t ih		: 1;
+	uint32_t res3		: 5;
+	uint64_t addr		: 52;
+	struct iotlb_reg iotlb;
+} __attribute__((__packed__));
+
+struct context_cache_invd_desc {
+	uint32_t id		: 4;
+	uint32_t g		: 2;
+	uint32_t res1		: 10;
+	uint32_t did		: 16;
+	uint32_t sid		: 16;
+	uint32_t fm		: 2;
+	uint32_t res2		: 14;
+	uint64_t res3		: 64;	
+} __attribute__((__packed__));
+
+struct inv_wait_desc {
+	uint32_t id		: 4;
+	uint32_t iflag		: 1;
+	uint32_t sw		: 1;
+	uint32_t fn		: 1;
+	uint32_t res1		: 25;
+	uint32_t stat_data	: 32;
+	uint32_t res2		: 2;
+	uint64_t stat_addr	: 62;
+} __attribute__((__packed__));
+
+struct int_entry_cache_invd_desc {
+	uint32_t id 		: 4;
+	uint32_t g		: 1;
+	uint32_t res1		: 22;
+	uint32_t im		: 5;
+	uint32_t iidx		: 16;
+	uint32_t res2		: 16;
+	uint64_t res3		: 64;
+} __attribute__((__packed__));
+
+struct dev_iotlb_invd_desc {
+	uint32_t id		: 4;
+	uint32_t res1		: 12;
+	uint32_t max_invs_pend 	: 5;
+	uint32_t res2		: 11;
+	uint32_t sid		: 16;
+	uint32_t res3		: 16;
+	uint32_t s		: 1;
+	uint32_t res4		: 11;
+	uint64_t addr		: 52;
+} __attribute__((__packed__));
+
+/*************************************/
+
+
+struct dmar_status
+{
+	/* cached data */
+	/*pthread_rwlock_t*/ spinlock_t access_lock;
+	struct iotlb_regs* iotlb_regs;
+	struct list_head devfn_context_head;
+	struct dmar_regs_region dmar_regs;
+	struct page *phy_dmar_regs_page;
+	struct dmar_regs_region* phy_dmar_regs;
+
+	/* Some caches - assuming the virtual machine has only one bus (0) */
+	struct IommuRootEntry re;
+	struct IommuContextEntry ce[IOMMU_CONTEXT_ENTRIES];
+	/* caching of pages */
+	struct page* iq_page;
+/*	gfn_t guest_cache_frame;
+	struct page* guest_cache_page;
+*/	struct iommu_generic_desc* iq;
+/*	int guest_cache_valid;
+*/	/*QLIST_HEAD(bus_root_head, bus_root) bus_root_head; */
+} __attribute__((aligned(8)));
+
+struct iommu_state {
+	int mmio_index;
+	int invalidation_queue;
+/*	int mfd;*/
+/*	volatile struct dmar_regs_region* pregs;*/
+	struct dmar_status dmar_status[DMAR_DEVICES_NUM];
+} __attribute__((aligned(8)));
+
+/* a structure to store all information we need
+   for our thread */
+struct kthread_t
+{
+        struct task_struct *thread;
+        int running;
+        
+	/* additional data to pass to kernel thread */
+/*	struct kvm *kvm;
+*/
+} __attribute__((aligned(8)));
+/*
+struct kthread_t *kthread = NULL;
+*/
+/*#define VIOMMU_PAGE_CACHE_SIZE 512
+*/
+
+/*
+static int sidecore;
+
+static int __init viommu_setup(char *str)
+{
+	if (!str)
+		return -EINVAL;
+	while (*str) {
+		if (!strncmp(str, "sidecore", 8)) {
+			sidecore = 1;
+			printk(KERN_INFO "VIOMMU: sidecore\n");
+		} 
+		str += strcspn(str, ",");
+		while (*str == ',')
+			str++;
+	}
+	return 0;
+}
+
+__setup("viommu=", viommu_setup);
+*/
+
+#define PROCFS_MAX_SIZE		512
+#define PROCFS_NAME 		"viommu"
+
+
+/**
+ * This structure hold information about the /proc file
+ *
+ */
+static struct proc_dir_entry *Our_Proc_File;
+
+/**
+ * The buffer used to store character for this module
+ *
+ */
+static char procfs_buffer[PROCFS_MAX_SIZE];
+
+/**
+ * The size of the buffer
+ *
+ */
+static unsigned long procfs_buffer_size = 0;
+
+struct counter_data {
+	uint64_t time_counter;
+	uint64_t temp_start_counter;
+	uint64_t events_counter;
+	uint64_t min_time_counter;
+	uint64_t max_time_counter;
+} __attribute__((aligned(8)));
+
+enum {HIT_COUNTER, MISS_COUNTER, MAP_COUNTER, UNMAP_COUNTER, FIND_IOVA_COUNTER, TRANSLATE_COUNTER, WRONG_WAKE_COUNTER, CORRECT_WAKE_COUNTER, COUNTERS_NUM};
+enum {SETS_CONTROL, WAYS_CONTROL, MSLEEP_CONTROL, CSTATE_CONTROL, CONTROLS_NUM};
+
+static const char* proc_control_names[] = {"sets", "ways", "msleep", "cstate"};
+static const char* proc_output_names[] = {"hits", "misses", "map", "unmap", "find iova", "translate", "wrong_wake", "correct_wake"};
+
+static struct counter_data counters_data[COUNTERS_NUM];
+static int64_t controls_data[CONTROLS_NUM];
+static uint64_t unhalted_cnt;
+static uint64_t cycles_cnt;
+
+static void clear_counters(void)
+{
+	int i;
+	for (i=0; i < COUNTERS_NUM; i++) {
+		counters_data[i].time_counter = 0;
+		counters_data[i].temp_start_counter = 0;
+		counters_data[i].events_counter = 0;
+	}
+}
+
+static void clear_controls(void)
+{
+	int i;
+	for (i=0; i < CONTROLS_NUM; i++) {
+		controls_data[i] = -1;
+	}
+}
+#define VIOMMU_SAMPLE 0x100
+
+static void print_counters(char* buf)
+{
+	int i;
+	char temp_buf[1000];
+	buf[0] = '\0';
+	for (i=0; i<COUNTERS_NUM; i++) {
+		uint64_t samples = counters_data[i].events_counter / VIOMMU_SAMPLE;
+		sprintf(temp_buf, "%s -  Total: %llx   Avg: %llx  Min:%llx  Max: %llx   Event: %llx\n", 
+			proc_output_names[i], 
+			counters_data[i].time_counter, (samples == 0) ? 0 :
+			(counters_data[i].time_counter / samples),
+			counters_data[i].min_time_counter,
+			counters_data[i].max_time_counter,
+			counters_data[i].events_counter);
+
+		strcat(buf, temp_buf);
+	}
+	sprintf(temp_buf, "unhalted: %llx\n", unhalted_cnt);
+	strcat(buf, temp_buf);
+	sprintf(temp_buf, "cycles: %llx\n", cycles_cnt);
+	strcat(buf, temp_buf);
+}
+
+
+static inline void viommu_update_counter(int counter, int start)
+{
+#if 0
+	int sample_time = ((counters_data[counter].events_counter % VIOMMU_SAMPLE) == 0);
+	
+	if (start && sample_time) {
+        	counters_data[counter].temp_start_counter = get_cycles();
+	} else if (!start) {
+		if (sample_time) {
+			uint64_t cycles = get_cycles() - counters_data[counter].temp_start_counter;
+			counters_data[counter].time_counter += cycles;
+			if ((cycles < counters_data[counter].min_time_counter ||
+				counters_data[counter].min_time_counter == 0) &&
+				cycles > 0)
+				counters_data[counter].min_time_counter = cycles;
+			if (cycles > counters_data[counter].max_time_counter)
+				counters_data[counter].max_time_counter = cycles;
+		}
+		counters_data[counter].events_counter++;
+	}
+#endif
+}
+
+
+
+/** 
+ * This function is called then the /proc file is read
+ *
+ */
+int 
+procfile_read(char *buffer,
+	      char **buffer_location,
+	      off_t offset, int buffer_length, int *eof, void *data)
+{
+	int ret;
+	
+	printk(KERN_INFO "procfile_read (/proc/%s) called\n", PROCFS_NAME);
+	
+	if (offset > 0) {
+		/* we have finished to read, return 0 */
+		ret  = 0;
+	} else {
+		/* fill the buffer, return the buffer size */
+		procfs_buffer[0] = '\0';
+		print_counters(procfs_buffer);
+		memcpy(buffer, procfs_buffer, strlen(procfs_buffer));
+		ret = strlen(procfs_buffer);
+	}
+
+	return ret;
+}
+
+/**
+ * This function is called with the /proc file is written
+ *
+ */
+int procfile_write(struct file *file, const char *buffer, unsigned long count,
+		   void *data)
+{
+	/* get buffer size */
+	char temp_buf[PROCFS_MAX_SIZE];
+	int found = 0;
+	int i;
+	if (count > PROCFS_MAX_SIZE ) {
+		count = PROCFS_MAX_SIZE;
+	}
+	
+	/* write data to the buffer */
+	if ( copy_from_user(temp_buf, buffer, count) ) {
+		return -EFAULT;
+	}
+
+	if (strncmp(temp_buf, "clear", 5) == 0) {
+		clear_counters();
+		printk(KERN_ERR "%s : cleared counters", __func__);
+		found =1;
+	}
+
+	for (i=0; i<CONTROLS_NUM; i++) {
+		if (strncmp(temp_buf, proc_control_names[i], strlen(proc_control_names[i])) == 0) {
+			long val;
+			char* temp;
+			printk(KERN_ERR "FOUND!");
+			if (*(temp_buf+strlen(proc_control_names[i])) != '=') {
+				printk(KERN_ERR "But the char is %s", temp_buf+strlen(proc_control_names[i]));
+/*				continue;*/
+			}
+ 			val = simple_strtol(temp_buf+strlen(proc_control_names[i])+1, 
+				&temp, 0);
+			printk(KERN_ERR "%s: setting the value of %s to %ld ", __func__, 
+				proc_control_names[i], val);
+			controls_data[i] = val;
+			found = 1;
+			break;
+		}
+	}
+	if (!found) {
+		printk(KERN_ERR "%s: could not find the control name", __func__);
+	}
+	
+	return count;
+}
+
+static void init_procfs(void)
+{
+	/* create the /proc file */
+	Our_Proc_File = create_proc_entry(PROCFS_NAME, 0644 ,NULL);
+	
+	if (Our_Proc_File == NULL) {
+		remove_proc_entry(PROCFS_NAME, Our_Proc_File);
+		printk(KERN_ALERT "Error: Could not initialize /proc/%s\n",
+			PROCFS_NAME);
+		return;
+	}
+
+	Our_Proc_File->read_proc  = procfile_read;
+	Our_Proc_File->write_proc = procfile_write;
+/*	Our_Proc_File->owner 	  = THIS_MODULE;*/
+/*	Our_Proc_File->mode 	  = S_IFREG | S_IRUGO;
+	Our_Proc_File->uid 	  = 0;
+	Our_Proc_File->gid 	  = 0;
+	Our_Proc_File->size 	  = 37;*/
+	Our_Proc_File->data = NULL;
+	clear_controls();
+}
+
+
+
+#define VIOMMU_PAGE_CACHE_DIRTY		 (1<<0)
+#define VIOMMU_PAGE_CACHE_USER		 (1<<1)
+#define VIOMMU_PAGE_CACHE_SETS 8192
+#define VIOMMU_PAGE_CACHE_WAYS 1
+#define VIOMMU_PAGE_CACHE_SIZE (VIOMMU_PAGE_CACHE_SETS * VIOMMU_PAGE_CACHE_WAYS)
+#define VIOMMU_PWC_SETS		1024
+
+struct iova_pwc_entry
+{
+        uint64_t iova;
+        uint64_t last_level_pte; /* if low bit is one - we will consider it as invalid */
+        uint16_t ref_cnt;
+        uint8_t busnr;
+        uint8_t devfn;
+	uint8_t perm;
+} __attribute__((packed));
+
+
+struct viommu_page_cache_entry
+{
+	struct page *page;
+	union {
+		pfn_t pfn;
+		void* ptr;
+	};
+	gfn_t gfn; /* key */
+	uint32_t flags;
+#ifdef VIOMMU_LRU
+	uint8_t lru;
+#endif
+} __attribute__((packed));
+
+struct iova_cache_entry
+{
+	unsigned long iova_fn;
+	pfn_t pfn;
+} __attribute__((packed));
+
+struct iova_entry
+{
+     	struct iommu_domain *iommu_domain; 
+	uint64_t iova;
+	uint64_t hpa;
+	int valid;
+	int mapped;
+} __attribute__((packed));
+
+struct pending_unmap
+{
+	struct devfn_context *devfn_context;
+	gpa_t iova;
+	gpa_t size;
+} __attribute__((packed));
+
+struct viommu
+{
+	/*uint8_t* page;*/
+        /*struct timer_interrupt_hook timer_hook;
+        */
+	#ifdef VIOMMU_LRU
+	struct viommu_page_cache_entry page_cache[VIOMMU_PAGE_CACHE_SETS][VIOMMU_PAGE_CACHE_WAYS];
+	#else
+	struct viommu_page_cache_entry page_cache[VIOMMU_PAGE_CACHE_SETS];
+	#endif
+	#ifdef IOMMU_PWC
+	struct iova_pwc_entry iova_pwc[VIOMMU_PWC_SETS];
+	#endif
+	struct iommu_state iommu_state;
+        spinlock_t lock;
+        int mapped;
+        int timer;
+	int mtrr;
+	struct kthread_t *sidecore_thread;
+	unsigned int timer_invocation;
+	struct iova_entry cached_iova;
+	struct pending_unmap pending_unmap;
+	int deferred;
+} __attribute__((aligned(8)));
+
+/* Forward declaration */
+static
+uint64_t __iommu_phy_addr_translate(struct kvm *kvm, gpa_t addr,
+                            uint8_t* access_perm, uint8_t busnr, 
+			    uint8_t devfn, uint64_t* size, int sidecore);
+
+static 
+int intel_iommu_analyze_cirg(uint32_t cirg, uint32_t fm,
+	uint8_t* invalidate_all, uint8_t* start_fm_bit, uint8_t* fm_bits_num);
+
+static
+void intel_iommu_clear_devfn_context(struct dmar_status* dmar_status);
+/*
+static
+struct list_head* intel_iommu_find_devfn(struct dmar_status* dmar_status, 
+	int did, struct list_head* start_devfn);
+*/
+static
+void intel_iommu_update_devfn_context(
+	struct kvm *kvm,
+	struct dmar_status* dmar_status,
+	uint32_t sid, uint32_t cirg, uint32_t fm);
+
+static
+int create_iommu_monitoring_thread(struct kvm* kvm);
+
+static
+void intel_iommu_ccmd_invd(struct kvm *kvm, struct dmar_status* dmar_status);
+
+static 
+int intel_iommu_process_iq(struct kvm *kvm, struct dmar_status* dmar_status);
+
+static void viommu_load_iq(struct kvm* kvm, struct dmar_status *dmar_status);
+
+/* Translation related functions */
+static inline int aw_to_levels(int aw) {
+	return aw+2;
+}
+
+
+static inline int level_shift(int level) {
+	return VTD_PAGE_SHIFT+(level*LEVEL_STRIDE);
+}
+
+static inline uint64_t level_size(int level) {
+	return (1ULL<<level_shift(level));
+}
+
+static inline uint64_t level_offset(uint64_t addr, int level) {
+	return (addr >> level_shift(level))&((1ULL<<LEVEL_STRIDE)-1);
+}
+
+static inline uint64_t level_mask(int level) {
+	return (((uint64_t)1)<<level_shift(level))-1;
+}
+
+
+#define copy_field(target, source, field) \
+	do {	\
+		if (target == NULL) \
+			break; \
+		printk(KERN_ERR "WRITE TO field VAL %x\n", *(uint32_t*)&source->field); \
+		copy_to_user(&target->field, &source->field, sizeof(target->field)); \
+		clflush_cache_range(&target->field, sizeof(target->field)); \
+		mb();  \
+	} while (0)
+
+#ifndef VIOMMU_LRU
+
+#if 0
+static void smp_clflush(void* info)
+{
+	clflush_cache_range(info, 8);
+}
+#endif
+
+static void all_cores_wbinvd(void* ptr)
+{
+	wbinvd();
+}
+
+
+static struct viommu_page_cache_entry* viommu_gpa_to_cache(struct kvm *kvm, gpa_t addr, int dirty, int user)
+{
+	uint16_t sets_num, set;
+	uint16_t found = 0;
+	struct viommu *viommu = kvm->viommu;
+	struct viommu_page_cache_entry* page_cache_entry;
+	gfn_t gfn = addr >> PAGE_SHIFT;
+	viommu_update_counter(HIT_COUNTER,1);
+	viommu_update_counter(MISS_COUNTER,1);
+	sets_num = VIOMMU_PAGE_CACHE_SETS;
+/*
+	if ((controls_data[SETS_CONTROL] >= 1) && (controls_data[SETS_CONTROL] <= VIOMMU_PAGE_CACHE_SETS))
+		sets_num = (uint16_t)controls_data[SETS_CONTROL];
+*/
+	set = (uint16_t)(gfn % sets_num);
+	page_cache_entry = &(viommu->page_cache[set]);
+	if (page_cache_entry->page) {
+		if (gfn != page_cache_entry->gfn ||
+			((page_cache_entry->flags & VIOMMU_PAGE_CACHE_USER) !=
+			(user ? VIOMMU_PAGE_CACHE_USER : 0 ))) 
+		{
+			if (!(page_cache_entry->flags & VIOMMU_PAGE_CACHE_USER))
+				kunmap(page_cache_entry->page);
+			if (page_cache_entry->flags & VIOMMU_PAGE_CACHE_DIRTY) {
+				kvm_release_page_dirty(page_cache_entry->page);
+			} else {
+				kvm_release_page_clean(page_cache_entry->page);
+			}
+		} 
+		else {
+			found = 1;
+		}
+	}
+	if (!found) {
+		pfn_t pfn;
+		pfn = gfn_to_pfn(kvm, gfn);
+		if (!kvm_is_mmio_pfn(pfn))
+ 			page_cache_entry->page = pfn_to_page(pfn);
+
+		if (unlikely(page_cache_entry->page == bad_page || page_cache_entry->page == NULL)) {
+			printk(KERN_ERR "%s : problem getting page for address %llx", 
+				__func__, (uint64_t)addr);
+			kvm_release_page_clean(page_cache_entry->page);
+			page_cache_entry->page = NULL;
+			return NULL;
+		}
+		/* gfn_to_hva might be redundant - can do things in better order */
+		if (user)
+			page_cache_entry->pfn = pfn;
+		else
+			page_cache_entry->ptr = kmap(page_cache_entry->page);
+
+		page_cache_entry->gfn = gfn;
+		page_cache_entry->flags = user ? VIOMMU_PAGE_CACHE_USER : 0;
+	}
+	
+	if (dirty)
+		page_cache_entry->flags |= VIOMMU_PAGE_CACHE_DIRTY;
+
+	viommu_update_counter(found?HIT_COUNTER:MISS_COUNTER, 0);
+	return page_cache_entry;
+}
+
+#else
+static struct viommu_page_cache_entry* viommu_gpa_to_cache(struct kvm *kvm, gpa_t addr, int dirty, int user)
+{
+	/* Assuming no page boundary */
+	int i, found=0;
+	struct viommu_page_cache_entry *found_entry = NULL;
+	struct viommu_page_cache_entry *lru_entry = NULL;
+	int found_lru = VIOMMU_PAGE_CACHE_WAYS;
+	struct viommu *viommu = kvm->viommu;
+	gfn_t gfn = addr >> PAGE_SHIFT;
+	uint16_t sets_num;
+	uint16_t ways_num; 
+	uint16_t set;
+	viommu_update_counter(HIT_COUNTER,1);
+	viommu_update_counter(MISS_COUNTER,1);
+	sets_num = VIOMMU_PAGE_CACHE_SETS;
+	ways_num = VIOMMU_PAGE_CACHE_WAYS;
+
+	if ((controls_data[SETS_CONTROL] >= 1) && (controls_data[SETS_CONTROL] <= VIOMMU_PAGE_CACHE_SETS))
+		sets_num = (uint16_t)controls_data[SETS_CONTROL];
+
+	if ((controls_data[WAYS_CONTROL] >= 1) && (controls_data[WAYS_CONTROL] <= VIOMMU_PAGE_CACHE_WAYS))
+		ways_num = (uint16_t)controls_data[WAYS_CONTROL];
+
+	set = (uint16_t)(gfn % sets_num);
+
+	for (i=0; i < ways_num; i++) {
+		struct viommu_page_cache_entry *page_cache_entry = 
+			&(viommu->page_cache[set][i]);
+		if (!page_cache_entry->page) {
+			/* No entry - remember as LRU */
+			lru_entry = page_cache_entry;
+			continue;
+		}
+		if (gfn == page_cache_entry->gfn && 
+			((page_cache_entry->flags & VIOMMU_PAGE_CACHE_USER) ==
+			(user ? VIOMMU_PAGE_CACHE_USER : 0 ))) {
+			/* found */
+			found_entry = page_cache_entry;
+			found_lru = found_entry->lru;
+			found = 1;
+			break;
+		}
+		/* since we are already here, let's find the lru entry */
+		if (lru_entry == NULL || 
+			(lru_entry->page != NULL && 
+				lru_entry->lru < page_cache_entry->lru)) {
+				lru_entry = page_cache_entry;
+		}
+	}
+
+	if (!found_entry) {
+		if (lru_entry->page) {
+			if (!(lru_entry->flags & VIOMMU_PAGE_CACHE_USER))
+				kunmap(lru_entry->page);
+			if (lru_entry->flags & VIOMMU_PAGE_CACHE_DIRTY) {
+				kvm_release_page_dirty(lru_entry->page);
+			} else {
+				kvm_release_page_clean(lru_entry->page);
+			}
+		}
+	} 
+	
+	/* need to find vacant entry and/or increase LRU values for the rest */
+	for (i=0; i < ways_num; i++) {
+		struct viommu_page_cache_entry* page_cache_entry = 
+			&(viommu->page_cache[set][i]);
+		if (page_cache_entry->lru < found_lru)
+			page_cache_entry->lru++;
+	}
+
+	/* not found - create a new one */
+	if (!found_entry) {
+		pfn_t pfn;
+		pfn = gfn_to_pfn(kvm, gfn);
+		lru_entry->page = NULL;
+		if (!kvm_is_mmio_pfn(pfn))
+ 			lru_entry->page = pfn_to_page(pfn);
+
+/*		lru_entry->page = gfn_to_page(kvm, gfn);*/
+		if (lru_entry->page == bad_page || lru_entry->page == NULL) {
+			printk(KERN_ERR "%s : problem getting page for address %llx", 
+				__func__, (uint64_t)addr);
+			kvm_release_page_clean(lru_entry->page);
+			lru_entry->page = NULL;
+			return NULL;
+		}
+		/* gfn_to_hva might be redundant - can do things in better order */
+		if (user)
+			lru_entry->pfn = pfn;
+		else
+			lru_entry->ptr = kmap(lru_entry->page);
+
+		lru_entry->gfn = gfn;
+		lru_entry->flags = user ? VIOMMU_PAGE_CACHE_USER : 0;
+		found_entry = lru_entry;
+	}
+	
+	if (dirty)
+		found_entry->flags |= VIOMMU_PAGE_CACHE_DIRTY;
+
+	found_entry->lru = 0;
+	viommu_update_counter(found?HIT_COUNTER:MISS_COUNTER, 0);
+	return found_entry;
+}
+#endif
+
+
+/* Main translation function */
+inline static
+struct iova_pwc_entry* get_iova_pwc_entry(struct kvm* kvm, uint64_t iova, uint8_t busnr, uint8_t devfn)
+{
+#if IOMMU_PWC
+	struct viommu* viommu = kvm->viommu;
+        uint64_t set = (iova>>level_shift(1)) % VIOMMU_PWC_SETS;
+        struct iova_pwc_entry *iova_pwc_entry = &viommu->iova_pwc[set];
+/*	printk(KERN_ERR "%s : iova %llx  masked %llx  set %llx  pwc_iova  %llx\n", __func__, iova, (iova&(~level_mask(1))), set, iova_pwc_entry->iova);*/
+        if (/*iova_pwc_entry->busnr == busnr && iova_pwc_entry->devfn == devfn &&*/
+                iova_pwc_entry->iova == (iova & (~level_mask(1)))) {
+        } else {
+/*		printk(KERN_ERR "%s : reset!\n", __func__);*/
+                iova_pwc_entry->iova = iova & (~level_mask(1));
+                iova_pwc_entry->busnr = busnr;
+                iova_pwc_entry->devfn = devfn;
+                iova_pwc_entry->ref_cnt = 0;
+        }
+        return iova_pwc_entry;
+#else
+	return NULL;
+#endif
+}
+
+inline static void iommu_pwc_map(struct kvm *kvm, uint64_t iova, uint8_t busnr, uint8_t devfn)
+{
+#if IOMMU_PWC
+	struct viommu* viommu = kvm->viommu;
+        struct iova_pwc_entry *iova_pwc_entry = get_iova_pwc_entry(kvm, iova, busnr, devfn);
+/*	printk(KERN_ERR "%s :...", __func__);*/
+/*
+        if (iova_pwc_entry->busnr != busnr || iova_pwc_entry->devfn != devfn ||
+                iova_pwc_entry->iova != (iova & (~level_mask(2)))) 
+		return;
+ */
+        if (iova_pwc_entry->ref_cnt < 0)
+                iova_pwc_entry->ref_cnt = 0;
+        iova_pwc_entry->ref_cnt++;
+#endif
+}
+
+inline static void iommu_pwc_unmap(struct kvm* kvm, uint64_t iova, uint8_t busnr, uint8_t devfn)
+{
+#if IOMMU_PWC
+	/*struct viommu* viommu = kvm->viommu;*/
+        struct iova_pwc_entry *iova_pwc_entry = get_iova_pwc_entry(kvm, iova, busnr, devfn);
+/*	printk(KERN_ERR "%s :...", __func__);*/
+/*        if (iova_pwc_entry->busnr != busnr || iova_pwc_entry->devfn != devfn ||
+                iova_pwc_entry->iova != (iova & (~level_mask(2)))) 
+		return;
+ */       iova_pwc_entry->ref_cnt--;
+#endif
+}
+
+
+
+inline static void* viommu_gpa_to_hva(struct kvm *kvm, gpa_t addr, int dirty)
+{
+	struct viommu_page_cache_entry* found_entry = 
+		viommu_gpa_to_cache(kvm, addr, dirty, 0);
+	if (unlikely(!found_entry))
+		return NULL;
+	return found_entry->ptr + (addr & (~PAGE_MASK));
+}
+
+inline static pfn_t viommu_gpa_to_pfn(struct kvm *kvm, gpa_t addr)
+{
+	struct viommu_page_cache_entry* found_entry = 
+		viommu_gpa_to_cache(kvm, addr, 1 /*dirty*/, 1);
+
+	if (unlikely(!found_entry))
+		return 0;
+	get_page(found_entry->page);
+	return found_entry->pfn;
+}
+
+static inline int copy_from_guest(struct kvm *kvm, void *buf, gpa_t addr, size_t len)
+{
+/*	struct page* page;
+*/	void* ptr;
+/*	page = gfn_to_page(kvm, addr >> PAGE_SHIFT);
+	if (page == bad_page) {
+		printk(KERN_ERR "%s : problem copy from guest %llx", 
+			__func__, (uint64_t)addr);
+		kvm_release_page_clean(page);
+		return len;
+	}
+	ptr = kmap(page);
+	ptr += (addr & (~PAGE_MASK));
+*/
+	ptr = viommu_gpa_to_hva(kvm, addr, 0);
+	switch (len) {
+		case 1: *(uint8_t*)buf = *(int8_t*)ptr; break;
+		case 2: *(uint16_t*)buf = *(uint16_t*)ptr; break;
+		case 4: *(uint32_t*)buf = *(uint32_t*)ptr; break;
+		case 8: *(uint64_t*)buf = *(uint64_t*)ptr; break;
+		case 16: *(uint64_t*)buf = *(uint64_t*)ptr; 
+			 *(((uint64_t*)buf)+1) = *(((uint64_t*)ptr)+1); break;
+		default:
+			panic("%s unsupported size", __func__);
+	}
+/*	kunmap(page);
+	kvm_release_page_clean(page);
+*/	return 0;
+} 
+
+static inline int copy_to_guest(struct kvm *kvm, void *buf, gpa_t addr, size_t len)
+{
+	void* ptr;
+	ptr = viommu_gpa_to_hva(kvm, addr, 1);
+	switch (len) {
+		case 1: *(uint8_t*)ptr = *(int8_t*)buf; break;
+		case 2: *(uint16_t*)ptr = *(uint16_t*)buf; break;
+		case 4: *(uint32_t*)ptr = *(uint32_t*)buf; break;
+		case 8: *(uint64_t*)ptr = *(uint64_t*)buf; break;
+		case 16: *(uint64_t*)ptr = *(uint64_t*)buf; 
+			 *(((uint64_t*)ptr)+1) = *(((uint64_t*)buf)+1); break;
+		default:
+			panic("%s unsupported size", __func__);
+	} /*
+	clflush_cache_range(ptr, len);
+*/
+	return 0;
+}
+
+
+#if 0
+#define copy_field(target, source, field)						\
+	do {										\
+	 	if (target == NULL) 							\
+			break;								\
+		switch (sizeof(target->field)) {					\
+			case 4: *(volatile uint32_t*)(&(target->field))=*(uint32_t*)(&source->field);	\
+				break;							\
+			case 8: *(volatile uint64_t*)(&(target->field))=*(uint64_t*)(&source->field);	\
+				break;							\
+		}									\
+		mb();									\
+	}										\
+	while (0)
+#endif
+static inline
+void volatile_memcpy(void* target, void* source, size_t length)
+{
+	switch (length) {
+		case 4: *(uint32_t*)target=*(volatile uint32_t*)(source);	
+			break;							
+		case 8: *(uint64_t*)target=*(volatile uint64_t*)(source);
+			break;
+		default:
+			printk(KERN_ERR "%s : unsupported length %x", __func__, (unsigned int)length);
+	}
+}
+
+static inline
+void primitive_memcpy(void* target, void* source, size_t length)
+{
+	switch (length) {
+		case 4: *(uint32_t*)target=*(uint32_t*)(source);	
+			break;							
+		case 8: *(uint64_t*)target=*(uint64_t*)(source);
+			break;
+		default:
+			printk(KERN_ERR "%s : unsupported length %x", __func__, (unsigned int)length);
+	}
+}
+
+static inline int primitive_memcmp(void* val_a, void* val_b, size_t length)
+{
+	switch (length) {
+		case 4: 
+			return *(uint32_t*)val_a != *(uint32_t*)val_b;
+		case 8:
+			return *(uint64_t*)val_a != *(uint64_t*)val_b;
+		default:
+			printk(KERN_ERR "%s : unsupported length %x", 
+				__func__, (unsigned int)length);
+	}
+	return 0;
+}
+
+
+
+/*
+	int i;
+	volatile uint8_t* t = (volatile uint8_t*)target;
+	volatile uint8_t* s = (volatile uint8_t*)source;
+	for (i=0; i<length; i++) {
+		*(t+i)=*(s+i);
+	}*/
+
+static 
+void iommu_reset_regs(struct dmar_regs_region* dmar) {
+	memset(dmar, 0, sizeof(*dmar));
+	dmar->ver.min=1;
+	dmar->cap.sps=0; /* Super-pages disabled */
+	dmar->cap.mgaw = MGAW - 1; 
+	dmar->cap.sagaw = AGAW_30_BIT | AGAW_39_BIT/* |  AGAW_48_BIT */;
+	dmar->cap.nfr = FRCD_REG_NUM;
+	dmar->cap.psi=1;	/* enabling page selective invalidation */
+	dmar->cap.mamv = 63; /* setting mamv to its maximal value */
+	dmar->cap.cm=1;	/* Caching mode is set to track non-present->present */
+	dmar->ecap.pt=1;
+	dmar->ecap.ch=1;
+	dmar->ecap.c=1;/*SIDECORE_MONITORING;*/	/* cache coherency is required for sidecore */
+	/* Assertions to make sure the offsets are a multiply of 16 */
+	BUG_ON((offsetof(struct dmar_regs_region, frr)) % 16 != 0);
+	BUG_ON((offsetof(struct dmar_regs_region, iva)) % 16 != 0);
+	
+	dmar->cap.fro = (offsetof(struct dmar_regs_region, frr)) / 16;
+	dmar->ecap.iro = (offsetof(struct dmar_regs_region, iva)) / 16;
+	dmar->ecap.qi = INVALIDATION_QUEUE;
+}
+
+uint16_t calc_assigned_dev_id(uint8_t busnr, uint8_t devfn)
+{
+ 	return (uint16_t)busnr << 8 | (uint16_t)devfn;
+}
+
+static inline struct devfn_context *iommu_find_assigned_dev(struct kvm* kvm,
+                                 uint8_t g_busnr, uint16_t g_devfn)
+{
+        struct list_head *ptr;
+	struct viommu* viommu = kvm->viommu;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	struct dmar_status *dmar_status = &iommu_state->dmar_status[0];
+ 	struct list_head *head = &dmar_status->devfn_context_head;
+	struct devfn_context *match;
+
+        list_for_each(ptr, head) {
+                match = list_entry(ptr, struct devfn_context, node);
+                if (match->g_busnr == g_busnr && match->g_devfn == g_devfn)
+                        return match;
+	}
+
+        return NULL;
+}
+
+
+inline int is_device_assigned(struct kvm* kvm, uint8_t g_busnr, uint16_t g_devfn)
+{
+/*
+	int assigned_dev_id = calc_assigned_dev_id(g_busnr, g_devfn);
+*/	int res = (iommu_find_assigned_dev(kvm, g_busnr, g_devfn) != NULL);
+	return res;
+}
+
+static void viommu_map_frames(struct kvm *kvm, struct devfn_context *devfn_context,
+		gpa_t iova, gpa_t gpa, gpa_t size, uint32_t flags)
+{
+	struct viommu* viommu = kvm->viommu;
+	struct iova_entry* cached_iova = &viommu->cached_iova;
+	uint64_t offset, hpa, new_hpa;
+	pfn_t pfn;
+	viommu_update_counter(MAP_COUNTER, 1);
+        for (offset=0; offset<size; offset+=PAGE_SIZE) {
+		int mapped;
+		/* first check if it's already mapped */
+		viommu_update_counter(FIND_IOVA_COUNTER,1);
+		if (cached_iova->valid && cached_iova->iommu_domain == &devfn_context->iommu_domain 
+			&& cached_iova->iova == iova) {
+			hpa = cached_iova->hpa;
+			mapped = cached_iova->mapped;
+		} else {
+	                hpa  = iommu_iova_to_phys(&devfn_context->iommu_domain, iova+offset, &mapped);
+		}
+/*		printk(KERN_ERR "%s : hpa=%lx  iova=%lx  mapped=%d", __func__, hpa, iova, mapped); 
+*/
+		viommu_update_counter(FIND_IOVA_COUNTER,0);
+                /* Pinning the new one and finding its address */
+		pfn = viommu_gpa_to_pfn(kvm, gpa+offset);
+                new_hpa = pfn << PAGE_SHIFT;
+/*		printk(KERN_ERR "%s : gpa+offset %llx  hva  %llx new_hpa %llx  = pfn %lx\n", __func__, gpa+offset, hva, new_hpa, (gfn_to_pfn(kvm, (gpa+offset)>>PAGE_SHIFT))<<PAGE_SHIFT);
+*/
+			/*gfn_to_pfn(kvm, (gpa+offset) >> PAGE_SHIFT) << PAGE_SHIFT;*/
+                if (mapped) {
+               		iommu_unmap(&devfn_context->iommu_domain, iova+offset, PAGE_SIZE);
+                		kvm_release_pfn_dirty((hpa+offset) >> PAGE_SHIFT);
+                }
+                if (hpa == new_hpa && new_hpa != 0) {
+                	printk(KERN_ERR "#### Releasing once if same hpa %llx\n", new_hpa);
+                }
+                else if (new_hpa != 0) {
+/*			if (!mapped)*/
+				iommu_pwc_map(kvm, iova+offset, devfn_context->g_busnr, devfn_context->g_devfn);
+/*			else printk(KERN_ERR "%s : already mapped :(");*/
+                	iommu_map(&devfn_context->iommu_domain, iova+offset,
+                		new_hpa, PAGE_SIZE, flags);
+                }
+             	else
+             	   	printk(KERN_ERR "%s:"
+                      	"cannot map iova= %llx flags=%d  \n ", __func__, iova, flags);
+    	}
+	cached_iova->iova = iova - PAGE_SIZE;
+	cached_iova->iommu_domain = &devfn_context->iommu_domain;
+	viommu_update_counter(MAP_COUNTER,0);
+}
+
+static void viommu_unmap_frames(struct kvm *kvm, struct devfn_context* devfn_context,
+	gpa_t iova, gpa_t size)
+{
+	u64 hpa, offset;
+    	int mapped;
+	BUG_ON(!devfn_context);
+	BUG_ON(!kvm);
+/*	struct viommu* viommu = kvm->viommu;*/
+    	if (((1ULL)<< devfn_context->iommu_width) < iova)
+       		return;
+	viommu_update_counter(UNMAP_COUNTER,1);
+       
+        if (((1ULL)<<devfn_context->iommu_width) < iova+size) {
+        	size = (1ULL<<devfn_context->iommu_width) - iova;
+        }
+       
+	for (offset=0; offset<size; offset+=VTD_PAGE_SIZE) {
+        	hpa = iommu_iova_to_phys(&devfn_context->iommu_domain, iova+offset, &mapped);
+        	iommu_unmap(&devfn_context->iommu_domain, iova+offset, VTD_PAGE_SIZE);
+        	if (mapped) {
+               		/* need to unmap in chunks according to size */
+	            	/* TODO: Perhaps release only according to dirty state */
+			iommu_pwc_unmap(kvm, iova+offset, devfn_context->g_busnr,
+				 devfn_context->g_devfn);
+               		kvm_release_pfn_dirty(hpa >> PAGE_SHIFT);
+                }
+        }
+	viommu_update_counter(UNMAP_COUNTER,0);
+}
+
+static void viommu_clear_all(struct kvm *kvm)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	struct dmar_status* dmar_status = &iommu_state->dmar_status[0];
+        struct list_head *ptr;
+	struct devfn_context *devfn_context;
+ 	struct list_head *head = &dmar_status->devfn_context_head;
+
+        list_for_each(ptr, head) {
+                devfn_context = list_entry(ptr, struct devfn_context, node);
+		printk(KERN_ERR "%s : clear all, %d %d\n", __func__, devfn_context->h_busnr, devfn_context->h_devfn);
+		viommu_unmap_frames(kvm, devfn_context,	0, ((1ULL)<<MGAW)-1);
+	}
+}
+
+
+#if 0
+int iommu_guest_cmd(struct kvm *kvm, struct kvm_iommu_ops* ops)
+{
+        int i, r=0;
+	struct devfn_context *match;
+        struct kvm_assigned_dev_kernel *match;
+/*printk("%s : entered with %d\n", __func__, ops->ops_num);     */
+	int lock = 0;
+	if (kvm) {
+/*	        down_read(&kvm->slots_lock);
+       		mutex_lock(&kvm->lock);
+*/		lock = 1;
+	}
+
+
+        for (i = 0; i < ops->ops_num; i++) {
+                struct kvm_iommu_single_op* op = &(ops->ops[i]);
+                struct iommu_domain* domain;
+                uint32_t width;
+                u64 size=0;
+                match = iommu_find_assigned_dev(&kvm->arch.assigned_dev_head,
+                                      op->g_busnr, op->g_devfn);
+                if (!match) {
+                        printk(KERN_ERR "%s: Device is not assigned %d\n",
+                                 __func__, op->g_busnr, op->g_devfn);
+                        continue;
+                }
+                domain = match->iommu_domain;
+                if (domain->iommu_flags & KVM_IOMMU_CACHE_COHERENCY)
+                        op->flags |= IOMMU_CACHE;
+                width = iommu_domain_has_cap(domain, IOMMU_CAP_ADDRESS_WIDTH);
+                switch (op->cmd) {
+                case IOMMU_MAP_FRAME: {
+                        u64 offset, hpa, new_hpa;
+                        for (offset=0; offset<op->size; offset+=PAGE_SIZE) {
+                                int mapped, temp_r = 0;
+
+                                /* first check if it's already mapped */
+                                hpa  = iommu_iova_to_phys(domain, op->iova+offset, &mapped);
+                                /* Pinning the new one and finding its address */
+                                new_hpa = gfn_to_pfn(kvm, (op->gpa+offset) >> PAGE_SHIFT) << PAGE_SHIFT;
+                                if (mapped) {
+                                        iommu_unmap_range(domain, op->iova+offset, PAGE_SIZE);
+                                        kvm_release_pfn_dirty((hpa+offset) >> PAGE_SHIFT);
+                                }
+                                if (hpa == new_hpa && new_hpa != 0) {
+                                        printk(KERN_ERR "#### Releasing once if same hpa %llx\n", new_hpa);
+                                }
+                                else if (new_hpa != 0) {
+                                        temp_r = iommu_map_range(domain, op->iova+offset,
+                                                 new_hpa, PAGE_SIZE, op->flags);
+                                        if (temp_r)
+                                                r = temp_r;
+                                }
+                                else
+                                printk(KERN_ERR "%s:"
+                                         "cannot map iova= %llx flags=%d  %d\n ", __func__, op->iova, op->flags, r);
+                        }
+                 break;
+                 }
+                case IOMMU_UNMAP_FRAME: {
+                        u64 hpa, offset;
+                        int mapped;
+                        r = 0;
+                        if (((1ULL)<< width) < op->iova)
+                                break;
+                        size = op->size;
+                        if (((1ULL)<<width) < op->iova+op->size) {
+                                size = (1ULL<<width) - op->iova;
+                        }
+                        for (offset=0; offset<size; offset+=PAGE_SIZE) {
+                                hpa  = iommu_iova_to_phys(domain, op->iova+offset, &mapped);
+                                if (!mapped) {
+                                        continue;
+                                }
+
+                                /* need to unmap in chunks according to size */
+                                iommu_unmap_range(domain, op->iova, size);
+                                /* TODO: Perhaps release only according to dirty state */
+                                kvm_release_pfn_dirty(hpa >> PAGE_SHIFT);
+                        }
+                        break;
+               	 }
+		}
+        }
+        if (lock) {
+		mutex_unlock(&kvm->lock);
+        	/*up_read(&kvm->slots_lock);*/
+	}
+        return r;
+}
+#endif
+
+static void clear_pending_unmap(struct kvm *kvm)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct pending_unmap *pending_unmap = &viommu->pending_unmap;
+	if (pending_unmap->size > 0)
+		viommu_unmap_frames(kvm, pending_unmap->devfn_context, 
+			pending_unmap->iova, pending_unmap->size);
+	pending_unmap->size = 0;
+}
+
+
+
+static
+void intel_iommu_map_shadow(struct kvm *kvm,
+	struct devfn_context* devfn_context, 
+	gpa_t start_iova, gpa_t end_iova)
+{
+	struct viommu *viommu = kvm->viommu;
+	uint8_t access_perm;
+	gpa_t iova = start_iova;
+	gpa_t iova_unmap_start = 0;
+	int pending_unmap = 0;
+
+	if (viommu->deferred && IOMMU_DEFERRED_UNMAP)
+		clear_pending_unmap(kvm);
+
+	while (iova < end_iova)	{
+		uint64_t size;
+		uint64_t gpa;
+		viommu_update_counter(TRANSLATE_COUNTER, 1);
+		gpa =  __iommu_phy_addr_translate(kvm,
+			iova, 
+			&access_perm, 
+			devfn_context->g_busnr,
+			devfn_context->g_devfn, &size, 1); 
+		viommu_update_counter(TRANSLATE_COUNTER, 0);
+		if (access_perm) {
+			if (pending_unmap) {
+				viommu_unmap_frames(kvm, devfn_context, 
+					iova_unmap_start, 
+					(iova - iova_unmap_start));
+				pending_unmap = 0;
+			}
+			viommu_map_frames(kvm, devfn_context,
+				iova, gpa, size, access_perm);
+		}
+		else {
+			if (!pending_unmap) {
+				iova_unmap_start = iova;
+				pending_unmap = 1;
+			}
+		} 
+		iova += size;
+	}
+
+	if (pending_unmap) {
+		if (IOMMU_DEFERRED_UNMAP && viommu->deferred) {
+			struct viommu* viommu = kvm->viommu;
+			struct pending_unmap *pending_unmap = &viommu->pending_unmap;
+			pending_unmap->devfn_context = devfn_context;
+			pending_unmap->size = (iova - iova_unmap_start);
+			pending_unmap->iova = iova_unmap_start;
+		} else {
+			viommu_unmap_frames(kvm, devfn_context, iova_unmap_start, 
+				(iova - iova_unmap_start));
+		}
+	}
+}
+
+void intel_iommu_add_dev(struct kvm *kvm,
+	struct devfn_mapping *mapping)
+{
+	struct devfn_context* devfn_context = kmalloc(sizeof(struct devfn_context), GFP_KERNEL);
+	struct viommu* viommu = kvm->viommu;
+	struct iommu_state* iommu_state = &viommu->iommu_state;
+	struct dmar_status *dmar_status = &iommu_state->dmar_status[0];
+	struct kvm_assigned_dev_kernel *match;
+	BUG_ON(devfn_context == NULL);
+	devfn_context->g_busnr = mapping->g_busnr;
+	devfn_context->g_devfn = mapping->g_devfn;
+	devfn_context->h_busnr = mapping->h_busnr;
+	devfn_context->h_devfn = mapping->h_devfn;
+	devfn_context->context = dmar_status->ce[devfn_context->g_devfn];
+     
+	match = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,
+        	calc_assigned_dev_id(mapping->h_busnr, mapping->h_devfn));
+      	if (!match) {
+      		panic("%s: Device is not assigned %x %x\n",
+                	__func__, devfn_context->g_busnr, devfn_context->g_devfn);
+       	}
+        iommu_get_iommu_domain_for_dev(&devfn_context->iommu_domain, match->dev);
+	devfn_context->iommu_width =
+		 iommu_domain_has_cap(&devfn_context->iommu_domain, 
+			IOMMU_CAP_ADDRESS_WIDTH);
+	iommu_vdomain_init(&devfn_context->iommu_domain);
+	list_add(&devfn_context->node, &dmar_status->devfn_context_head);
+	printk(KERN_ERR "%s : device added", __func__);
+	viommu_clear_all(kvm);
+}
+
+static
+void intel_iommu_iotlb_invd(struct kvm *kvm,
+	struct dmar_status* dmar_status,
+	uint32_t iirg, uint32_t did, uint64_t addr, uint32_t am)
+{
+	uint64_t pages;
+	struct devfn_context* devfn_context;
+	struct list_head* cur;
+
+	switch(iirg)
+	{
+	case IIRG_GLOBAL_INVALIDATION:
+#ifdef ENABLE_GLOBAL_INVALIDATION
+		devfn_context = QLIST_FIRST(&dmar_status->devfn_context_head);
+		while (devfn_context) {
+			intel_iommu_map_shadow(kvm, devfn_context, 0, (1ULL)<<MGAW);			
+			devfn_context = QLIST_NEXT(devfn_context, node);
+		}
+#endif
+		break;
+	case IIRG_DOMAIN_SELECTIVE_INVALIDATION: {
+#ifdef ENABLE_GLOBAL_INVALIDATION
+			devfn_context = NULL;
+			list_for_each(cur, dmar_status->devfn_context) {
+				devfn_context = list_entry(cur, struct devfn_context, node);
+				if (devfn_context->context.did != did)
+					continue;
+				intel_iommu_map_shadow(kvm, devfn_context, 
+					0, (1ULL)<<MGAW);
+			}
+#endif
+		break;
+		}
+	case IIRG_DOMAIN_PAGE_SELECTIVE_INVALIDATION: {
+			gfn_t start_addr = addr << VTD_PAGE_SHIFT;
+			gfn_t end_addr;
+			pages = ((uint64_t)1)<<am;
+			end_addr = start_addr + (pages<<VTD_PAGE_SHIFT);
+			list_for_each(cur, &dmar_status->devfn_context_head) {
+				devfn_context = list_entry(cur, struct devfn_context, node);
+				if (devfn_context->context.did != did)
+					continue;
+				if (!is_device_assigned(kvm, devfn_context->g_busnr,
+					devfn_context->g_devfn))
+					continue;
+				intel_iommu_map_shadow(kvm, devfn_context, 
+					start_addr, end_addr);
+			}
+			break;
+		}
+	default:
+		printk( KERN_ERR "%s : guest requres reserved tlb invalidation\n", __func__);
+	}
+}
+
+#if 0
+static void sync_dmar(struct dmar_status* dmar_status)
+{
+/*        if (copy_to_user(dmar_status->phy_dmar_regs, &dmar_status->dmar_regs,
+		sizeof(dmar_status->dmar_regs))) {
+		panic("%s Problem syncing dmar regs %p", __func__, dmar_status->phy_dmar_regs);
+	}
+*/
+	memcpy((void*)dmar_status->phy_dmar_regs, 
+		&(dmar_status->dmar_regs), sizeof(dmar_status->dmar_regs));
+	clflush_cache_range(dmar_status->phy_dmar_regs, sizeof(*dmar_status->phy_dmar_regs));
+	mb();
+}
+#endif
+
+static void
+intel_iommu_write_status_update(struct kvm *kvm,
+	struct dmar_status* dmar_status, gpa_t addr)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct iommu_state* iommu_state = &viommu->iommu_state;
+	int offset = addr & 0xFFF & ~3;
+	/* volatile */ struct dmar_regs_region* dmar_regs = &(dmar_status->dmar_regs);
+	volatile struct dmar_regs_region* shadow_regs = dmar_status->phy_dmar_regs;
+
+	switch (offset)
+	{
+	case offsetof(struct dmar_regs_region, gcmd):	{
+		/* first we will update gcmd, and only then gsts */
+
+		dmar_regs->gsts.fls = dmar_regs->gcmd.sfl;		/* set fault log */
+		dmar_regs->gsts.rtps |= dmar_regs->gcmd.srtp;	/* set root table pointer */
+		dmar_regs->gsts.wbfs = dmar_regs->gcmd.wbf;		/* write buffer flush */
+		dmar_regs->gsts.tes = dmar_regs->gcmd.te;		/* translation enable */
+		dmar_regs->gcmd.srtp = 0; /* to ease detection of setting of the bit */
+		if (iommu_state->invalidation_queue && dmar_regs->gsts.qies == 0 &&
+			dmar_regs->gcmd.qie == 1) {
+			viommu_load_iq(kvm, dmar_status);
+			shadow_regs->gsts.qies = dmar_regs->gsts.qies = 1;
+		}
+		/*copy_field(shadow_regs, dmar_regs, gcmd);
+		copy_field(shadow_regs, dmar_regs, gsts);*/
+		shadow_regs->gcmd = dmar_regs->gcmd;
+		shadow_regs->gsts = dmar_regs->gsts;
+		break;
+		}
+	case offsetof(struct dmar_regs_region, ccmd):
+		if (dmar_regs->ccmd.icc)
+			intel_iommu_ccmd_invd(kvm, dmar_status);
+		dmar_regs->ccmd.caig = dmar_regs->ccmd.cirg;		
+		dmar_regs->ccmd.icc = 0;	/* Clearing the invalidate context cache to "emulate" invalidation */
+/*		copy_field(shadow_regs, dmar_regs, ccmd);
+*/
+		shadow_regs->ccmd = dmar_regs->ccmd;
+		break;
+	case offsetof(struct dmar_regs_region, pmen):
+		dmar_regs->pmen.prs = dmar_regs->pmen.epm;		/* enable memory protection */
+/*		copy_field(shadow_regs, dmar_regs, pmen);
+*/
+		shadow_regs->pmen = dmar_regs->pmen;
+		break;
+	case offsetof(struct dmar_regs_region, iotlb):		/* this one is actually unnecassary */
+		if (dmar_regs->iotlb.ivt == 1 /*&& 
+			(dmar_regs->iotlb.iirg != 3 || 
+			(dmar_regs->iva.res == 0 && dmar_regs->iotlb.res1 == 0) )*/) {
+			struct iotlb_reg* iotlb = &(dmar_regs->iotlb); 
+			struct iva_reg* iva = &(dmar_regs->iva);
+			*iva = shadow_regs->iva;
+			intel_iommu_iotlb_invd(kvm, dmar_status, iotlb->iirg,
+				iotlb->did, iva->addr, iva->am);
+			iva->addr = 0;
+			iva->am = 0;
+			iva->ih = 0;
+			/*dmar_regs->iva.res = 1;*/
+/*			copy_field(shadow_regs, dmar_regs, iva);*/
+			shadow_regs->iva = dmar_regs->iva;
+			/*dmar_regs->iotlb.res1 = 1;*/
+			dmar_regs->iotlb.iaig = dmar_regs->iotlb.iirg; 	/* actual granularity matches requested */
+			shadow_regs->iotlb = dmar_regs->iotlb;
+			/*copy_field(shadow_regs, dmar_regs, iotlb);
+			*/dmar_regs->iotlb.ivt = 0;						/* marking invalidation is over */
+			shadow_regs->iotlb = dmar_regs->iotlb;
+			/*copy_field(shadow_regs, dmar_regs, iotlb);
+			*/#ifdef IOTLB_LOG
+			log_flush(dmar_status);
+			#endif
+		}
+		break;
+	case offsetof(struct dmar_regs_region, iqt):
+		if (iommu_state->invalidation_queue && 
+			dmar_regs->gsts.qies == 1) 
+			intel_iommu_process_iq(kvm, dmar_status);
+		break;
+	default:
+		;
+	}
+	
+}
+
+void viommu_perform_write(struct kvm *kvm, struct viommu_write* viommu_write)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	struct dmar_status *dmar_status = &iommu_state->dmar_status[0];
+/*	int offset = viommu_write->offset & 0xFFF */ /*& ~3;*/
+	void* shadow_regs = dmar_status->phy_dmar_regs;
+	void* dmar_regs = &(dmar_status->dmar_regs);
+	*(uint32_t*)(shadow_regs + viommu_write->offset) = viommu_write->value;
+	*(uint32_t*)(dmar_regs + viommu_write->offset) = viommu_write->value;
+	intel_iommu_write_status_update(kvm, dmar_status, viommu_write->offset);
+}
+
+/*
+Software is expected to access 32-bit registers as aligned doublewords. For example, to modify a
+field (e.g., bit or byte) in a 32-bit register, the entire doubleword is read, the appropriate field(s)
+are modified, and the entire doubleword is written back.
+*/
+
+static void iommu_clear_caches(struct dmar_status* dmar_status)
+{
+	memset(&dmar_status->re, 0, sizeof(dmar_status->re));
+	memset(dmar_status->ce, 0, IOMMU_CONTEXT_ENTRIES *
+		sizeof(struct IommuContextEntry));
+}
+
+static 
+void iommu_reset(struct viommu *viommu)
+{
+	int i;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	for (i=0; i<DMAR_DEVICES_NUM; i++)
+	{
+		struct dmar_status *dmar_status = &iommu_state->dmar_status[i];
+		struct dmar_regs_region *phy_dmar_regs = 
+			dmar_status->phy_dmar_regs;
+		struct dmar_regs_region *dmar_regs = &dmar_status->dmar_regs;
+		iommu_clear_caches(dmar_status);
+		iommu_reset_regs(dmar_regs);
+		iommu_reset_regs(phy_dmar_regs);
+		/*sync_dmar(dmar_status);*/
+		/*iommu_reset_regs(iommu_state->dmar_status[i].phy_dmar_regs);
+		*/printk(KERN_ERR "%s: Finished resetting regs\n", __func__); 
+   		INIT_LIST_HEAD(&(iommu_state->dmar_status[i].devfn_context_head));
+	}
+
+}
+
+#if 0
+static
+void iommu_mmio_map(struct iommu_state* iommu_state) {
+    gfn_t addr = DMAR_REG_BASE;
+    gfn_t regs_size = DMAR_REGS_PAGE * DMAR_DEVICES_NUM;
+    
+	/* allocating actual memory for the regs */
+	int ret = 0;
+	int i=0;
+	if ((iommu_state->mfd = open("/dev/viommu", O_RDWR)) < 0) {
+		pr_debug("open /dev/viommu failed");
+		exit(1);
+
+
+	/*iommu_state->pregs = malloc(VTD_PAGE_SIZE);*/
+	iommu_state->pregs = mmap(NULL, VTD_PAGE_SIZE, PROT_WRITE | PROT_READ,
+		 /*MAP_ANONYMOUS | MAP_PRIVATE */ MAP_SHARED, iommu_state->mfd, (off_t)0);
+	printk (KERN_ERR "%s: pregs = %p mfd = %d addr = %lx\n", __func__, iommu_state->pregs, iommu_state->mfd, addr);
+	for (i=0 ; i < 1000; i++) {	
+		if ((*(((uint8_t*)(iommu_state->pregs)) + i)) != 0	) {
+			panic("error, not zero! %d\n", i);
+		}
+	}
+	ret = kvm_register_phys_mem(kvm_context, addr, (struct dmar_regs_region*)iommu_state->pregs, VTD_PAGE_SIZE, 0); 
+	ret = kvm_vm_ioctl(kvm_state, KVM_GET_UNCACHABLE, NULL);
+	
+	if (ret != 0) {
+		panic("%s : Error create new mapping failed \n", __func__);
+	}
+    }
+}
+#endif
+
+#if 0
+void intel_iommu_init(void) {
+    struct iommu_state *d = &iommu_state;
+    d->invalidation_queue = INVALIDATION_QUEUE;
+    /*iommu_mmio_map(d);*/
+    /*register_savevm(info_str, -1, 1, iommu_save, iommu_load, d);
+    qemu_register_reset(iommu_reset, d);*/
+    spin_lock_init(&d->dmar_status[0].access_lock);
+    iommu_reset(d);
+
+    create_iommu_monitoring_thread(d->kvm, d);
+}
+#endif
+
+/* Main translation function */
+static
+uint64_t __iommu_phy_addr_translate(struct kvm *kvm, gpa_t addr,
+                            uint8_t* access_perm, uint8_t busnr, uint8_t devfn, uint64_t* size, int sidecore)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	struct dmar_status *dmar_status = &iommu_state->dmar_status[0];
+	struct IommuRootEntry re = dmar_status->re;
+	struct IommuContextEntry ce = dmar_status->ce[devfn];
+	int level;
+	uint64_t pte_addr;
+	/*volatile*/ struct dmar_regs_region* dmar_regs = &(iommu_state->dmar_status[0].dmar_regs);
+	uint64_t temp, addr_offset;
+	struct iova_pwc_entry* iova_pwc_entry;
+
+/*	printk(KERN_ERR "%s :...", __func__);*/
+	iova_pwc_entry = sidecore ? get_iova_pwc_entry(kvm, addr, busnr, devfn) : NULL;
+/*
+	if (iova_pwc_entry)
+		printk(KERN_ERR "%s : addr %llx ref_cnt %x\n", __func__, addr, iova_pwc_entry->ref_cnt);
+ */
+       if (iova_pwc_entry && iova_pwc_entry->ref_cnt > 0) {
+                level = 1;
+                pte_addr = iova_pwc_entry->last_level_pte;
+                *access_perm = iova_pwc_entry->perm;
+	} else {
+		if (!size)
+			size = &temp;
+		*access_perm = IOMMU_READ | IOMMU_WRITE; 
+		/* Reading the root entry */
+		if (!dmar_regs->gsts.tes) {
+			return addr;				/* Translation is disabled */
+		}
+
+		if (unlikely(!dmar_regs->gsts.rtps)) {
+			*access_perm = 0;
+			*size = ((1ULL) << MGAW) - addr;
+			return -1; 
+		}
+		if (unlikely(!re.p)) {
+			*access_perm = 0;
+			*size = ((1ULL) << MGAW) - addr;
+			return -1;
+		}
+
+		/* Reading the relevant context entry */
+		if (unlikely(!ce.p)) {
+			*access_perm = 0;
+			*size = ((1ULL) << MGAW) - addr;
+			return -1;
+		}
+
+		if (unlikely(ce.t == TRANSLATE_RESERVED)) {
+			*access_perm = 0;
+			*size = ((1ULL) << MGAW) - addr;
+			return -1;
+		}
+		if (unlikely(ce.t == TRANSLATE_PASS_THROUGH)) {
+			*size = ((1ULL) << MGAW) - addr;
+			return addr;
+		}
+
+		level = aw_to_levels(ce.aw);		/* Analyzing the levels number from the context */
+
+		pte_addr = ce.asr << DMAR_REGS_PAGE_SHIFT;
+	}
+
+	while (level>0 && *access_perm != 0) {
+/*		struct IommuPageTableEntry* pte_ptr;
+*/		struct IommuPageTableEntry pte;
+		pte_addr += level_offset(addr, level-1)*sizeof(struct IommuPageTableEntry);
+/*		pte_ptr = (struct IommuPageTableEntry *)gfn_to_hva(kvm, 
+			pte_addr >> VTD_PAGE_SHIFT);
+*/	
+/*		if (kvm_is_error_hva((unsigned long)pte_ptr)) {
+			printk(KERN_ERR "%s: problem mapping pte ptr\n", __func__);
+		}
+*/		
+		if (unlikely(copy_from_guest(kvm, &pte, pte_addr, sizeof(pte)))) {
+			printk (KERN_ERR "%s : problem reading pte", __func__);
+			*access_perm = 0;
+			return 0;
+		}
+		pte_addr = pte.addr;
+		pte_addr <<= VTD_PAGE_SHIFT;
+		if (!pte.r) 
+			(*access_perm) &= (~IOMMU_READ);
+		if (!pte.w) 
+			(*access_perm) &= (~IOMMU_WRITE);
+		if (pte.sp)
+			break;		/* Super page */
+		level--;
+#if IOMMU_PWC
+                if (iova_pwc_entry && level==1) {
+                        iova_pwc_entry->last_level_pte = pte_addr;
+                        iova_pwc_entry->perm = *access_perm;
+                }
+#endif
+	}
+	addr_offset = addr&level_mask(level);
+	*size = level_size(level) - addr_offset;
+	/* shift left also if super-page */
+	return (pte_addr<<(LEVEL_STRIDE*level))+(addr&level_mask(level));	
+}
+
+uint64_t iommu_phy_addr_translate(struct kvm* kvm, uint64_t addr,
+                            int is_write, uint8_t g_busnr, uint8_t g_devfn, int* err)
+{
+	uint8_t access_perm;
+	uint16_t req_access_perm = is_write ? IOMMU_WRITE : IOMMU_READ;
+	uint64_t phy_addr;
+	phy_addr = __iommu_phy_addr_translate(kvm, addr, &access_perm,  
+		g_busnr, g_devfn, NULL, 0);
+	if (err && (req_access_perm & access_perm)==0) {
+		printk (KERN_ERR "%s : access problem addr %llx", __func__, addr);
+		*err = -1;
+	}
+
+	return phy_addr;	
+}
+
+static inline
+uint8_t get_devfn(uint16_t sid) 
+{
+	return (sid & 0xFF); 
+}
+
+static inline
+uint8_t get_busnr(uint16_t sid) 
+{
+	return (sid>>8)&0xFF;
+}
+
+static inline
+struct dmar_regs_region* get_dmar_regs(struct dmar_status* dmar_status, 
+	uint16_t dmar_unit)
+{
+	return &(dmar_status->dmar_regs);
+}
+
+static 
+int intel_iommu_analyze_cirg(uint32_t cirg, uint32_t fm, 
+	uint8_t* invalidate_all, uint8_t* start_fm_bit, 
+	uint8_t* fm_bits_num)
+{
+	*start_fm_bit = 0;
+	*fm_bits_num = 0;
+	*invalidate_all = 0;
+	printk(KERN_ERR "%s\n", __func__);
+	switch (cirg)
+	{
+		case CIRG_GLOBAL_INVALIDATION:
+		case CIRG_DOMAIN_SELECTIVE_INVALIDATION:
+			*invalidate_all = 1;
+			break;
+		case CIRG_DEVICE_SELECTIVE_INVALIDATION:
+			*invalidate_all = 0;
+			switch (fm)
+			{
+				case 0: *start_fm_bit=1; *fm_bits_num=0; break;
+				case 1: *start_fm_bit=2; *fm_bits_num=1; break;
+				case 2: *start_fm_bit=1; *fm_bits_num=2; break;
+				case 3: *start_fm_bit=0; *fm_bits_num=3; break;
+			}
+			break;
+		default:
+			pr_debug("invalid cirg command");
+			return -1;
+	}
+	return 0;
+}
+
+static void intel_iommu_clear_context_cache(struct kvm *kvm,
+	struct dmar_status* dmar_status)
+{
+	int i;
+	/*struct viommu* viommu = kvm->viommu;*/
+	struct dmar_regs_region *phy_dmar_regs = 
+		dmar_status->phy_dmar_regs;
+/*	struct IommuRootEntry *re = 
+		(struct IommuRootEntry*)gfn_to_hva(kvm, dmar_regs[0].readdr >> VTD_PAGE_SHIFT);
+	struct IommuContextEntry *ce;
+*/
+/*	if (kvm_is_error_hva((unsigned long)re)) {
+		printk(KERN_ERR "%s: problem mapping pte ptr\n", __func__);
+		return;
+	}
+*/
+	if (!phy_dmar_regs->gcmd.te)
+		return; /* Translation is disabled */
+	if (copy_from_guest(kvm, &dmar_status->re, phy_dmar_regs[0].readdr, sizeof(dmar_status->re))) {
+		printk(KERN_ERR "%s : problem reading root entry\n", __func__);
+	}
+/*
+	copy_from_user(&dmar_status->re, re, sizeof(struct IommuRootEntry));
+*//*	dmar_status->re = *re;*/
+/*	ce = (struct IommuContextEntry*)gfn_to_hva(kvm,
+		dmar_status->re.ctp<<VTD_PAGE_SHIFT);
+*/
+	for (i = 0; i < IOMMU_CONTEXT_ENTRIES; i++) {
+		int r = 0;
+		if (!dmar_status->re.p) {
+			dmar_status->ce[i].p = 0;
+			continue;
+		}
+			
+/*		copy_from_user(&ce[i], 
+			&dmar_status->ce[i], sizeof(struct IommuContextEntry));
+*/
+		r = copy_from_guest(kvm, &dmar_status->ce[i], 
+			(dmar_status->re.ctp<<VTD_PAGE_SHIFT) + i * sizeof(struct IommuContextEntry),
+			sizeof(struct IommuContextEntry));
+		if (r) {
+			printk(KERN_ERR "%s : problem reading context entry\n", __func__);
+		}	
+/*		dmar_status->ce[i] = ce[i];*/
+	}
+}
+
+static
+void intel_iommu_ccmd_invd(struct kvm *kvm,
+	struct dmar_status* dmar_status)
+{
+	struct dmar_regs_region* dmar_regs = 
+		&(dmar_status->dmar_regs);
+	struct ccmd_reg* ccmd = &(dmar_regs->ccmd);
+
+	printk(KERN_ERR "%s\n", __func__);
+	/* Currently always performing global context invalidation */
+	intel_iommu_clear_context_cache(kvm, dmar_status);
+	intel_iommu_update_devfn_context(kvm, dmar_status, 
+		ccmd->sid, ccmd->cirg, ccmd->fm);
+}
+
+static
+void intel_iommu_iq_context_invd(struct kvm *kvm,
+	struct dmar_status* dmar_status,
+	struct context_cache_invd_desc* desc)
+{
+	printk(KERN_ERR "%s\n", __func__);
+
+	intel_iommu_clear_context_cache(kvm, dmar_status);
+	intel_iommu_update_devfn_context(kvm, dmar_status, 
+		desc->sid, desc->g, desc->fm);
+}
+
+static
+void intel_iommu_iq_wait(struct kvm* kvm,
+	struct dmar_status* dmar_status,
+	struct inv_wait_desc* desc)
+{
+	uint32_t data;
+	if (desc->iflag)
+		pr_debug("interrupts are not supported");
+	if (desc->sw) {
+/*		uint32_t* data_target = NULL;
+		if (kvm_is_error_hva((unsigned long)data_target)) {
+			printk(KERN_ERR "%s: problem mapping pte ptr\n", __func__);
+			return;
+		}
+*/		data = desc->stat_data;
+		copy_to_guest(kvm, &data, desc->stat_addr << 2, sizeof(data));
+	}
+}
+
+static
+void intel_iommu_iq_iotlb_invd(struct kvm *kvm,
+	struct dmar_status* dmar_status,
+	struct iotlb_invd_desc* desc) 
+{
+	intel_iommu_iotlb_invd(kvm, dmar_status,
+		desc->g, desc->did, desc->addr, desc->am);
+}
+
+static
+int iommu_process_iq_desc (struct kvm *kvm,
+	struct iommu_generic_desc* desc, 
+	struct dmar_status* dmar_status) 
+{
+	int r = 0;
+	switch (desc->id)
+	{
+	case IQ_CONTEXT_INVD_DESC_ID: {
+		struct context_cache_invd_desc* context_cache_invd_desc = 
+			(struct context_cache_invd_desc*)desc;
+		intel_iommu_iq_context_invd(kvm, dmar_status,
+			context_cache_invd_desc);
+		break;
+		}
+	case IQ_IOTLB_INVD_DESC_ID: {
+		struct iotlb_invd_desc* iotlb_invd_desc = 
+			(struct iotlb_invd_desc*)desc;
+		intel_iommu_iq_iotlb_invd(kvm, dmar_status, iotlb_invd_desc);
+		break;	
+		}
+	case IQ_INVD_WAIT_DESC_ID: {
+		struct inv_wait_desc* wait_desc = 
+			(struct inv_wait_desc*)desc; 
+		intel_iommu_iq_wait(kvm, dmar_status, wait_desc);
+		break;
+		}
+	case IQ_DEV_IOTLB_INVD_DESC_ID: {
+		pr_debug("IQ_DEV_IOTLB_INVD_DESC_ID is not implemented");
+		break;
+		}
+	case IQ_INT_CACHE_INV_DESC_ID: {
+		pr_debug("IQ_INT_CACHE_INV_DESC_ID is not implemented");
+		break;
+		}
+	default:
+		r = -1;
+		pr_debug("invalid descriptor id %x", (uint32_t)desc->id);
+	}
+	return r;
+}
+
+static void viommu_load_iq(struct kvm* kvm, struct dmar_status *dmar_status)
+{
+	struct page **iq_page = &dmar_status->iq_page;
+	struct iommu_generic_desc **iq = &dmar_status->iq; 
+	struct dmar_regs_region* dmar_regs = 
+		&(dmar_status->dmar_regs);
+	struct dmar_regs_region* phy_dmar_regs = 
+		dmar_status->phy_dmar_regs;
+	volatile struct iqa_reg* actual_iqa = &phy_dmar_regs->iqa;
+	struct iqa_reg* iqa = &dmar_regs->iqa;
+	struct iqa_reg old_iqa = *iqa;
+	*iqa = *actual_iqa;
+	printk(KERN_ERR "%s : loading new iq page %llx %llx\n", __func__, *((uint64_t*)(iqa)), 
+		*(((uint64_t*)(&old_iqa))));
+	if (iqa->iqa == 0) {
+		printk(KERN_ERR "%s : iqa->iqa is zero - must be a mistake\n", __func__);
+		return;
+	}
+	if (*iq_page != NULL) {
+		kunmap(*iq_page);
+		kvm_release_page_clean(*iq_page);
+		*iq_page = NULL;
+		*iq = NULL;
+	}
+	*iq_page = gfn_to_page(kvm, iqa->iqa);
+	if (*iq_page == bad_page) {
+		printk(KERN_ERR "%s : problem mapping invalidation queue\n", __func__);
+	}
+	*iq = (struct iommu_generic_desc*)kmap(*iq_page);
+	if (*iq == NULL) {
+		printk(KERN_ERR "%s : could not map iq\n", __func__);
+	}
+}
+
+static 
+int intel_iommu_process_iq(struct kvm* kvm, struct dmar_status* dmar_status)
+{
+	struct dmar_regs_region* dmar_regs = 
+		&(dmar_status->dmar_regs);
+	struct dmar_regs_region* phy_dmar_regs = 
+		dmar_status->phy_dmar_regs;
+
+	struct iqa_reg old_iqa;
+	struct iqa_reg* iqa = &dmar_regs->iqa;
+	struct iqt_reg* iqt = &dmar_regs->iqt;
+	struct iqh_reg* iqh = &dmar_regs->iqh;
+	volatile struct iqa_reg* actual_iqa = &phy_dmar_regs->iqa;
+	volatile struct iqh_reg* actual_iqh = &phy_dmar_regs->iqh;
+	size_t len;
+	struct page **iq_page = &dmar_status->iq_page;
+	struct iommu_generic_desc **iq = &dmar_status->iq;
+/*	uint16_t skipped_iqh_updates = 0;*/
+
+
+	old_iqa = *iqa;
+	*iqa = *actual_iqa;
+/*
+	if (*(uint64_t*)iqa == 0)
+		viommu_load_iq(kvm, dmar_status);
+*/
+	
+	if (unlikely((*((uint64_t*)(iqa))) != (*((uint64_t*)&old_iqa)) || *iq == NULL)) {
+		printk(KERN_ERR "%s : loading new iq page %llx %llx\n", __func__, *((uint64_t*)(iqa)), 
+			*(((uint64_t*)(&old_iqa))));
+		if (*iq_page != NULL) {
+			kunmap(*iq_page);
+			kvm_release_page_clean(*iq_page);
+			*iq_page = NULL;
+			*iq = NULL;
+		}
+		*iq_page = gfn_to_page(kvm, iqa->iqa);
+		if (*iq_page == bad_page) {
+			printk(KERN_ERR "%s : problem mapping invalidation queue\n", __func__);
+		}
+		*iq = (struct iommu_generic_desc*)kmap(*iq_page);
+		if (*iq == NULL) {
+			printk(KERN_ERR "%s : could not map iq\n", __func__);
+		}
+	}
+
+
+	len = 1 << (8 + iqa->qs);
+		
+	while (iqh->qh != iqt->qt) {
+		iommu_process_iq_desc(kvm, &((*iq)[iqh->qh]), dmar_status);
+		iqh->qh++;
+		iqh->qh %= len;
+/*
+		skipped_iqh_updates++;
+
+		if (++skipped_iqh_updates >= 10) {
+			*actual_iqh = *iqh;
+			skipped_iqh_updates = 0;
+		}
+*/
+	}	
+	*actual_iqh = *iqh;
+	clear_pending_unmap(kvm);
+	return 0;
+}
+
+
+static
+void intel_iommu_update_devfn_context(struct kvm *kvm, struct dmar_status* dmar_status,
+	uint32_t sid, uint32_t cirg, uint32_t fm)
+{
+	struct devfn_context* devfn_context;
+	/*struct IommuRootEntry re;
+	struct IommuContextEntry ce;
+*/	struct dmar_regs_region* dmar_regs = &dmar_status->dmar_regs;
+	uint8_t busnr, devfn, invalidate_all, found;
+	uint16_t i, cur_sid, sid_mask;
+	uint8_t start_fm_bit, fm_bits_num;
+	struct list_head *cur, *tmp;
+
+	if (!dmar_regs->gsts.rtps) {
+		pr_debug("cannot find root, gcmd=%lx", *((unsigned long*)(&dmar_regs->gcmd)));
+		return;
+	}
+	if (intel_iommu_analyze_cirg(cirg, fm, 
+		&invalidate_all, &start_fm_bit, &fm_bits_num))
+		return;
+
+	if (0 && invalidate_all) {
+		intel_iommu_clear_devfn_context(dmar_status);
+		return;
+	}
+
+	sid_mask = (fm_bits_num == 0) ? (~(uint16_t)0) :
+		(sid & (~(((uint16_t)1)<<(start_fm_bit+fm_bits_num)) -
+		(((uint16_t)1)<<start_fm_bit)));
+	
+	for (i = 0; i < (1<<fm_bits_num); i++) {
+		cur_sid = (sid & sid_mask) + (i<<start_fm_bit);
+		busnr = get_busnr(cur_sid);
+		devfn = get_devfn(cur_sid);
+		if (!is_device_assigned(kvm, busnr, devfn))
+			continue;
+		found = 0;
+
+		list_for_each_safe(cur, tmp, &dmar_status->devfn_context_head) {
+			devfn_context = list_entry(cur, struct devfn_context, node);
+			if (devfn_context->g_devfn == devfn && devfn_context->g_busnr == busnr)  {
+			/*	if (dmar_status->ce[devfn].p)
+			*/		devfn_context->context = dmar_status->ce[devfn];
+			/*	else {
+					list_del(cur);
+					kfree(devfn_context);
+					devfn_context->context->p = NULL;
+				}*/
+				found = 1;
+				break;
+			}
+		}
+		if (!found && dmar_status->ce[devfn].p) {
+			devfn_context = 
+				kmalloc(sizeof(struct devfn_context), GFP_KERNEL);
+			if (!devfn_context)
+			devfn_context->context = dmar_status->ce[devfn];
+			devfn_context->g_devfn = devfn;
+			devfn_context->g_busnr = busnr;
+			list_add(&dmar_status->devfn_context_head,
+				&devfn_context->node);
+		}
+	} 
+}
+
+static
+void intel_iommu_clear_devfn_context(struct dmar_status* dmar_status)
+{
+	struct list_head *cur, *tmp;
+	struct devfn_context* devfn_context;
+	list_for_each_safe(cur, tmp, &dmar_status->devfn_context_head) {
+		devfn_context = list_entry(cur, struct devfn_context, node);
+		iommu_vdomain_destroy(&devfn_context->iommu_domain);
+		list_del(cur);
+		kfree(devfn_context);
+	}
+}
+/*
+static
+struct list_head* intel_iommu_find_devfn(struct dmar_status* dmar_status, 
+	int did, struct list_head* start_devfn)
+{
+	struct devfn_context* devfn_context;
+	struct list_head *cur;
+
+	list_for_each(cur, start_devfn) {
+		devfn_context = list_entry(cur, struct devfn_context, node);
+		if (devfn_context->context.did == did) 
+			return cur;
+	}
+
+	return NULL;	
+}
+*/
+struct iommu_monitored_region {
+	uint16_t offset;
+	size_t size;
+/*	uint16_t weight; */
+};
+
+struct iommu_monitored_region iommu_monitored_regions[] = {
+	{offsetof(struct dmar_regs_region, iqt), sizeof(struct iqt_reg)/*, 1000*/},
+	{offsetof(struct dmar_regs_region, gcmd), sizeof(struct gcmd_reg)/*, 1*/},
+	{offsetof(struct dmar_regs_region, ccmd), sizeof(struct ccmd_reg)/*, 1*/},
+	{offsetof(struct dmar_regs_region, pmen), sizeof(struct pmen_reg)/*, 1*/},
+/*	{offsetof(struct dmar_regs_region, readdr), sizeof(uint64_t)},*/
+	{offsetof(struct dmar_regs_region, iotlb), sizeof(struct iotlb_reg)/*, 1*/},
+/*	{offsetof(struct dmar_regs_region, iva), sizeof(struct iva_reg), 1},*/
+/*	{offsetof(struct dmar_regs_region, iqa), sizeof(struct iqa_reg)},*/ /* TODO: not here */
+	{0, 0/*, 0*/}
+};
+
+
+static void iommu_monitoring_start(void* opaque)
+{
+        int bufsize = 16;
+        uint8_t buf[bufsize+1];
+/*	size_t snapshot_size = 0;*/
+	struct iommu_monitored_region* region = iommu_monitored_regions;
+/*	struct iommu_monitored_region** changed_regions = NULL;*/
+	struct kvm *kvm = (struct kvm*)opaque;
+	struct viommu *viommu = kvm->viommu;
+	struct iommu_state *iommu_state = &viommu->iommu_state;
+	struct dmar_status *dmar_status = &iommu_state->dmar_status[0];
+	struct dmar_regs_region *phy_dmar_regs = dmar_status->phy_dmar_regs;
+	void *dmar_regs = (void*)&dmar_status->dmar_regs;
+	struct kthread_t *kthread = viommu->sidecore_thread;
+	struct iova_entry *cached_iova = &viommu->cached_iova;
+	/*uint16_t weight = 0;*/
+	void *shadow, *actual;
+	int go_sleep = 0;
+	int samples = 0;
+#if MONITOR_CYCLES
+	cycles_t cur_cycles;
+	cycles_t prev_cycles;
+	u16 cstate;
+	u16 sequential_processed = 0;
+	
+	/* HACK: Enabling the performance counters */
+	wrmsr(0x38d, 0x333, 0);
+
+	phy_dmar_regs->active_cycles = 0;
+	phy_dmar_regs->inactive_cycles = 0;
+
+	/* Still problems with the perf counter enabling */
+	
+
+#endif
+#ifdef OTHER_CL_MONITOR
+	set_cl_resched_ptr(&phy_dmar_regs->res5);
+	set_cl_monitored_ptr((uint32_t*)&phy_dmar_regs->iqt);
+	set_cl_monitored_val(0);
+	smp_call_function(all_cores_wbinvd, NULL, 1);
+#endif
+
+/*
+	local_irq_disable();
+*/
+
+/*	int changed, i, regions_num = 0;*/
+	/*gfn_to_page(kvm, DMAR_REG_BASE>>PAGE_SHIFT);;	
+	unsigned long flags;
+	int alock=0;*/
+	set_fs(KERNEL_DS);
+        /* kernel thread initialization */
+        lock_kernel();
+        kthread->running = 1;
+
+        current->flags |= PF_NOFREEZE;
+
+        /* daemonize (take care with signals, after daemonize() they are disabled) */
+        daemonize(MODULE_NAME);
+        allow_signal(SIGKILL);
+        unlock_kernel();
+
+	/* allocating space for the registers image */
+	cur_cycles = get_cycles();
+	region = iommu_monitored_regions;
+        /* main loop */
+        for (;;)
+        {
+		int changed = 0;
+		int wait = 0;
+                if (signal_pending(current)) {
+                        break;
+		}
+
+		if (need_resched()) {
+			schedule();
+		}
+
+		/*weight = region->weight;*/
+		shadow = dmar_regs + region->offset;
+		actual = ((void*)phy_dmar_regs) + region->offset;
+		volatile_memcpy(buf, actual, region->size);
+
+		/*go_sleep = 1;*/
+		if (region->offset == offsetof(struct dmar_regs_region, iqt) &&
+			go_sleep) {
+/*			viommu_update_counter(WRONG_WAKE_COUNTER,1);
+			viommu_update_counter(CORRECT_WAKE_COUNTER,1);
+*/			
+
+#ifdef OTHER_CL_MONITOR
+#if 0
+			if (primitive_memcmp(buf, shadow, region->size)) {
+				changed = 1;
+			} else {
+
+				stop_critical_timings();
+				(*pm_idle)();
+				start_critical_timings();
+				volatile_memcpy(buf, actual, region->size);
+			}
+#else
+			cstate = (controls_data[CSTATE_CONTROL] > 1) ? controls_data[CSTATE_CONTROL] : 
+				1;
+			(*pm_cl_monitoring)((u64*)actual, *(u64*)shadow, cstate);
+#endif
+#endif
+			set_cl_monitored_val(*(u32*)(&phy_dmar_regs->iqt));
+			wait = 1;
+		}
+		
+		if (changed || primitive_memcmp(buf, shadow, region->size)) {
+			changed = 1;
+			primitive_memcpy(shadow, buf, region->size);
+		}
+
+		/*go_sleep = 0;	*/
+		if (!go_sleep)
+			samples++;
+/*
+		if (wait) {
+			if (!changed && !need_resched())
+				viommu_update_counter(WRONG_WAKE_COUNTER,0);
+			else
+				viommu_update_counter(CORRECT_WAKE_COUNTER,0);
+		}
+*/			
+
+/*			weight--;*/
+/*
+			if (weight == 0) {
+				region++;
+				weight = region->weight;
+			}		
+		} while (region->size);
+*/
+		if (changed) {
+#if MONITOR_CYCLES
+			prev_cycles = cur_cycles;
+			cur_cycles = get_cycles();
+			phy_dmar_regs->inactive_cycles +=
+				(cur_cycles - prev_cycles);
+#endif	
+
+			intel_iommu_write_status_update(kvm,
+				iommu_state->dmar_status, 
+				region->offset);
+			/* Go back to the first record */
+			region = iommu_monitored_regions;
+
+#if MONITOR_CYCLES
+			sequential_processed++;
+			prev_cycles = cur_cycles;
+			cur_cycles = get_cycles();
+			phy_dmar_regs->active_cycles +=
+				(cur_cycles - prev_cycles);
+#endif	
+
+		} else {
+			if (samples >= 1000)
+				region++;
+			if (region->size == 0) {
+				region = iommu_monitored_regions;
+				samples = 0;
+			}
+
+			if (cached_iova->iommu_domain && !cached_iova->valid) {
+                		cached_iova->hpa = iommu_iova_to_phys(cached_iova->iommu_domain, 
+					cached_iova->iova, &cached_iova->mapped);
+				cached_iova->valid = 1;
+			}
+		}
+#if MONITOR_CYCLES
+/*		if (!changed || sequential_processed > 100) {
+			unhalted_cnt = native_read_msr(0x30b);
+			sequential_processed = 0;
+		}
+*/
+#endif	
+        }
+
+
+#ifdef OTHER_CL_MONITOR
+	/* Restoring the mwait to perform without ptr */
+	set_cl_resched_ptr(NULL);
+	set_cl_monitored_ptr(NULL);
+	on_each_cpu(all_cores_wbinvd, NULL, 1);
+#endif
+        kthread->thread = NULL;
+        kthread->running = 0;
+}
+
+void kvm_viommu_exit(struct kvm* kvm)
+{
+        int err = 0;
+	struct pid *pid;
+	struct viommu *viommu; 
+	struct kthread_t *kthread;
+	struct iommu_state *iommu_state;
+	struct dmar_status *dmar_status; 
+
+	if (kvm->viommu == NULL)
+		return;
+	viommu = kvm->viommu;
+	iommu_state = &viommu->iommu_state ;
+	dmar_status = &iommu_state->dmar_status[0];
+	kthread = viommu->sidecore_thread;
+
+        if (kthread == NULL)
+                printk(KERN_INFO MODULE_NAME": no kernel thread to kill\n");
+        else 
+        {
+#if 0
+		struct dmar_regs_region* dmar_regs = dmar_status->phy_dmar_regs;
+		struct thread_info* = current_thread_info();
+		thread_info->set_cl_monitoring
+		/* TODO: Must  */
+		on_each_cpu(smp_clflush, &dmar_regs->res5, 1);
+#endif
+		
+                lock_kernel();
+		pid = find_vpid(kthread->thread->pid);
+		if (pid) {
+			pid = get_pid(find_vpid(kthread->thread->pid));
+                	err = kill_pid(pid, SIGKILL, 1);
+		} else {
+			kthread->running = 0;
+		}
+                unlock_kernel();
+
+                /* wait for kernel thread to die */
+                if (err < 0)
+                        printk(KERN_INFO MODULE_NAME": unknown error %d while trying to terminate kernel thread\n",-err);
+                else 
+                {
+                        while (kthread->running == 1)
+                                msleep(10);
+                        printk(KERN_INFO MODULE_NAME": succesfully killed kernel thread!\n");
+                }
+        	kfree(kthread);
+        }
+
+
+        /* free allocated resources before exit */
+        if (viommu != NULL) 
+        {
+		int i;
+		struct page **iq_page = &dmar_status->iq_page;
+		struct page **phy_dmar_regs_page = &dmar_status->phy_dmar_regs_page;
+
+		/* unmap all before exit */
+		viommu_clear_all(kvm);
+
+
+		if (*phy_dmar_regs_page) {
+			kunmap(*phy_dmar_regs_page);
+			kvm_release_page_clean(*phy_dmar_regs_page);
+			*phy_dmar_regs_page = NULL;
+		}
+		if (*iq_page) {
+			kunmap(*iq_page);
+			kvm_release_page_clean(*iq_page);
+			*iq_page = NULL;
+		}
+		intel_iommu_clear_devfn_context(dmar_status);
+
+		for (i = 0; i < VIOMMU_PAGE_CACHE_SETS; i++) {
+#ifdef VIOMMU_LRU
+			int j;
+			for (j = 0; j < VIOMMU_PAGE_CACHE_WAYS; j++) {
+				struct viommu_page_cache_entry *page_cache_entry = 
+					&viommu->page_cache[i][j];
+#else
+				struct viommu_page_cache_entry *page_cache_entry = 
+					&viommu->page_cache[i];
+#endif
+				if (page_cache_entry->page) {
+					kunmap(page_cache_entry->page);
+					if (page_cache_entry->flags & VIOMMU_PAGE_CACHE_DIRTY) {
+						kvm_release_page_dirty(page_cache_entry->page);
+					} else {
+						kvm_release_page_clean(page_cache_entry->page);
+					}
+				}
+#ifdef VIOMMU_LRU
+       			}
+#endif
+		}
+	}
+
+        kthread = NULL;
+	kvm->viommu = NULL;
+/*
+        printk(KERN_INFO MODULE_NAME": module unloaded\n");
+*/
+}
+
+static
+int create_iommu_monitoring_thread(struct kvm *kvm)
+{
+	struct viommu *viommu = kvm->viommu;
+	struct kthread_t **p_kthread = &viommu->sidecore_thread;
+	*p_kthread = kmalloc(sizeof(struct kthread_t), GFP_KERNEL);
+	memset(*p_kthread, 0, sizeof(struct kthread_t));
+/*
+	kthread->kvm = kvm;
+*/
+        /* start kernel thread */
+/*        (*p_kthread)->thread = kthread_run((void *)iommu_monitoring_start, kvm, MODULE_NAME);
+*/	(*p_kthread)->thread = kthread_create((void *)iommu_monitoring_start, kvm, MODULE_NAME);
+
+	if (IS_ERR((*p_kthread)->thread)) {
+                printk(KERN_INFO MODULE_NAME": unable to start kernel thread\n");
+                kfree(*p_kthread);
+                *p_kthread = NULL;
+                return -ENOMEM;
+	}
+
+	kthread_bind((*p_kthread)->thread, 3);
+	wake_up_process((*p_kthread)->thread);  
+
+        return 0;
+}
+
+
+
+/*******************************************/
+
+static int init_viommu(struct kvm* kvm, uint64_t addr, uint8_t sidecore)
+{
+        int ret;
+	struct viommu* viommu = kvm->viommu;
+	struct iommu_state* iommu_state = &viommu->iommu_state;
+	struct dmar_status* dmar_status = &iommu_state->dmar_status[0];
+	struct page **phy_dmar_regs_page = &dmar_status->phy_dmar_regs_page;
+	int i, npages;
+
+        memset(viommu, 0, sizeof(*viommu));
+	printk(KERN_ERR "%s : start %llx", __func__, addr);
+
+        spin_lock_init(&viommu->lock);
+
+        ret = -ENOMEM;
+/*	*phy_dmar_regs_page = gfn_to_page(kvm, DMAR_REG_BASE>>PAGE_SHIFT);
+*/
+        npages = get_user_pages(kvm->tsk, kvm->mm, addr, 1, 1, 0, phy_dmar_regs_page,
+                                        NULL);
+        printk(KERN_ERR "%s : Got %d pages", __func__, npages);
+
+	if (!*phy_dmar_regs_page || *phy_dmar_regs_page == bad_page) {
+		printk(KERN_ERR "Error mapping physical dmar regs page");
+		*phy_dmar_regs_page = NULL;
+		return -1; /* TODO: better error code */
+	}
+
+	dmar_status->phy_dmar_regs = kmap(*phy_dmar_regs_page);
+	viommu->cached_iova.iommu_domain = NULL;
+
+
+	
+	/* Setting as uncachable */
+	/*mtrr_add(gfn_to_pfn(kvm, addr>>PAGE_SHIFT)<<PAGE_SHIFT, PAGE_SIZE, MTRR_TYPE_UNCACHABLE, 1);
+	set_pages_uc(gfn_to_page(kvm, addr>>PAGE_SHIFT), 1);
+*/
+/*
+	dmar_status->phy_dmar_regs = (struct dmar_regs_region*)gfn_to_hva(kvm, addr>>PAGE_SHIFT);
+
+	if (!dmar_status->phy_dmar_regs) { 
+		printk(KERN_ERR "%s : cannot find hva of guest page!", __func__);	
+                goto out;
+	}
+
+	printk(KERN_ERR "%s : phy_dmar_regs = %p %lx\n", __func__, dmar_status->phy_dmar_regs,
+		gfn_to_pfn(kvm, addr>>PAGE_SHIFT));
+*/
+/*
+	kvm_get_pfn(gfn_to_pfn(kvm,addr>>PAGE_SHIFT));
+	kvm_set_pfn_accessed(gfn_to_pfn(kvm, addr>>PAGE_SHIFT));
+	kvm_set_pfn_dirty(gfn_to_pfn(kvm, addr>>PAGE_SHIFT));
+*/	/*
+	set_memory_uc(virt_to_phys((unsigned long)k->page),1);
+*/
+
+/*	printk(KERN_ERR "physical %llx\n", virt_to_phys(dmar_status->phy_dmar_regs));
+*/
+/*
+	k->mtrr = mtrr_add(virt_to_phys(k->page), PAGE_SIZE, MTRR_TYPE_UNCACHABLE, 1);
+	set_pages_uc(virt_to_page(k->page), 1);
+*/
+
+/*
+        k->timer_hook.func = klife_timer_irq_handler;
+        k->timer_hook.data = k;
+*/
+
+    	viommu->iommu_state.invalidation_queue = INVALIDATION_QUEUE;
+
+	viommu->deferred = sidecore && !iommu_domain_has_cap(NULL, IOMMU_CAP_STRICT);
+/*    	qemu_register_reset(iommu_reset, viommu);*/
+	spin_lock_init(&viommu->iommu_state.dmar_status[0].access_lock);
+
+	viommu->iommu_state.dmar_status[0].iq_page = NULL;
+	viommu->iommu_state.dmar_status[0].iq = NULL;
+	
+	printk(KERN_ERR "%s : Starting to init cache", __func__);
+
+	/* init page cache */
+	for (i = 0; i < VIOMMU_PAGE_CACHE_SETS; i++) {
+#ifdef VIOMMU_LRU
+		int j;
+		for (j=0; j < VIOMMU_PAGE_CACHE_WAYS; j++) {
+			struct viommu_page_cache_entry *page_cache_entry = &viommu->page_cache[i][j];
+#else
+			struct viommu_page_cache_entry *page_cache_entry = &viommu->page_cache[i];
+#endif
+			page_cache_entry->page = NULL;
+			page_cache_entry->ptr = NULL;
+			page_cache_entry->gfn = 0;
+			page_cache_entry->flags = 0;
+#ifdef VIOMMU_LRU
+		}
+#endif
+	 }
+	printk(KERN_ERR "%s : End init cache", __func__);
+	iommu_reset(viommu);
+
+	if (sidecore)
+	 	create_iommu_monitoring_thread(kvm);
+	init_procfs();
+
+        return 0;
+/*
+free_page:
+        free_page((unsigned long)k->page);
+
+out:
+*/
+        return ret;
+}
+
+#if 0
+static int viommu_mmap(struct file* file, struct vm_area_struct* vma)
+{
+        int ret;
+        struct viommu* viommu;
+	unsigned long flags;
+	
+	printk(KERN_ERR "Entered viommu_mmap \n");
+	viommu  = file->private_data;
+	spin_lock_irqsave(&viommu->lock, flags);
+
+        pr_debug("inside viommu_mmap, file %p, vma %p\n",
+                 file, vma);
+/*
+        if (vma->vm_flags & VM_SHARED)
+                return -EINVAL; 
+*/	
+/* we don't support MAP_SHARED */
+        pr_debug("vma %p, vma->vm_start %lx, vma->vm_end %lx, gridsize %lu\n",
+                 vma, vma->vm_start, vma->vm_end, PAGE_SIZE);
+
+#if 0
+        if (vma->vm_end - vma->vm_start != sizeof(k->grid))
+                return -ENOSPC;
+	for (ret=0; ret<PAGE_SIZE; ret++)
+		if (*(viommu->page + ret) != 0)
+			printk(KERN_ERR "Hey, not zero!! %d\n", ret);
+#endif /* 0 */
+	
+	ret =0;
+        SetPageReserved(virt_to_page(viommu->page));
+        ret = remap_pfn_range(vma, vma->vm_start,
+                              virt_to_phys(viommu->page) >> PAGE_SHIFT,
+                              PAGE_SIZE, vma->vm_page_prot);
+
+        pr_debug("io_remap_page_range returned %d\n", ret);
+
+	printk(KERN_ERR "%s : exit\n", __func__);
+        if (ret == 0)
+                viommu->mapped = 1;
+	spin_unlock_irqrestore(&viommu->lock, flags);
+
+        return ret;
+}
+
+static void free_viommu(struct viommu* k)
+{
+        free_page((unsigned long)k->page);
+        kfree(k);
+}
+
+static ssize_t
+viommu_read_mapped(struct file *filp, char *ubuf, size_t count, loff_t *f_pos)
+{
+	struct viommu* viommu;
+	unsigned long flags;
+
+	viommu = filp->private_data;
+
+	spin_lock_irqsave(&viommu->lock, flags);
+	spin_unlock_irqrestore(&viommu->lock, flags);
+	return 0;	
+}
+
+static ssize_t
+viommu_read(struct file *filp, char *ubuf, size_t count, loff_t *f_pos)
+{
+	struct viommu* viommu;
+	ssize_t len, ret;
+	loff_t pos;
+
+	viommu = filp->private_data;
+
+	if (f_pos == NULL)
+		goto out;
+
+	pos = max((size_t)(*f_pos), (size_t)(PAGE_SIZE-1));
+
+	if (viommu->mapped)
+		return viommu_read_mapped(filp, ubuf, count, f_pos);
+
+        len = count < PAGE_SIZE-pos ? count : (size_t)(PAGE_SIZE-pos); /* len can't be negative */
+
+        if (len < 0) {
+                ret = len;
+                goto out;
+        }
+
+        if (copy_to_user(ubuf, viommu->page+pos, len)) {
+                ret = -EFAULT;
+                goto out;
+        }
+out:
+	return 0;
+}
+
+static ssize_t
+viommu_write(struct file* filp, const char __user * ubuf,
+                           size_t count, loff_t *f_pos)
+{
+        size_t sz;
+        struct viommu* k = filp->private_data;
+        ssize_t ret;
+	loff_t pos;
+
+	if (!f_pos)
+		return 0;
+
+	pos = *f_pos;
+
+	if (!k->mapped)
+		return 0;
+
+        sz = max(count, PAGE_SIZE-(size_t)pos);
+
+        ret = -EFAULT;
+        if (copy_from_user(k->page+pos, ubuf, sz))
+                goto out;
+
+        if (ret == 0)
+                ret = sz;
+out:
+        return ret;
+}
+
+static int viommu_release(struct inode *inode, struct file *filp)
+{
+        struct viommu* k = filp->private_data;
+
+        if (k->mapped) {
+                /* undo setting the grid page to be reserved */
+                ClearPageReserved(virt_to_page(k->page));
+		set_pages_wb(virt_to_page(k->page), 1);
+/*
+	k->mtrr = mtrr_add(virt_to_phys(k->page), PAGE_SIZE, MTRR_TYPE_UNCACHABLE, 1);
+		mtrr_del(k->mtrr, virt_to_phys(k->page), PAGE_SIZE);
+  */      }
+
+        free_viommu(k);
+
+        return 0;
+}
+#endif
+
+int alloc_viommu(struct kvm *kvm, uint64_t addr, int sidecore)
+{
+        int ret;
+        struct viommu* k;
+	struct viommu** pk = &kvm->viommu;
+
+        k = kmalloc(sizeof(*k), GFP_KERNEL);
+        if (!k)
+                return -ENOMEM;
+        
+	*pk = k;
+
+        ret = init_viommu(kvm, addr, sidecore);
+        if (ret) {
+                kfree(k);
+		k = NULL;
+	}
+
+        return ret;
+}
+
+void viommu_mark_vmexit(struct kvm* kvm)
+{
+}
+
+
+
+
+#if 0
+static int viommu_open(struct inode *inode, struct file *filp)
+{
+        struct viommu* k;
+        int ret;
+
+        ret = alloc_viommu(&k);
+        if (ret)
+                return ret;
+
+        filp->private_data = k;
+
+        return 0;
+}
+
+static int
+viommu_ioctl(struct inode* inode, struct file* file, unsigned int cmd,
+            unsigned long data)
+{
+	struct viommu *viommu = file->private_data;
+	int ret = 0;
+	unsigned long flags;
+	struct kvm *kvm = viommu->kvm;
+	int lock = 0;
+	
+	if (kvm) {	
+	        /*down_read(&kvm->slots_lock);*/
+       		mutex_lock(&kvm->lock);
+		lock = 1;
+	}
+
+	switch (cmd) {
+		case VIOMMU_TRANSLATE: {
+			struct iommu_translate translate;
+			printk(KERN_ERR "Start VIOMMU_TRANS\n");
+			if (copy_from_user(&translate, (void*)data, sizeof(translate))) {
+				ret = -EFAULT;
+				goto done;
+			}
+			printk(KERN_ERR "VIOMMU_TRANS copied\n");
+			translate.gpa = iommu_phy_addr_translate(viommu,
+				translate.iova, translate.is_write, 
+				translate.devfn, &translate.err);
+			printk(KERN_ERR "Start VIOMMU_TRANS translated\n");
+			if (copy_to_user((void*)data, &translate, sizeof(translate))) {
+				ret = -EFAULT;
+				goto done;
+			}
+			printk(KERN_ERR "VIOMMU_TRANS done\n");
+			break;	
+
+		}
+		case VIOMMU_SET_DEVFN_MAP: {
+			struct devfn_mapping devfn_mapping;
+			printk(KERN_ERR "Start SET_DEVFN_MAP\n");
+			if (copy_from_user(&devfn_mapping, (void*)data, sizeof(devfn_mapping))) {
+				ret = -EFAULT;
+				goto done;
+			}
+			intel_iommu_add_dev(viommu, &devfn_mapping);
+
+		/* TODO: Mapping */	
+			break;
+		}
+		case VIOMMU_SET_KVM: {
+			struct file *filp = (struct file*)data;
+			printk(KERN_ERR "Start SET_KVM %p\n", filp->private_data);
+			viommu->kvm = filp->private_data;
+			break;
+		}
+		default:
+			printk(KERN_ERR "The commands is %ul %ul %ul\n", VIOMMU_SET_KVM, VIOMMU_SET_DEVFN_MAP,
+				VIOMMU_TRANSLATE); 
+			printk(KERN_ERR "%s : no command %d\n", __func__, cmd);
+	}
+/*
+	spin_lock_irqsave(&iommu_state.dmar_status[0].access_lock, flags);	
+	spin_lock_irqrestore(&iommu_state.dmar_status[0].access_lock, flags);
+*/
+done:
+	if (lock) {
+	        mutex_unlock(&kvm->lock);
+      		up_read(&kvm->slots_lock);
+	}
+	return ret;
+
+}
+/*
+static struct file_operations viommu_fops = {
+        .owner = THIS_MODULE,
+        .open = viommu_open,
+        .release = viommu_release,
+        .read = viommu_read,
+        .write = viommu_write,
+        .mmap = viommu_mmap,
+        .ioctl = viommu_ioctl
+};
+*/
+/*
+static int __init viommu_module_init(void)
+{
+        int ret;
+
+        pr_debug("viommu module init called\n");
+
+        if ((ret = register_chrdev(MMONITOR_MAJOR_NUM, "viommu", &viommu_fops)) < 0)
+                printk(KERN_ERR "register_chrdev problem: %d\n", ret);
+
+                printk(KERN_ERR "register_chrdev: %d\n", ret);
+        return ret;
+}
+*/
+/*
+static void viommu_module_cleanup(void)
+{
+	iommu_monitoring_exit();
+        unregister_chrdev(MMONITOR_MAJOR_NUM, "viommu");
+}
+*/
+
+#endif
+
+
+
